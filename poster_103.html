

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            TheGlueNote: Learned Representations for Robust and Flexible Note Alignment
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Silvan David Peter (JKU)*"
               class="text-muted"
            >Silvan David Peter (JKU)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Gerhard Widmer (Johannes Kepler University)"
               class="text-muted"
            > Gerhard Widmer (Johannes Kepler University)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UPU92QGK" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p4-14-thegluenote-learned-representations</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; alignment, synchronization, and score following"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; alignment, synchronization, and score following</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; symbolic music processing"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; symbolic music processing</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1GHRxv4pvcpny1pO3Lgx1W1u7kbGoUXnn/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1PfmicdgUqlBQIlc7QFRllz3Km0qc3w_D/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/12FVcGsJpZaS3qKKGy82vNAFXsaaoTm_9/preview" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>This is a meta-review, summarising the main points of the individual reviews.</p>
<p>Traditional methods like Dynamic Time Warping (DTW) or Hidden Markov Models (HMM) struggle with aligning symbolic note sequences that have large mismatches. This paper proposes a novel approach using various complex manipulations (e.g., repeats, skips, block insertions, and long trills) to train a model that predicts pairwise note similarities for two 512-note subsequences. As a post-processing step, DTW is used and compared to two alternatives. The approach is shown to lead to very high accuracy, even in the presence of large mismatches, and it is directly applicable to any kind of MIDI file.</p>
<p>Strengths:
- Innovative Approach: The paper proposes a novel method using a transformer-based model for aligning symbolic note sequences, which addresses the limitations of traditional methods like DTW and HMM, especially in the presence of large mismatches.<br />
- Detailed Explanation: The model and experimental setup are explained in detail, providing a clear understanding of the approach and its implementation.<br />
- High Accuracy: The proposed method demonstrates very high accuracy, outperforming baseline methods and showing robustness against a large number of mismatches.<br />
- Reproducibility: The authors promise to provide code and datasets online, which facilitates reproducibility and further research.</p>
<p>Weaknesses:
- Clarity in Description: Some parts of the paper, particularly the data augmentations and certain concepts (see e.g. the comment on attention by reviewer 4) could be explained more clearly.<br />
- Figures 1 and 2 have been noted to either lack informativeness or be too dense. Additionally, inconsistencies in terminology and numbering in tables need addressing.  </p>
<p>Please also consider the very detailed and helpful comments by the individual reviewers!</p>
<p>Overall, the paper is a valuable contribution to ISMIR and recommended for acceptance.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This paper presents a method to perform note alignment via learned representation.
It essentially learns pairwise similarity of two 512-note sequences.
The core of the method is a transformer encoder that encodes the note sequences to determine a similarity measure, which is then ultimately matched by a match extractor, using either (1) taking the maximally similar pairs, (2) using a transformer decoder to return the matches, and (3) using DTW.  DTW-based matching is done in two stages, by first roughly aligning by typical DTW path constraints, and match the notes pitch-wise so that is close to the initial DTW result.</p>
<p>The method is evaluated with various augmentations including repeats, skips, insertions, deletions and trills.  The results show the method outperforms baseline of using pitch-onset similairty matrix, and shows that the DTW-based processing performs the best.  The paper also shows larger models with more residual dimension and blocks perform better.
Finally, the method is compared against existing methods, which shows the method is more robust under great number of mismatches.</p>
<p>The paper is interesting and provides a robust method for note-level alignment, which is valuable for the community.  There are a few questions, however.</p>
<p>First, please state the terminal conditions for the DTW, i.e. if there are constraints at the beginning and the end of the sequences.  Second, in theory this model can be applied to sequences of arbitrary lengths, except potentially when using a transformer decoder.  It would have been nice to understand the performance when the note duration is varied (and hence the extent of nonlocal information is provided to the model is varied).</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper presents “TheGlueNote”, a transformer encoder that predicts note similarities for the alignment of two MIDI sequences with potentially complex mismatch cases such as note repetitions, insertions, or deletions. The authors give a good introduction into the topic, and, to the best of my knowledge, reference all important related work. The core model and its extensions are described in detail, as well as the experimental setup. Together with the promise of providing the code and datasets online, this offers researches an excellent starting point for reproducing results and testing the architecture in their own scenarios. The proposed model is evaluated in three different levels of complexity (# parameters) and three different decoding variants. A comparison with suitable baseline methods is conducted and the results of the proposed model seem to be impressive, while requiring relatively row runtime. </p>
<p>I therefore recommend a “strong accept” for this paper.</p>
<p>For the camera-ready version of this paper, I have a few minor suggestions:
- Make Fig. 1 more illustrative, e.g., by zooming in to a smaller segment and highlighting the problems for interesting sections like trills, insertions,… 
- Enhance the readability of the similarity matrices in Fig. 2 by e.g., choosing black and white colormaps and improving the contrast
- Explain the data augmentation in Table 2 better. What does gT_t2^{n_t} mean? What’s does U(-50,50) (given in MIDI ticks) mean in seconds? Does P_{repeat/skip/trill}=1 mean that the probability of repeats/skips/trills is one? Does it mean that every note is repeated or becomes a trill?
- Facilitate the quick readability of Table 4; It is very interesting to see the results individually for all pieces, but space would permit to add two more columns representing the mean score over all pieces for Default data and 20% mismatch data.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper proposed TheGlueNote, a transformer-based model to learn note-wise representations for note alignment in Midi files. The training data is synthesized by data augmentation with complex mismatch cases. Authors also compare three ways to extract note matches from the similarity matrix computed on the learned representations. The experiment and results are convincing, but the paper writing can be improved (see details below).</p>
<p>major comments:
1. Since the proposed model and data augmentation are the key parts of the paper, they should be described more clearly.
Line 183 -186
‘processed using the fixed-length structured tokenization [22, 23], which encodes relative onset, pitch, duration, and velocity.’
Authors should describe how onsets, pitch, duration and velocity are encoded, rather than assuming that the readers have read [22, 23]. Only with this information, the numbers in Table 2 will make sense.</p>
<p>Section 4.1
Data augmentations should be describe more clearly. The notations in the Table2 should be explained clearly in the paper.</p>
<p>Line 306
“an experiment including extended (100+ note) mismatches”
The author should describe more about how the extended mismatches are augmented. This will help readers to understand in which aspects the proposed model is robust to.</p>
<ol>
<li>the confusion of the concepts
Line 12-13, Line 192-195
The authors concatenate s1 and s2 as one sequence as the network input.
The concepts of ‘within-sequence self-attention’, and ‘between-sequence cross-attention’ are confused with the self-attention and cross-attention commonly-used in the Transformer. Please describe this concepts in a better way.</li>
</ol>
<p>minor comments:</p>
<p>‘CEL Row, CEL Col, CEL’
The loss functions are only mentioned in the Figure 2 caption. 
Please clearly describe the losses in the paper as well and clearly write ‘cross-entropy loss (CEL)’ in the paper. </p>
<p>Figure 2
‘Data processing’ in Figure 2, but ‘pre-processing module’ in the caption. Please make them consistent.
Please clearly mark the ‘Attention Block’ in TheGlueNote part, and it should also be ’n \times Attention Block’ as in the Decoder Head part.</p>
<p>Table 3
‘#ph’ in the table, but ‘#p dh’ in the caption. Please make them consistent.</p>
<p>Line 60
what do you mean by ‘non-local information’? Please rewrite the sentence to make it clear. </p>
<p>Line 74-78
Prior approaches … 
add references at the end of the sentence 
or mention that, for example, ‘Please see details in related work (Section 2).’</p>
<p>Line 163
to to =&gt; to</p>
<p>Line 175-177
‘A pairwise similarity matrix of the note representations of either sequence’ =&gt;A pairwise similarity matrix computed between the note representations of two sequences</p>
<p>Line 195 
cross-attention (s1-s2, s2-s1)
does s1-s2 is the same as s2-s1 after transposing s2-s1? Please describe the relationship between s1-s2 and s2-s1.</p>
<p>Section 5.1 title
model configuration =&gt; Ablation study of model configuration</p>
<p>Some references are not in correct format.
Please check carefully.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We thank the reviewers for their thoughtful and constructive feedback! We made the following changes for the final submission:
- removed uninformative Figure 1 (note alignment visualization) mainly to make room for:
- extended section 4.1 with a thorough description of the data augmentations
- improved readability of Figure 2 (now Figure 1): consistent wording, high-contrast matrices, loss description in main text
- Table 4 includes mean results for all model/dataset variations
- added links and acknowledgements
- changed title of 5.1
- fixed several typos
- reworded "non-local information"
- removed confusing use of cross- and self-attention
- fixed table numbering
- fixed several references</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;103&#39;, &#39;session&#39;: &#39;4&#39;, &#39;position&#39;: &#39;15&#39;, &#39;forum&#39;: &#39;103&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1CcnPsI0Fq-S-7zkqwYY6WIRd5K3wrNAn/view?usp=share_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;TheGlueNote: Learned Representations for Robust and Flexible Note Alignment&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Peter, Silvan*&#39;, &#39; Widmer, Gerhard&#39;], &#39;authors_and_affil&#39;: [&#39;Silvan David Peter (JKU)*&#39;, &#39; Gerhard Widmer (Johannes Kepler University)&#39;], &#39;keywords&#39;: [&#39;MIR tasks -&gt; alignment, synchronization, and score following&#39;, &#39;MIR fundamentals and methodology -&gt; symbolic music processing&#39;], &#39;abstract&#39;: &#39;Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.&#39;, &#39;TLDR&#39;: &#39;Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1PfmicdgUqlBQIlc7QFRllz3Km0qc3w_D/view?usp=sharing&#39;, &#39;session&#39;: [&#39;4&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1GHRxv4pvcpny1pO3Lgx1W1u7kbGoUXnn/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/12FVcGsJpZaS3qKKGy82vNAFXsaaoTm_9/view?usp=sharing&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UPU92QGK&#39;, &#39;slack_channel&#39;: &#39;p4-14-thegluenote-learned-representations&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;This paper presents a method to perform note alignment via learned representation.\nIt essentially learns pairwise similarity of two 512-note sequences.\nThe core of the method is a transformer encoder that encodes the note sequences to determine a similarity measure, which is then ultimately matched by a match extractor, using either (1) taking the maximally similar pairs, (2) using a transformer decoder to return the matches, and (3) using DTW.  DTW-based matching is done in two stages, by first roughly aligning by typical DTW path constraints, and match the notes pitch-wise so that is close to the initial DTW result.\n\nThe method is evaluated with various augmentations including repeats, skips, insertions, deletions and trills.  The results show the method outperforms baseline of using pitch-onset similairty matrix, and shows that the DTW-based processing performs the best.  The paper also shows larger models with more residual dimension and blocks perform better.\nFinally, the method is compared against existing methods, which shows the method is more robust under great number of mismatches.\n\nThe paper is interesting and provides a robust method for note-level alignment, which is valuable for the community.  There are a few questions, however.\n\nFirst, please state the terminal conditions for the DTW, i.e. if there are constraints at the beginning and the end of the sequences.  Second, in theory this model can be applied to sequences of arbitrary lengths, except potentially when using a transformer decoder.  It would have been nice to understand the performance when the note duration is varied (and hence the extent of nonlocal information is provided to the model is varied).&#39;, &#39;review_2&#39;: &#39;This paper presents “TheGlueNote”, a transformer encoder that predicts note similarities for the alignment of two MIDI sequences with potentially complex mismatch cases such as note repetitions, insertions, or deletions. The authors give a good introduction into the topic, and, to the best of my knowledge, reference all important related work. The core model and its extensions are described in detail, as well as the experimental setup. Together with the promise of providing the code and datasets online, this offers researches an excellent starting point for reproducing results and testing the architecture in their own scenarios. The proposed model is evaluated in three different levels of complexity (# parameters) and three different decoding variants. A comparison with suitable baseline methods is conducted and the results of the proposed model seem to be impressive, while requiring relatively row runtime. \n\nI therefore recommend a “strong accept” for this paper.\n\nFor the camera-ready version of this paper, I have a few minor suggestions:\n- Make Fig. 1 more illustrative, e.g., by zooming in to a smaller segment and highlighting the problems for interesting sections like trills, insertions,… \n- Enhance the readability of the similarity matrices in Fig. 2 by e.g., choosing black and white colormaps and improving the contrast\n- Explain the data augmentation in Table 2 better. What does gT_t2^{n_t} mean? What’s does U(-50,50) (given in MIDI ticks) mean in seconds? Does P_{repeat/skip/trill}=1 mean that the probability of repeats/skips/trills is one? Does it mean that every note is repeated or becomes a trill?\n- Facilitate the quick readability of Table 4; It is very interesting to see the results individually for all pieces, but space would permit to add two more columns representing the mean score over all pieces for Default data and 20% mismatch data.&#39;, &#39;review_3&#39;: &#39;This paper proposed TheGlueNote, a transformer-based model to learn note-wise representations for note alignment in Midi files. The training data is synthesized by data augmentation with complex mismatch cases. Authors also compare three ways to extract note matches from the similarity matrix computed on the learned representations. The experiment and results are convincing, but the paper writing can be improved (see details below).\n\n\nmajor comments:\n1. Since the proposed model and data augmentation are the key parts of the paper, they should be described more clearly.\nLine 183 -186\n‘processed using the fixed-length structured tokenization [22, 23], which encodes relative onset, pitch, duration, and velocity.’\nAuthors should describe how onsets, pitch, duration and velocity are encoded, rather than assuming that the readers have read [22, 23]. Only with this information, the numbers in Table 2 will make sense.\n\nSection 4.1\nData augmentations should be describe more clearly. The notations in the Table2 should be explained clearly in the paper.\n\nLine 306\n“an experiment including extended (100+ note) mismatches”\nThe author should describe more about how the extended mismatches are augmented. This will help readers to understand in which aspects the proposed model is robust to.\n\n2. the confusion of the concepts\nLine 12-13, Line 192-195\nThe authors concatenate s1 and s2 as one sequence as the network input.\nThe concepts of ‘within-sequence self-attention’, and ‘between-sequence cross-attention’ are confused with the self-attention and cross-attention commonly-used in the Transformer. Please describe this concepts in a better way.\n\n\nminor comments:\n\n‘CEL Row, CEL Col, CEL’\nThe loss functions are only mentioned in the Figure 2 caption. \nPlease clearly describe the losses in the paper as well and clearly write ‘cross-entropy loss (CEL)’ in the paper. \n\nFigure 2\n‘Data processing’ in Figure 2, but ‘pre-processing module’ in the caption. Please make them consistent.\nPlease clearly mark the ‘Attention Block’ in TheGlueNote part, and it should also be ’n \\times Attention Block’ as in the Decoder Head part.\n\nTable 3\n‘#ph’ in the table, but ‘#p dh’ in the caption. Please make them consistent.\n\nLine 60\nwhat do you mean by ‘non-local information’? Please rewrite the sentence to make it clear. \n\nLine 74-78\nPrior approaches … \nadd references at the end of the sentence \nor mention that, for example, ‘Please see details in related work (Section 2).’\n\nLine 163\nto to =&gt; to\n\nLine 175-177\n‘A pairwise similarity matrix of the note representations of either sequence’ =&gt;A pairwise similarity matrix computed between the note representations of two sequences\n\nLine 195 \ncross-attention (s1-s2, s2-s1)\ndoes s1-s2 is the same as s2-s1 after transposing s2-s1? Please describe the relationship between s1-s2 and s2-s1.\n\nSection 5.1 title\nmodel configuration =&gt; Ablation study of model configuration\n\nSome references are not in correct format.\nPlease check carefully.&#39;, &#39;meta_review&#39;: &#39;This is a meta-review, summarising the main points of the individual reviews.\n\nTraditional methods like Dynamic Time Warping (DTW) or Hidden Markov Models (HMM) struggle with aligning symbolic note sequences that have large mismatches. This paper proposes a novel approach using various complex manipulations (e.g., repeats, skips, block insertions, and long trills) to train a model that predicts pairwise note similarities for two 512-note subsequences. As a post-processing step, DTW is used and compared to two alternatives. The approach is shown to lead to very high accuracy, even in the presence of large mismatches, and it is directly applicable to any kind of MIDI file.\n\nStrengths:\n- Innovative Approach: The paper proposes a novel method using a transformer-based model for aligning symbolic note sequences, which addresses the limitations of traditional methods like DTW and HMM, especially in the presence of large mismatches.  \n- Detailed Explanation: The model and experimental setup are explained in detail, providing a clear understanding of the approach and its implementation.  \n- High Accuracy: The proposed method demonstrates very high accuracy, outperforming baseline methods and showing robustness against a large number of mismatches.  \n- Reproducibility: The authors promise to provide code and datasets online, which facilitates reproducibility and further research.\n\nWeaknesses:\n- Clarity in Description: Some parts of the paper, particularly the data augmentations and certain concepts (see e.g. the comment on attention by reviewer 4) could be explained more clearly.  \n- Figures 1 and 2 have been noted to either lack informativeness or be too dense. Additionally, inconsistencies in terminology and numbering in tables need addressing.  \n\nPlease also consider the very detailed and helpful comments by the individual reviewers!\n\nOverall, the paper is a valuable contribution to ISMIR and recommended for acceptance.&#39;, &#39;author_changes&#39;: &#39;We thank the reviewers for their thoughtful and constructive feedback! We made the following changes for the final submission:\n- removed uninformative Figure 1 (note alignment visualization) mainly to make room for:\n- extended section 4.1 with a thorough description of the data augmentations\n- improved readability of Figure 2 (now Figure 1): consistent wording, high-contrast matrices, loss description in main text\n- Table 4 includes mean results for all model/dataset variations\n- added links and acknowledgements\n- changed title of 5.1\n- fixed several typos\n- reworded &#34;non-local information&#34;\n- removed confusing use of cross- and self-attention\n- fixed table numbering\n- fixed several references\n&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>