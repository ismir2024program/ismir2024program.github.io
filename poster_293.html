

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.", "abstract": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.", "author_changes": "Thank you all for your reviews. Please find below the changes we have made to take them into account.\n\nAs proposed, we modified the title to reflect the fact that we evaluate our approach on drum music.\n\nWe added a reference to get further insight on formal grammar and context-free grammar in the related section.\n\nWe clarified the Figure 2 and the associated text in order to make it more easily understandable.\n\nWe also explained that model hyperparameters, including depth and width of the architectures, as well as the number of bars considered for each encoding were obtained using a hyperparameter search on the validation set.\n\nFurthermore, we also added a note explaining that while we are indeed limited by some of qparse limitations, we aim at evaluating the rhythmic tree representation, regardless of how it has been built. Researching more robust ways of building such trees would be suited for some follow-up research.\n\nIn the supplementary material, we added some details on the grammar we used, and how the tree is built.\n\nAs proposed, we also added the confusion matrix of one of the models trained with the best architecture in the supplementary material, with some succinct interpretation.", "authors": ["G\u00e9r\u00e9, L\u00e9o*", " Audebert, Nicolas", " Rigaux, Philippe"], "authors_and_affil": ["L\u00e9o G\u00e9r\u00e9 (Cnam)*", " Nicolas Audebert (IGN)", " Philippe Rigaux (Cnam)"], "channel_url": "https://ismir2024.slack.com/archives/C07U9F8BYB1", "day": "2", "keywords": ["Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -\u003e representations of music; MIR fundamentals and methodology -\u003e symbolic music processing; Musical features and properties -\u003e representations of music", "MIR tasks -\u003e automatic classification"], "long_presentation": "FALSE", "meta_review": "The discussion of this paper was by and large positive. There is some question of the generality of the representation, and whether the rules extracted by qparse are sufficiently general to allow lots of different rhythmic possibilities. Its proximity to the qparse implementation is also concerning. The authors should spend some time improving their paper long the several directions identified by the reviewers.", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/view?usp=drive_link", "poster_pdf": "https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/view?usp=drive_link", "review_1": "Summary: The authors develop a new tree-based MIDI representation of drumming data (linear rhythmic tree) and test it with 3 other representational models and present their results.  The LRT does the best.\n\nStrengths: This is a very good paper with reproducible results.  The major novel idea is the LRT.  I like that the new model is tested against other models as well, both with LSTMs and transformers.\n\nWeaknesses: Nothing is incredibly ground-breaking or earth-shattering. The results are incremental.", "review_2": "This paper uses grammatical representation of MIDI files for style classification. Experimental results show that comparable or significantly better performance is achieved using less model parameters. The description and results are convincing, but explaining more details could make this paper better.\n\n- Line #165: Please define \u201coptimization-based music transcription systems\u201d. Does it mean \u201ca music transcription system that invokes optimization algorithm\u201d? If yes, then deep models (line 168) are one type of such system, but in this sentence (While designed \u2026) they are viewed as different.\n- Line #299: Please give the durations of these sets if they are not roughly proportional to the original durations.\n- Line #381: Please give specs of CPU and GPU. Elapsed times are not meaningful enough if specs are unknown.\n", "review_3": "This paper presents a new representation for quantized MIDI files based on principles from a library called qparse. The approach is very interesting and the paper well-written. However, the paper also raises a number of questions that were left partly unanswered:\n- Partial Evaluation: The task is evaluated based on a single specific task without considering alternative approaches. The authors focus solely on this task, comparing only their own results. Evaluating the method against an established task and existing approaches would significantly strengthen the case for introducing a novel data representation.\n- Hard to interpret experiment tables: Table 1 contains a configuration study but the choice of bars and parameter size is not very intuitive. The choice of bars across tokenizations and models for example seems a bit arbitrary. Figure 4 and 5 also follow the same ambiguity, and raise the question of why the proposed representation using a transformer is compared to an LSTM, I would argue that some justification is missing.\n- Relevance of the Task: The task chosen for evaluation is too simplistic to effectively demonstrate a completely new representation, raising doubts about the general applicability of the new representation.\n- Generality of the representation: The new representation is very simple and clear but it does not cover all (or even broadly enough) musical durations such as composite durations, dotted notes, tuplets other than triplets, tied notes, etc.", "session": ["4"], "slack_channel": "p4-11-improved-symbolic-drum", "slides_pdf": "https://drive.google.com/file/d/1Mu5h0nYN0QFGlmtpty8vMscvMIXyRI5T/view?usp=drive_link", "title": "Improved symbolic drum style classification with grammar-based hierarchical representations", "video": "https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/view?usp=drive_link"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Improved symbolic drum style classification with grammar-based hierarchical representations
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Léo Géré (Cnam)*"
               class="text-muted"
            >Léo Géré (Cnam)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Nicolas Audebert (IGN)"
               class="text-muted"
            > Nicolas Audebert (IGN)</a
            >,
            
            <a href="papers.html?filter=authors&search= Philippe Rigaux (Cnam)"
               class="text-muted"
            > Philippe Rigaux (Cnam)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07U9F8BYB1" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p4-11-improved-symbolic-drum</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -&gt; representations of music; MIR fundamentals and methodology -&gt; symbolic music processing; Musical features and properties -&gt; representations of music"
                    class="text-secondary text-decoration-none"
            >Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -&gt; representations of music; MIR fundamentals and methodology -&gt; symbolic music processing; Musical features and properties -&gt; representations of music</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; automatic classification"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; automatic classification</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://drive.google.com/file/d/1Mu5h0nYN0QFGlmtpty8vMscvMIXyRI5T/preview?usp=drive_link" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The discussion of this paper was by and large positive. There is some question of the generality of the representation, and whether the rules extracted by qparse are sufficiently general to allow lots of different rhythmic possibilities. Its proximity to the qparse implementation is also concerning. The authors should spend some time improving their paper long the several directions identified by the reviewers.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Summary: The authors develop a new tree-based MIDI representation of drumming data (linear rhythmic tree) and test it with 3 other representational models and present their results.  The LRT does the best.</p>
<p>Strengths: This is a very good paper with reproducible results.  The major novel idea is the LRT.  I like that the new model is tested against other models as well, both with LSTMs and transformers.</p>
<p>Weaknesses: Nothing is incredibly ground-breaking or earth-shattering. The results are incremental.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper uses grammatical representation of MIDI files for style classification. Experimental results show that comparable or significantly better performance is achieved using less model parameters. The description and results are convincing, but explaining more details could make this paper better.</p>
<ul>
<li>Line #165: Please define “optimization-based music transcription systems”. Does it mean “a music transcription system that invokes optimization algorithm”? If yes, then deep models (line 168) are one type of such system, but in this sentence (While designed …) they are viewed as different.</li>
<li>Line #299: Please give the durations of these sets if they are not roughly proportional to the original durations.</li>
<li>Line #381: Please give specs of CPU and GPU. Elapsed times are not meaningful enough if specs are unknown.</li>
</ul></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper presents a new representation for quantized MIDI files based on principles from a library called qparse. The approach is very interesting and the paper well-written. However, the paper also raises a number of questions that were left partly unanswered:
- Partial Evaluation: The task is evaluated based on a single specific task without considering alternative approaches. The authors focus solely on this task, comparing only their own results. Evaluating the method against an established task and existing approaches would significantly strengthen the case for introducing a novel data representation.
- Hard to interpret experiment tables: Table 1 contains a configuration study but the choice of bars and parameter size is not very intuitive. The choice of bars across tokenizations and models for example seems a bit arbitrary. Figure 4 and 5 also follow the same ambiguity, and raise the question of why the proposed representation using a transformer is compared to an LSTM, I would argue that some justification is missing.
- Relevance of the Task: The task chosen for evaluation is too simplistic to effectively demonstrate a completely new representation, raising doubts about the general applicability of the new representation.
- Generality of the representation: The new representation is very simple and clear but it does not cover all (or even broadly enough) musical durations such as composite durations, dotted notes, tuplets other than triplets, tied notes, etc.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Thank you all for your reviews. Please find below the changes we have made to take them into account.</p>
<p>As proposed, we modified the title to reflect the fact that we evaluate our approach on drum music.</p>
<p>We added a reference to get further insight on formal grammar and context-free grammar in the related section.</p>
<p>We clarified the Figure 2 and the associated text in order to make it more easily understandable.</p>
<p>We also explained that model hyperparameters, including depth and width of the architectures, as well as the number of bars considered for each encoding were obtained using a hyperparameter search on the validation set.</p>
<p>Furthermore, we also added a note explaining that while we are indeed limited by some of qparse limitations, we aim at evaluating the rhythmic tree representation, regardless of how it has been built. Researching more robust ways of building such trees would be suited for some follow-up research.</p>
<p>In the supplementary material, we added some details on the grammar we used, and how the tree is built.</p>
<p>As proposed, we also added the confusion matrix of one of the models trained with the best architecture in the supplementary material, with some succinct interpretation.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;293&#39;, &#39;session&#39;: &#39;4&#39;, &#39;position&#39;: &#39;12&#39;, &#39;forum&#39;: &#39;293&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/14RvMqnf57mymZFZ5vRsTRTR4aCVFpJ5Z/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Improved symbolic drum style classification with grammar-based hierarchical representations&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Géré, Léo*&#39;, &#39; Audebert, Nicolas&#39;, &#39; Rigaux, Philippe&#39;], &#39;authors_and_affil&#39;: [&#39;Léo Géré (Cnam)*&#39;, &#39; Nicolas Audebert (IGN)&#39;, &#39; Philippe Rigaux (Cnam)&#39;], &#39;keywords&#39;: [&#39;Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -&gt; representations of music; MIR fundamentals and methodology -&gt; symbolic music processing; Musical features and properties -&gt; representations of music&#39;, &#39;MIR tasks -&gt; automatic classification&#39;], &#39;abstract&#39;: &#39;Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.&#39;, &#39;TLDR&#39;: &#39;Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;4&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07U9F8BYB1&#39;, &#39;slack_channel&#39;: &#39;p4-11-improved-symbolic-drum&#39;, &#39;slides_pdf&#39;: &#39;https://drive.google.com/file/d/1Mu5h0nYN0QFGlmtpty8vMscvMIXyRI5T/view?usp=drive_link&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;Summary: The authors develop a new tree-based MIDI representation of drumming data (linear rhythmic tree) and test it with 3 other representational models and present their results.  The LRT does the best.\n\nStrengths: This is a very good paper with reproducible results.  The major novel idea is the LRT.  I like that the new model is tested against other models as well, both with LSTMs and transformers.\n\nWeaknesses: Nothing is incredibly ground-breaking or earth-shattering. The results are incremental.&#39;, &#39;review_2&#39;: &#39;This paper uses grammatical representation of MIDI files for style classification. Experimental results show that comparable or significantly better performance is achieved using less model parameters. The description and results are convincing, but explaining more details could make this paper better.\n\n- Line #165: Please define “optimization-based music transcription systems”. Does it mean “a music transcription system that invokes optimization algorithm”? If yes, then deep models (line 168) are one type of such system, but in this sentence (While designed …) they are viewed as different.\n- Line #299: Please give the durations of these sets if they are not roughly proportional to the original durations.\n- Line #381: Please give specs of CPU and GPU. Elapsed times are not meaningful enough if specs are unknown.\n&#39;, &#39;review_3&#39;: &#39;This paper presents a new representation for quantized MIDI files based on principles from a library called qparse. The approach is very interesting and the paper well-written. However, the paper also raises a number of questions that were left partly unanswered:\n- Partial Evaluation: The task is evaluated based on a single specific task without considering alternative approaches. The authors focus solely on this task, comparing only their own results. Evaluating the method against an established task and existing approaches would significantly strengthen the case for introducing a novel data representation.\n- Hard to interpret experiment tables: Table 1 contains a configuration study but the choice of bars and parameter size is not very intuitive. The choice of bars across tokenizations and models for example seems a bit arbitrary. Figure 4 and 5 also follow the same ambiguity, and raise the question of why the proposed representation using a transformer is compared to an LSTM, I would argue that some justification is missing.\n- Relevance of the Task: The task chosen for evaluation is too simplistic to effectively demonstrate a completely new representation, raising doubts about the general applicability of the new representation.\n- Generality of the representation: The new representation is very simple and clear but it does not cover all (or even broadly enough) musical durations such as composite durations, dotted notes, tuplets other than triplets, tied notes, etc.&#39;, &#39;meta_review&#39;: &#39;The discussion of this paper was by and large positive. There is some question of the generality of the representation, and whether the rules extracted by qparse are sufficiently general to allow lots of different rhythmic possibilities. Its proximity to the qparse implementation is also concerning. The authors should spend some time improving their paper long the several directions identified by the reviewers.&#39;, &#39;author_changes&#39;: &#39;Thank you all for your reviews. Please find below the changes we have made to take them into account.\n\nAs proposed, we modified the title to reflect the fact that we evaluate our approach on drum music.\n\nWe added a reference to get further insight on formal grammar and context-free grammar in the related section.\n\nWe clarified the Figure 2 and the associated text in order to make it more easily understandable.\n\nWe also explained that model hyperparameters, including depth and width of the architectures, as well as the number of bars considered for each encoding were obtained using a hyperparameter search on the validation set.\n\nFurthermore, we also added a note explaining that while we are indeed limited by some of qparse limitations, we aim at evaluating the rhythmic tree representation, regardless of how it has been built. Researching more robust ways of building such trees would be suited for some follow-up research.\n\nIn the supplementary material, we added some details on the grammar we used, and how the tree is built.\n\nAs proposed, we also added the confusion matrix of one of the models trained with the best architecture in the supplementary material, with some succinct interpretation.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>