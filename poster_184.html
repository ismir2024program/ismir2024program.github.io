

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Yu-Hua Chen (NTU)*"
               class="text-muted"
            >Yu-Hua Chen (NTU)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Yen-Tung Yeh (National Taiwan University)"
               class="text-muted"
            > Yen-Tung Yeh (National Taiwan University)</a
            >,
            
            <a href="papers.html?filter=authors&search= Yuan-Chiao Cheng  (Positive Grid)"
               class="text-muted"
            > Yuan-Chiao Cheng  (Positive Grid)</a
            >,
            
            <a href="papers.html?filter=authors&search= Jui-Te Wu (Positive Grid)"
               class="text-muted"
            > Jui-Te Wu (Positive Grid)</a
            >,
            
            <a href="papers.html?filter=authors&search=  Yu-Hsiang Ho (Positive Grid )"
               class="text-muted"
            >  Yu-Hsiang Ho (Positive Grid )</a
            >,
            
            <a href="papers.html?filter=authors&search= Jyh-Shing Roger Jang (National Taiwan University)"
               class="text-muted"
            > Jyh-Shing Roger Jang (National Taiwan University)</a
            >,
            
            <a href="papers.html?filter=authors&search= Yi-Hsuan Yang (National Taiwan University)"
               class="text-muted"
            > Yi-Hsuan Yang (National Taiwan University)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MTPMMF" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p3-11-towards-zero-shot</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; music signal processing"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; music signal processing</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Applications -&gt; music composition, performance, and production; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; timbre, instrumentation, and singing voice"
                    class="text-secondary text-decoration-none"
            >Applications -&gt; music composition, performance, and production; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; timbre, instrumentation, and singing voice</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1LB9waLGDAqxl6v7OeYOtljs-yG4VwW0B/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1nL1IHXeDaJOGw2_R-N-6JIyCDSoW0pjs/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1ylPmxWdUuuKhEQn34Y79rLP3ET2SUQcr/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>Summary</p>
<p>In this work, the authors present a new method to perform zero-shot amplifier/tone transfer from one recording to another. Compared to past work, the method focuses on the amplifier/tone application and the idea of having a single network simultaneously model multiple amplified devices instead of a single device with multiple parameters. In this case, all devices are various forms of tone/amplifier fx. To train a style transfer model, a tone embedding module is learned via contrastive learning. This embedding module is then used to condition a second module to process audio. Evaluation is done via comparing a few within method configurations as well as comparing against a search-based method. The search-based method, however, is only used for testing out-of-domain tones.</p>
<p>Initial Reviews</p>
<p>Overall, the initial reviews for this work are mostly positive with 2x weak accept, 1 strong accept, and weak reject (meta). Positive points for the work include
•   R2 “ well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.”
•   R2 “well organized and the proposed method is described in sufficient detail.”
•   R3 “Personally, I appreciated the approach of this work. “
•   R4 “a strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. “
•   </p>
<p>Areas for improvement include
•   R2 “discussion on similarities and differences with related work would help to place the proposed method within the context of existing work.”
•   R2 “evaluation may be lacking to fully understand the efficacy of the proposed method”
•   R2 “no discussion on the computational efficiency of this method. Does this model enable real-time processing”
•   R3 “ the architecture used for the encoder "3.5 Implementation Details" is unclear… t of guesswork to anyone trying to reproduce or compare to this approach.. “
•   R4 “The authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder”
•   See also the initial meta review.</p>
<p>Discussion</p>
<p>During the discussion, there were some comments on issues that could cause a reject, but multiple reviewers commented seeking to hold their positive results to champion the current scores. </p>
<p>Recommendation</p>
<p>Due to the initial reviews and discussion of reviewers seeking to champion the work, we recommend to accept. Please see several issues in the initial meta review to address as well.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Summary</p>
<p>In this work, the authors propose a method for zero-shot guitar amplifier modeling. They achieve this by first training an encoder in a contrastive pre-task to extract features related to audio effect style or “tone” in the context of guitar amplifiers. They then produce a tone embedding from a reference signal, which is used as conditioning to a neural audio effect modeling network, called the generator. This generator is trained with a dataset of paired examples and a reconstruction loss. The convolutional model will process a clean signal to produce an output that has the same tone as the reference signal. </p>
<p>Strengths</p>
<p>The proposed method is well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.</p>
<p>The manuscript is well organized and the proposed method is described in sufficient detail. </p>
<p>Weaknesses</p>
<p>Further discussion on similarities and differences with related work would help to place the proposed method within the context of existing work.</p>
<p>The presented evaluation may be lacking to fully understand the efficacy of the proposed method. The first experiment demonstrates that ToneEmb conditioning can enable. While a listening test is not strictly required, even a simple perceptual study would significantly strengthen the conclusions of the work. </p>
<p>While the authors do provide a discussion on some potential limitations of their work, they fail to address some important aspects. For example, there is no discussion on the computational efficiency of this method. Does this model enable real-time processing? This is critical for guitar amplifier modeling applications. </p>
<p>The authors do not mention if they will provide open source code or datasets. While this is not strictly required per ISMIR guidelines, it would further strengthen the work.</p>
<p>Questions</p>
<p>The claim of “unpaired references” used to train the generator may be problematic. While the underlying content of the reference and the input (clean) may be differing, these training examples are “paired” in the sense that the data must be synthetically generated such that the guitar amplifier configuration is identical between the two recordings. In other contexts, “unpaired” data generally means </p>
<p>Recommendation </p>
<p>The proposed method is novel in that it combines two existing methods to enable a new task of zero-shot guitar amplifier modeling. While the presented evaluation demonstrates the efficacy of some aspects of the proposed method, the evaluation could be stronger, and hence limits the potential strength of the conclusions and overall generalization. As a result, this work is recommended for a weak acceptance. The authors are encouraged to strengthen the work through more rigorous evaluation which could include a perceptual study, the inclusion of more zero-shot baselines and a more detailed case study. </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>In this work, a tone embedding is developed using contrastive training. This embedding is used to train a decoder architecture that conditions on the tone embedding. The use of the embedding allows simultaneous modeling of different amp types within a single model.</p>
<p>Two leading conditioning strategies, concatenation and FiLM, are compared. The work is evaluated using both seen and unseen amplifiers. The authors identify current weaknesses in the system output, that the system fails to generate high-frequency components.</p>
<p>As pointed out by another reviewer, the architecture used for the encoder "3.5 Implementation Details" is unclear. This would give a lot of guesswork to anyone trying to reproduce or compare to this approach.</p>
<p>One point on Figure 4: the diagram suggested to me that the embedding failed to distinguish many different types of tones-as I see only two big clusters and quite a bit of overlap. That t-SNE fails to separate the amps makes me wonder how separable they are in the embedding space. Therefore, I wonder if the diversity of the modeled tones is low, if the embedding fails to distinguish between some tones, or both.</p>
<p>Personally, I appreciated the approach of this work. For future work, I would love to hear this approach applied to a larger range of production styles/effects, not just amplifier simulations.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This work addresses neural modelling of guitar amplifiers in the zero-shot setting. That is, it seeks to find a model that can generalise to amplifiers unseen at training time using a conditioning mechanism. This is achieved using a combination of a relatively standard GCN model for audio effect modeling, and an encoder trained with a contrastive (SimCLR) objective, where different pieces of input audio with the same processing constitute positive pairs.</p>
<p>In many ways this piece of work is overdue — neural modeling of amplifiers and distortion circuits has been predominantly focused to date on fitting a single device, with more complex models employing conditioning mechanisms to account for different effect parameters. Generalising to multiple different devices is more challenging, however, owing to the variety of different designs and hence behaviours, but is well suited to the zero-shot task formulation proposed here. This is, all round, a strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. Whilst not a complete solution to the stated problem, this is a clear step towards it and the paper both acknowledges the current limitations and proposes viable directions for future work.</p>
<p>The authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder, describing it instead simply as a “contemporary audio encoder”. Given the industrial collaboration they report, I assume this is simply an IP issue. If this is the case, however, it should be directly and clearly acknowledged in the text, rather than ambiguously omitting certain details. I hope the authors will make this change in the camera-ready version.</p>
<p>Otherwise, given the clear merits of this paper I am very happy to recommend this work for acceptance.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We would like to thank our reviewers for their valuable feedback. Their comments have helped us further improve and strengthen our work.
In the camera-ready version of the paper, we made several updates to enhance clarity and improve the presentation of our results. We directly indicated the name of the collaboration company Positive Grid in sections 3.1, 3.2, and 3.4, and we added an acknowledgements section to clearly mention the collaboration. We included a citation regarding other zero-shot FX style transfer tasks that do not model effects on amplifiers.
We noted that the performance of tone embedding is expected to be influenced by the number of amplifiers used in training, and we plan to address this in future work by adding more amplifiers to the training set to facilitate a better understanding of tone transformation in the generator.
Regarding the computational efficiency of our method, we mentioned the number of parameters in our generator and identified the development of a plugin version as future work. In real-world guitar effect plugins, users may not have access to GPU resources, so writing a plugin version and testing its computational efficiency or cost is considered more feasible.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;184&#39;, &#39;session&#39;: &#39;3&#39;, &#39;position&#39;: &#39;12&#39;, &#39;forum&#39;: &#39;184&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1NP9o0FXcdpRSEbAPJP8hIPj-kIrqvLl4/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Chen, Yu-Hua*&#39;, &#39; Yeh, Yen-Tung&#39;, &#39; Cheng , Yuan-Chiao&#39;, &#39; Wu, Jui-Te&#39;, &#39; Ho,  Yu-Hsiang&#39;, &#39; Jang, Jyh-Shing Roger&#39;, &#39; Yang, Yi-Hsuan&#39;], &#39;authors_and_affil&#39;: [&#39;Yu-Hua Chen (NTU)*&#39;, &#39; Yen-Tung Yeh (National Taiwan University)&#39;, &#39; Yuan-Chiao Cheng  (Positive Grid)&#39;, &#39; Jui-Te Wu (Positive Grid)&#39;, &#39;  Yu-Hsiang Ho (Positive Grid )&#39;, &#39; Jyh-Shing Roger Jang (National Taiwan University)&#39;, &#39; Yi-Hsuan Yang (National Taiwan University)&#39;], &#39;keywords&#39;: [&#39;MIR fundamentals and methodology -&gt; music signal processing&#39;, &#39;Applications -&gt; music composition, performance, and production; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; timbre, instrumentation, and singing voice&#39;], &#39;abstract&#39;: &#39;The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.&#39;, &#39;TLDR&#39;: &#39;The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1nL1IHXeDaJOGw2_R-N-6JIyCDSoW0pjs/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;3&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1LB9waLGDAqxl6v7OeYOtljs-yG4VwW0B/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1ylPmxWdUuuKhEQn34Y79rLP3ET2SUQcr/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MTPMMF&#39;, &#39;slack_channel&#39;: &#39;p3-11-towards-zero-shot&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;Summary\n\nIn this work, the authors propose a method for zero-shot guitar amplifier modeling. They achieve this by first training an encoder in a contrastive pre-task to extract features related to audio effect style or “tone” in the context of guitar amplifiers. They then produce a tone embedding from a reference signal, which is used as conditioning to a neural audio effect modeling network, called the generator. This generator is trained with a dataset of paired examples and a reconstruction loss. The convolutional model will process a clean signal to produce an output that has the same tone as the reference signal. \n\nStrengths\n\nThe proposed method is well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.\n\nThe manuscript is well organized and the proposed method is described in sufficient detail. \n\nWeaknesses\n\nFurther discussion on similarities and differences with related work would help to place the proposed method within the context of existing work.\n\nThe presented evaluation may be lacking to fully understand the efficacy of the proposed method. The first experiment demonstrates that ToneEmb conditioning can enable. While a listening test is not strictly required, even a simple perceptual study would significantly strengthen the conclusions of the work. \n\nWhile the authors do provide a discussion on some potential limitations of their work, they fail to address some important aspects. For example, there is no discussion on the computational efficiency of this method. Does this model enable real-time processing? This is critical for guitar amplifier modeling applications. \n\nThe authors do not mention if they will provide open source code or datasets. While this is not strictly required per ISMIR guidelines, it would further strengthen the work.\n\nQuestions\n\nThe claim of “unpaired references” used to train the generator may be problematic. While the underlying content of the reference and the input (clean) may be differing, these training examples are “paired” in the sense that the data must be synthetically generated such that the guitar amplifier configuration is identical between the two recordings. In other contexts, “unpaired” data generally means \n\nRecommendation \n\nThe proposed method is novel in that it combines two existing methods to enable a new task of zero-shot guitar amplifier modeling. While the presented evaluation demonstrates the efficacy of some aspects of the proposed method, the evaluation could be stronger, and hence limits the potential strength of the conclusions and overall generalization. As a result, this work is recommended for a weak acceptance. The authors are encouraged to strengthen the work through more rigorous evaluation which could include a perceptual study, the inclusion of more zero-shot baselines and a more detailed case study. &#39;, &#39;review_2&#39;: &#39;In this work, a tone embedding is developed using contrastive training. This embedding is used to train a decoder architecture that conditions on the tone embedding. The use of the embedding allows simultaneous modeling of different amp types within a single model.\n\nTwo leading conditioning strategies, concatenation and FiLM, are compared. The work is evaluated using both seen and unseen amplifiers. The authors identify current weaknesses in the system output, that the system fails to generate high-frequency components.\n\nAs pointed out by another reviewer, the architecture used for the encoder &#34;3.5 Implementation Details&#34; is unclear. This would give a lot of guesswork to anyone trying to reproduce or compare to this approach.\n\nOne point on Figure 4: the diagram suggested to me that the embedding failed to distinguish many different types of tones-as I see only two big clusters and quite a bit of overlap. That t-SNE fails to separate the amps makes me wonder how separable they are in the embedding space. Therefore, I wonder if the diversity of the modeled tones is low, if the embedding fails to distinguish between some tones, or both.\n\nPersonally, I appreciated the approach of this work. For future work, I would love to hear this approach applied to a larger range of production styles/effects, not just amplifier simulations.&#39;, &#39;review_3&#39;: &#39;This work addresses neural modelling of guitar amplifiers in the zero-shot setting. That is, it seeks to find a model that can generalise to amplifiers unseen at training time using a conditioning mechanism. This is achieved using a combination of a relatively standard GCN model for audio effect modeling, and an encoder trained with a contrastive (SimCLR) objective, where different pieces of input audio with the same processing constitute positive pairs.\n\nIn many ways this piece of work is overdue — neural modeling of amplifiers and distortion circuits has been predominantly focused to date on fitting a single device, with more complex models employing conditioning mechanisms to account for different effect parameters. Generalising to multiple different devices is more challenging, however, owing to the variety of different designs and hence behaviours, but is well suited to the zero-shot task formulation proposed here. This is, all round, a strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. Whilst not a complete solution to the stated problem, this is a clear step towards it and the paper both acknowledges the current limitations and proposes viable directions for future work.\n\nThe authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder, describing it instead simply as a “contemporary audio encoder”. Given the industrial collaboration they report, I assume this is simply an IP issue. If this is the case, however, it should be directly and clearly acknowledged in the text, rather than ambiguously omitting certain details. I hope the authors will make this change in the camera-ready version.\n\nOtherwise, given the clear merits of this paper I am very happy to recommend this work for acceptance.&#39;, &#39;meta_review&#39;: &#39;Summary\n\nIn this work, the authors present a new method to perform zero-shot amplifier/tone transfer from one recording to another. Compared to past work, the method focuses on the amplifier/tone application and the idea of having a single network simultaneously model multiple amplified devices instead of a single device with multiple parameters. In this case, all devices are various forms of tone/amplifier fx. To train a style transfer model, a tone embedding module is learned via contrastive learning. This embedding module is then used to condition a second module to process audio. Evaluation is done via comparing a few within method configurations as well as comparing against a search-based method. The search-based method, however, is only used for testing out-of-domain tones.\n\n\nInitial Reviews\n\n\nOverall, the initial reviews for this work are mostly positive with 2x weak accept, 1 strong accept, and weak reject (meta). Positive points for the work include\n•\tR2 “ well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.”\n•\tR2 “well organized and the proposed method is described in sufficient detail.”\n•\tR3 “Personally, I appreciated the approach of this work. “\n•\tR4 “a strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. “\n•\t\n\nAreas for improvement include\n•\tR2 “discussion on similarities and differences with related work would help to place the proposed method within the context of existing work.”\n•\tR2 “evaluation may be lacking to fully understand the efficacy of the proposed method”\n•\tR2 “no discussion on the computational efficiency of this method. Does this model enable real-time processing”\n•\tR3 “ the architecture used for the encoder &#34;3.5 Implementation Details&#34; is unclear… t of guesswork to anyone trying to reproduce or compare to this approach.. “\n•\tR4 “The authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder”\n•\tSee also the initial meta review.\n\n\nDiscussion\n\nDuring the discussion, there were some comments on issues that could cause a reject, but multiple reviewers commented seeking to hold their positive results to champion the current scores. \n\n\nRecommendation\n\nDue to the initial reviews and discussion of reviewers seeking to champion the work, we recommend to accept. Please see several issues in the initial meta review to address as well.\n&#39;, &#39;author_changes&#39;: &#39;We would like to thank our reviewers for their valuable feedback. Their comments have helped us further improve and strengthen our work.\nIn the camera-ready version of the paper, we made several updates to enhance clarity and improve the presentation of our results. We directly indicated the name of the collaboration company Positive Grid in sections 3.1, 3.2, and 3.4, and we added an acknowledgements section to clearly mention the collaboration. We included a citation regarding other zero-shot FX style transfer tasks that do not model effects on amplifiers.\nWe noted that the performance of tone embedding is expected to be influenced by the number of amplifiers used in training, and we plan to address this in future work by adding more amplifiers to the training set to facilitate a better understanding of tone transformation in the generator.\nRegarding the computational efficiency of our method, we mentioned the number of parameters in our generator and identified the development of a plugin version as future work. In real-world guitar effect plugins, users may not have access to GPU resources, so writing a plugin version and testing its computational efficiency or cost is considered more feasible.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>