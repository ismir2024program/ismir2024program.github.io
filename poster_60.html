

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Composer&#39;s Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Malandro, Martin E*"
               class="text-muted"
            >Malandro, Martin E*</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07USGDV12Q" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p3-09-composer-s-assistant</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music"
                    class="text-secondary text-decoration-none"
            >Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Creativity -&gt; human-ai co-creativity; MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music generation"
                    class="text-secondary text-decoration-none"
            >Creativity -&gt; human-ai co-creativity; MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music generation</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1tjF6ljH2xbNTv6enbQfeU4UecpR-C3vH/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1yI-Ig00wI16BRqVhS7cqLuLwPHqXoiNy/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1PDuCcmOrjiQU0Gxzm0JIy3C6jCases84/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The reviewers broadly appreciate the contribution of a generative model integrated into a DAW. While I think the empirical evaluation is somewhat weak, this is counterbalanced by the relevance of the artifact. The system will be released and open source, making this a valuable baseline for future work on DAW-integrated models.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Strengths
I highly recognize this work for its wide range of controls that
- focus on different aspects of music, including pitch, rhythm, density and novelty, which composers would likely find important and useful; and,
- allow very high flexibility, i.e., users can choose whichever control to apply, on arbitrary track or measure.</p>
<p>I also thank the authors a lot for releasing their model/implementation as part of a DAW, which increases the system's potential real-world impact.</p>
<p>Weaknesses
(W1) Somewhat insufficient ML technical descriptions --
The authors put most of the technical details in the appendix. To enhance readers' understanding of the method, I would recommend including at least the following content in the main text:
- How does the model take care of future context? Is it by reordering, or placing future context on the encoder side?
- How are the control tokens and tokens to be generated ordered/arranged?
- What are the differences/advancements compared to REAPER v1? (The improvement seems sizable from the numbers in Table 1.)</p>
<p>(W2) Misleading plot about control effectiveness (Figure 5) --
I find this plot a bit difficult to understand, particularly because the control signals/levels were not shown. Perhaps a better presentation is to display generated examples conditioned on different note density levels.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p><strong>strengths</strong>:</p>
<ul>
<li>strong objective evaluation of the improvements and controls added to the infilling model</li>
<li>authors think about deep learning models (and the way we should control them) from a co-creative standpoint, developing their models and conditioning techniques guided by the interaction needs of a composer working with MIDI in a DAW,  co-creating with a generative model.</li>
<li>the authors contribute a fully open-source ecosystem with source code, pretrained models, and REAPER scripts for incorporating their system into a DAW. </li>
</ul>
<p><strong>weaknesses:</strong> </p>
<ul>
<li>would have been good to know about the musical background of the volunteers in the subjective evaluation.</li>
</ul></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper describes additional features to REAPER Infiller incorporating user controls (pitch, rhythm, horizontal and vertical note density). The methods and the model are well evaluated along with subjective evaluations.</p>
<p>While the paper is well written and most details are properly presented, it builds on previous work (RI). A section to review RI itself would have been helpful for an uninitiated reader. However, lines 280-289 lends some insight into the model architecture, so I am inclined to believe that the text in this paper is sufficient to understand the method.</p>
<p>Overall I feel this work is an important addition to RI and thus should be published. </p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>-De-anonymized title and text.</p>
<p>-Added/discussed the 3 references suggested by Reviewer 2 and Meta-Reviewer 1.</p>
<p>-Two reviewers felt that Figure 5 was difficult to understand. Figure 5 has been replaced with a new figure (still Figure 5) that gives more information, and the text referring to Figure 5 has been rewritten.</p>
<p>-"How does the model take care of future context?...", "How are the control tokens and tokens to be generated ordered/arranged?"</p>
<p>It places future context on the encoder side. (Think of an input as being a page of sheet music with some track-measures masked. The model can "see" all unmasked notes on the page before writing any notes.) The paper has been updated in Fig 2 and Sec 4 to make this more clear. Sec 4 was also updated to describe where the control tokens are placed.</p>
<p>-"In Table 1, how much of the improvement of CA2 is attributable to general improvements of the CA model, vs. the greater control signal...?"</p>
<p>There were some general improvements to the CA codebase (primarily around training example generation), as well as improvements to the training dataset (some additional CC0 files and some reweighting of the training data) that we did not feel were worth discussing in the paper. To ensure we made a fair comparison, the CA model in the paper is actually a retrained CA model that incorporates these improvements. Hence, the numbers in Table 1 demonstrate exactly the improvement derived from the greater control signal.</p>
<p>The original CA model scores are as follows:</p>
<p>F1 50.63; 29.98; 53.35
Precision 52.21; 33.29; 55.16
Recall 49.67; 29.38; 52.76
PCHE difference 33.92; 52.77; 33.29
Groove sim 97.85; 96.17; 97.91</p>
<p>Roughly, general improvements increased performance by 1-2 points, and the remaining 20-40 points of improvement in the paper come from the increased control signal from the ground truth.</p>
<p>-Rewrote Sec 5.3 for clarity and discussion of subjective results.</p>
<p>-Removed a few sentences from Sections 1 and 2 to make room for the above changes.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;60&#39;, &#39;session&#39;: &#39;3&#39;, &#39;position&#39;: &#39;10&#39;, &#39;forum&#39;: &#39;60&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1GIhTiaj-zJipZdmgeIm2H8uoBPfU77vZ/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#34;Composer&#39;s Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control&#34;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Malandro, Martin E*&#39;], &#39;authors_and_affil&#39;: [&#39;Martin E Malandro (Sam Houston State University)*&#39;], &#39;keywords&#39;: [&#39;Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music&#39;, &#39;Creativity -&gt; human-ai co-creativity; MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music generation&#39;], &#39;abstract&#39;: &#34;We introduce Composer&#39;s Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer&#39;s Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system&#39;s outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.&#34;, &#39;TLDR&#39;: &#34;We introduce Composer&#39;s Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer&#39;s Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system&#39;s outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.&#34;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1yI-Ig00wI16BRqVhS7cqLuLwPHqXoiNy/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;3&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1tjF6ljH2xbNTv6enbQfeU4UecpR-C3vH/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1PDuCcmOrjiQU0Gxzm0JIy3C6jCases84/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07USGDV12Q&#39;, &#39;slack_channel&#39;: &#39;p3-09-composer-s-assistant&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#34;Strengths\nI highly recognize this work for its wide range of controls that\n- focus on different aspects of music, including pitch, rhythm, density and novelty, which composers would likely find important and useful; and,\n- allow very high flexibility, i.e., users can choose whichever control to apply, on arbitrary track or measure.\n\nI also thank the authors a lot for releasing their model/implementation as part of a DAW, which increases the system&#39;s potential real-world impact.\n\nWeaknesses\n(W1) Somewhat insufficient ML technical descriptions --\nThe authors put most of the technical details in the appendix. To enhance readers&#39; understanding of the method, I would recommend including at least the following content in the main text:\n- How does the model take care of future context? Is it by reordering, or placing future context on the encoder side?\n- How are the control tokens and tokens to be generated ordered/arranged?\n- What are the differences/advancements compared to REAPER v1? (The improvement seems sizable from the numbers in Table 1.)\n\n(W2) Misleading plot about control effectiveness (Figure 5) --\nI find this plot a bit difficult to understand, particularly because the control signals/levels were not shown. Perhaps a better presentation is to display generated examples conditioned on different note density levels.&#34;, &#39;review_2&#39;: &#39;**strengths**:\n\n- strong objective evaluation of the improvements and controls added to the infilling model\n- authors think about deep learning models (and the way we should control them) from a co-creative standpoint, developing their models and conditioning techniques guided by the interaction needs of a composer working with MIDI in a DAW,  co-creating with a generative model.\n-  the authors contribute a fully open-source ecosystem with source code, pretrained models, and REAPER scripts for incorporating their system into a DAW. \n\n**weaknesses:** \n\n- would have been good to know about the musical background of the volunteers in the subjective evaluation.&#39;, &#39;review_3&#39;: &#39;This paper describes additional features to REAPER Infiller incorporating user controls (pitch, rhythm, horizontal and vertical note density). The methods and the model are well evaluated along with subjective evaluations.\n\nWhile the paper is well written and most details are properly presented, it builds on previous work (RI). A section to review RI itself would have been helpful for an uninitiated reader. However, lines 280-289 lends some insight into the model architecture, so I am inclined to believe that the text in this paper is sufficient to understand the method.\n\nOverall I feel this work is an important addition to RI and thus should be published. &#39;, &#39;meta_review&#39;: &#39;The reviewers broadly appreciate the contribution of a generative model integrated into a DAW. While I think the empirical evaluation is somewhat weak, this is counterbalanced by the relevance of the artifact. The system will be released and open source, making this a valuable baseline for future work on DAW-integrated models.&#39;, &#39;author_changes&#39;: &#39;-De-anonymized title and text.\n\n-Added/discussed the 3 references suggested by Reviewer 2 and Meta-Reviewer 1.\n\n-Two reviewers felt that Figure 5 was difficult to understand. Figure 5 has been replaced with a new figure (still Figure 5) that gives more information, and the text referring to Figure 5 has been rewritten.\n\n-&#34;How does the model take care of future context?...&#34;, &#34;How are the control tokens and tokens to be generated ordered/arranged?&#34;\n\nIt places future context on the encoder side. (Think of an input as being a page of sheet music with some track-measures masked. The model can &#34;see&#34; all unmasked notes on the page before writing any notes.) The paper has been updated in Fig 2 and Sec 4 to make this more clear. Sec 4 was also updated to describe where the control tokens are placed.\n\n-&#34;In Table 1, how much of the improvement of CA2 is attributable to general improvements of the CA model, vs. the greater control signal...?&#34;\n\nThere were some general improvements to the CA codebase (primarily around training example generation), as well as improvements to the training dataset (some additional CC0 files and some reweighting of the training data) that we did not feel were worth discussing in the paper. To ensure we made a fair comparison, the CA model in the paper is actually a retrained CA model that incorporates these improvements. Hence, the numbers in Table 1 demonstrate exactly the improvement derived from the greater control signal.\n\nThe original CA model scores are as follows:\n\nF1 50.63; 29.98; 53.35\nPrecision 52.21; 33.29; 55.16\nRecall 49.67; 29.38; 52.76\nPCHE difference 33.92; 52.77; 33.29\nGroove sim 97.85; 96.17; 97.91\n\nRoughly, general improvements increased performance by 1-2 points, and the remaining 20-40 points of improvement in the paper come from the increased control signal from the ground truth.\n\n-Rewrote Sec 5.3 for clarity and discussion of subjective results.\n\n-Removed a few sentences from Sections 1 and 2 to make room for the above changes.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>