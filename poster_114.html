

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Tan, Chih-Pin*"
               class="text-muted"
            >Tan, Chih-Pin*</a
            >,
            
            <a href="papers.html?filter=authors&search= Ai, Hsin"
               class="text-muted"
            > Ai, Hsin</a
            >,
            
            <a href="papers.html?filter=authors&search= Chang, Yi-Hsin"
               class="text-muted"
            > Chang, Yi-Hsin</a
            >,
            
            <a href="papers.html?filter=authors&search= Guan, Shuen-Huei"
               class="text-muted"
            > Guan, Shuen-Huei</a
            >,
            
            <a href="papers.html?filter=authors&search= Yang, Yi-Hsuan"
               class="text-muted"
            > Yang, Yi-Hsuan</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MNBZT3" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p4-07-picogen2-piano-cover</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Applications -&gt; music composition, performance, and production; Generative Tasks -&gt; transformations; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music generation; Musical features and properties -&gt; musical style and genre"
                    class="text-secondary text-decoration-none"
            >Applications -&gt; music composition, performance, and production; Generative Tasks -&gt; transformations; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music generation; Musical features and properties -&gt; musical style and genre</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks"
                    class="text-secondary text-decoration-none"
            >Generative Tasks</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1ZtVeAwZ4H40Ed2JSPt2uKGBJNjhMvXcP/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1YM75UhJ5geuLqXYH111Zk8vm9diDLPtd/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1GN7GywKGpF4j-DxhbAJey0_uLccSOJre/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>Based on the reveiwers and internal discussion, we would like to  voted for a weak acceptance. Please see R2 on the concern on evaluation, and please also do incorporate the important missing reference to deliver a complete study.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>The paper introduces a method for generating piano covers from pop songs using a transfer learning model that employs weakly-aligned data instead of strongly-aligned, note-remapped data. This method retains the musical integrity of the original piano compositions and addresses data inaccuracy issues due to forced synchronization in previous methods. The paper is overall well-written and informative, yet several aspects could be improved:</p>
<p>First, the proposed pairing function for weakly aligning the piano and song addresses the rhythmic distortion caused by note-remapping but relies heavily on the performance of the beat detection algorithm. The paper lacks an investigation into the accuracy and efficiency of this algorithm. Since 50% of the bars were removed due to mapping errors (mentioned in Line 303), this method results in a significant loss of data for training or testing the model. A better justification for choosing this method over others, such as midi-to-audio alignment algorithms, is needed.</p>
<p>Second, the use of SheetSage to extract "high-level" musical ideas from song audio raises some concerns. While SheetSage was primarily trained for melody extraction, which aligns with generating accurate melody contours for piano cover generation, it may not effectively handle tempo and accompaniment reconstruction. The SheetSage encoder could lead to unexpected information loss compared to using the MT3 encoder for processing song audio. This issue could be observed in the provided listening samples, where chords and tempo were reconstructed less effectively compared to melody.</p>
<p>Finally, considering that melody matching is a crucial criterion for evaluating the success of a piano cover system, I recommend re-evaluating the melody chroma accuracy after some post-processing to align the generation with the target. This would provide a more meaningful comparison with the Pop2Piano model. Given that the target (human composition) has a low MCA score, it is difficult to justify the performance of the proposed model without this adjustment.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>Good task direction! without relying on perfect alignment is crucial for the task. The methodology is clear and well-explained. The final fine-tuning to avoid errors from beat detection is intelligent.</p>
<p>My first comment is that i would remove from the title "transfer learning approach and". Although the contributions of the paper are both, the latter for me is a strongest contribution.</p>
<p>However, my criticisms are mainly about the task evaluation. The subjective metrics are significantly better than the objective ones, which is noteworthy. Considering there is no bias in the subjective tests, I believe an analysis of why the objective metrics perform poorly would strengthen the paper. It would also be beneficial to provide reasons for the musical dimensions that are not being considered, and perhaps even propose a new metric for the task.</p>
<p>Another point of concern is the significant cherry-picking in the examples provided. I would prefer to see the YouTube video,  "pop2song" and your main approach. Given that the dataset you compiled has fewer than 100 pairs, it would have been fairer to show everything. It is evident that the model does not perform like a human, but examining the cases where the algorithm fails is also interesting.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <h4>Strengths</h4>
<p>The paper "PIANO COVER GENERATION WITH TRANSFER LEARNING APPROACH AND WEAKLY ALIGNED DATA" presents a significant advancement in the field of automatic piano cover generation. One of the notable strengths of this work is its innovative use of transfer learning, which leverages pre-training on piano-only data followed by fine-tuning on weakly-aligned song-to-piano paired data. This approach effectively addresses the common issue of losing piano information during the synchronization process, which has plagued previous models that relied on strongly aligned data.</p>
<p>The methodology is robust, involving the use of a lead sheet transcription model as the encoder during the pre-training phase. This step ensures that the model learns high-level musical concepts rather than merely transcribing audio. By fine-tuning on weakly-aligned data, the model is able to retain the musical quality of the original piano performances, thereby producing more natural and musically coherent outputs. The experiments conducted across five music genres demonstrate the model's versatility and effectiveness, as it outperforms baseline models in both objective and subjective evaluations.</p>
<p>Moreover, the use of a decoder-only Transformer model that interleaves song audio and piano performance sequences enhances the temporal correspondence between the condition and target outputs. This architectural choice, coupled with the innovative training strategy, contributes to the high quality of the generated piano covers.</p>
<h4>Limitations</h4>
<p>Despite the strengths, the paper does have some limitations that should be addressed. One of the primary concerns is the lack of a direct comparison with the model presented in the arXiv paper "AUDIO-TO-SYMBOLIC ARRANGEMENT VIA CROSS-MODAL MUSIC REPRESENTATION LEARNING" (arXiv 2112.15110). This omission is significant, as the arXiv paper addresses a similar problem space using a cross-modal representation learning approach, which also aims to capture major audio information for symbolic music generation. A comparative analysis would provide a clearer benchmark for the performance of the proposed transfer learning model.</p>
<p>Furthermore, while the paper emphasizes the advantages of weakly-aligned data, it does not fully explore the potential limitations and challenges associated with this approach. For instance, the alignment errors between the piano segments and their corresponding song segments, even at a beat level, could introduce inconsistencies that affect the overall quality of the generated covers. </p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We appreciate the reviewers' feedback and have made the following changes in the final version:</p>
<ol>
<li>Added a comparison with Wang et al.'s work to provide more context and highlight our contributions.</li>
<li>Removed the figure illustrating the alignment algorithm.</li>
<li>Redrawn the system overview figure, incorporating more details about the training process.</li>
</ol>
<p>Regarding the limitations of MCA in reflecting output quality and the inconsistencies in weak alignment, we have included them in our discussion of future work to emphasize their importance for further research.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;114&#39;, &#39;session&#39;: &#39;4&#39;, &#39;position&#39;: &#39;08&#39;, &#39;forum&#39;: &#39;114&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1c8Y5WtQiVYHB4GSufL3261GJ2Y00-rWZ/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Tan, Chih-Pin*&#39;, &#39; Ai, Hsin&#39;, &#39; Chang, Yi-Hsin&#39;, &#39; Guan, Shuen-Huei&#39;, &#39; Yang, Yi-Hsuan&#39;], &#39;authors_and_affil&#39;: [&#39;Chih-Pin Tan (National Taiwan University)*&#39;, &#39; Hsin Ai (National Taiwan University)&#39;, &#39; Yi-Hsin Chang (National Taiwan University)&#39;, &#39; Shuen-Huei Guan (KKCompany Techonologies)&#39;, &#39; Yi-Hsuan Yang (National Taiwan University)&#39;], &#39;keywords&#39;: [&#39;Applications -&gt; music composition, performance, and production; Generative Tasks -&gt; transformations; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music generation; Musical features and properties -&gt; musical style and genre&#39;, &#39;Generative Tasks&#39;], &#39;abstract&#39;: &#39;Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.&#39;, &#39;TLDR&#39;: &#39;Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1YM75UhJ5geuLqXYH111Zk8vm9diDLPtd/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;4&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1ZtVeAwZ4H40Ed2JSPt2uKGBJNjhMvXcP/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1GN7GywKGpF4j-DxhbAJey0_uLccSOJre/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MNBZT3&#39;, &#39;slack_channel&#39;: &#39;p4-07-picogen2-piano-cover&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;The paper introduces a method for generating piano covers from pop songs using a transfer learning model that employs weakly-aligned data instead of strongly-aligned, note-remapped data. This method retains the musical integrity of the original piano compositions and addresses data inaccuracy issues due to forced synchronization in previous methods. The paper is overall well-written and informative, yet several aspects could be improved:\n\nFirst, the proposed pairing function for weakly aligning the piano and song addresses the rhythmic distortion caused by note-remapping but relies heavily on the performance of the beat detection algorithm. The paper lacks an investigation into the accuracy and efficiency of this algorithm. Since 50% of the bars were removed due to mapping errors (mentioned in Line 303), this method results in a significant loss of data for training or testing the model. A better justification for choosing this method over others, such as midi-to-audio alignment algorithms, is needed.\n\nSecond, the use of SheetSage to extract &#34;high-level&#34; musical ideas from song audio raises some concerns. While SheetSage was primarily trained for melody extraction, which aligns with generating accurate melody contours for piano cover generation, it may not effectively handle tempo and accompaniment reconstruction. The SheetSage encoder could lead to unexpected information loss compared to using the MT3 encoder for processing song audio. This issue could be observed in the provided listening samples, where chords and tempo were reconstructed less effectively compared to melody.\n\nFinally, considering that melody matching is a crucial criterion for evaluating the success of a piano cover system, I recommend re-evaluating the melody chroma accuracy after some post-processing to align the generation with the target. This would provide a more meaningful comparison with the Pop2Piano model. Given that the target (human composition) has a low MCA score, it is difficult to justify the performance of the proposed model without this adjustment.&#39;, &#39;review_2&#39;: &#39;Good task direction! without relying on perfect alignment is crucial for the task. The methodology is clear and well-explained. The final fine-tuning to avoid errors from beat detection is intelligent.\n\nMy first comment is that i would remove from the title &#34;transfer learning approach and&#34;. Although the contributions of the paper are both, the latter for me is a strongest contribution.\n\n\nHowever, my criticisms are mainly about the task evaluation. The subjective metrics are significantly better than the objective ones, which is noteworthy. Considering there is no bias in the subjective tests, I believe an analysis of why the objective metrics perform poorly would strengthen the paper. It would also be beneficial to provide reasons for the musical dimensions that are not being considered, and perhaps even propose a new metric for the task.\n\n\nAnother point of concern is the significant cherry-picking in the examples provided. I would prefer to see the YouTube video,  &#34;pop2song&#34; and your main approach. Given that the dataset you compiled has fewer than 100 pairs, it would have been fairer to show everything. It is evident that the model does not perform like a human, but examining the cases where the algorithm fails is also interesting.\n&#39;, &#39;review_3&#39;: &#39;#### Strengths\n\nThe paper &#34;PIANO COVER GENERATION WITH TRANSFER LEARNING APPROACH AND WEAKLY ALIGNED DATA&#34; presents a significant advancement in the field of automatic piano cover generation. One of the notable strengths of this work is its innovative use of transfer learning, which leverages pre-training on piano-only data followed by fine-tuning on weakly-aligned song-to-piano paired data. This approach effectively addresses the common issue of losing piano information during the synchronization process, which has plagued previous models that relied on strongly aligned data.\n\nThe methodology is robust, involving the use of a lead sheet transcription model as the encoder during the pre-training phase. This step ensures that the model learns high-level musical concepts rather than merely transcribing audio. By fine-tuning on weakly-aligned data, the model is able to retain the musical quality of the original piano performances, thereby producing more natural and musically coherent outputs. The experiments conducted across five music genres demonstrate the model\&#39;s versatility and effectiveness, as it outperforms baseline models in both objective and subjective evaluations.\n\nMoreover, the use of a decoder-only Transformer model that interleaves song audio and piano performance sequences enhances the temporal correspondence between the condition and target outputs. This architectural choice, coupled with the innovative training strategy, contributes to the high quality of the generated piano covers.\n\n#### Limitations\n\nDespite the strengths, the paper does have some limitations that should be addressed. One of the primary concerns is the lack of a direct comparison with the model presented in the arXiv paper &#34;AUDIO-TO-SYMBOLIC ARRANGEMENT VIA CROSS-MODAL MUSIC REPRESENTATION LEARNING&#34; (arXiv 2112.15110). This omission is significant, as the arXiv paper addresses a similar problem space using a cross-modal representation learning approach, which also aims to capture major audio information for symbolic music generation. A comparative analysis would provide a clearer benchmark for the performance of the proposed transfer learning model.\n\nFurthermore, while the paper emphasizes the advantages of weakly-aligned data, it does not fully explore the potential limitations and challenges associated with this approach. For instance, the alignment errors between the piano segments and their corresponding song segments, even at a beat level, could introduce inconsistencies that affect the overall quality of the generated covers. &#39;, &#39;meta_review&#39;: &#39;Based on the reveiwers and internal discussion, we would like to  voted for a weak acceptance. Please see R2 on the concern on evaluation, and please also do incorporate the important missing reference to deliver a complete study.&#39;, &#39;author_changes&#39;: &#34;We appreciate the reviewers&#39; feedback and have made the following changes in the final version:\n\n1. Added a comparison with Wang et al.&#39;s work to provide more context and highlight our contributions.\n2. Removed the figure illustrating the alignment algorithm.\n3. Redrawn the system overview figure, incorporating more details about the training process.\n\nRegarding the limitations of MCA in reflecting output quality and the inconsistencies in weak alignment, we have included them in our discussion of future work to emphasize their importance for further research.\n\n&#34;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">Â© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>