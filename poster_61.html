

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Content-based Controls for Music Large-scale Language Modeling
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Lin, Liwei*"
               class="text-muted"
            >Lin, Liwei*</a
            >,
            
            <a href="papers.html?filter=authors&search= Xia, Gus"
               class="text-muted"
            > Xia, Gus</a
            >,
            
            <a href="papers.html?filter=authors&search= Jiang, Junyan"
               class="text-muted"
            > Jiang, Junyan</a
            >,
            
            <a href="papers.html?filter=authors&search= Zhang, Yixiao"
               class="text-muted"
            > Zhang, Yixiao</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07UHCN1N8N" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p5-13-content-based-controls</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; music generation"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; music generation</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation"
                    class="text-secondary text-decoration-none"
            >Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \textit{We aim to further equip the models with direct and \textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \textit{Coco-Mulla}, a \textbf{co}ntent-based \textbf{co}ntrol method for \textbf{mu}sic \textbf{l}arge \textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\footnote{\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\footnote{\url{https://kikyo-16.github.io/coco-mulla/}.}.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1HutKNTlmekAao1OWMxx7J-4HFwD3EA0K/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1g1WMrdURcqIJpRyjJrpz4kpifCg57BZl/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1s9CKLuAQRybFBY1b20UyGI6-JLmKv9Lw/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>Reviewers agree that this paper is well written and clearly presented. Further, the code to be published along with the paper is a huge plus.</p>
<p>It is a logical approach to training controllable music generation with limited resources, using llama adaptors, and great to see a model that can take the range of modes of control that were presented. However, many reviewers point out that there is room for improvement in the evaluation. There are comments regarding the clarity around the description of the evaluation process, the presence of good baselines, and regarding limited insight into how useful each of the modes of control are to the model. Please take careful note of the each of the reviewer's comments and use them to address outstanding reviewers concerns.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This paper presents an approach for learning content-based controls for the large open-source text to music transformer model, musicGen. The controls come from an example piece of music, from which automatic chord recognition, music transcription, and audio codec features are computed. Results are presented in terms of chord recognition and beat detection accuracy, however, I found it difficult to put the results in context, as only upper-bound ground truth results were provided without any baseline or lower-bound comparisons.</p>
<p>Strengths:
- The proposed conditioning mechanism is clearly explained 
- Makes strong use of off-the-shelf transcription and source separation tools</p>
<p>Weakness:
- Some of the design choices in the proposed conditioning scheme seem arbitrary
- No baseline results (even unconditioned MusicGen) are included for chord and beat recognition accuracy
- I found it difficult to take away reusable insights about what worked and didn’t work in the proposed approach. For example, training a model without the acoustic representation input would have been insightful, as then someone could condition the model without an audio example. The large drop in performance between full conditioning and chord only conditioning make it seem like the model is cheating quite a bit with the audio and melody inputs. Since MusicGen was already conditioned using audio and melody inputs, this makes it seem like the proposed extension to chord conditioning doesn’t actually work in practice.</p>
<p>Specific comments:
- Table 1: It would be nice to have an example here where root and bass aren’t always the same, unless they’re required to always be the same, in which case, why do you need both?</p>
<ul>
<li>
<p>Section 4.2: It was unclear to me why the condition prefix tokens are the same length as the generated encodec tokens? Doesn’t the number of generated encodec tokens change over time?</p>
</li>
<li>
<p>Equations (8)-(13) the notation for the W matrices appears to be overloaded, e.g., the same W symbols are used for (8) and (11), but they must certainly mean different things?</p>
</li>
<li>
<p>End of Section 4.2, it would be nice to list the symbols or refer to the equation numbers  to specify exactly what the trainable parameters are. For example, I’m unclear as to what the “joint embedding” trainable parameters are.</p>
</li>
<li>
<p>End of Section 4.2, assigning a random text description during training. Why do this, as opposed to no text conditioning at all? It seems like this could very negatively impact fine-tuning, and makes it difficult to trust this work without exploring this aspect more.</p>
</li>
<li>
<p>Table 2: why not include chord and beat accuracy results for unconditioned musicgen as a lower bound?</p>
</li>
<li>
<p>Table 3: Why are the CLAP score results so much higher than in Table 2 and never discussed? Is this a typo?</p>
</li>
<li>
<p>Section 5.2: How are the 20s samples chosen? Randomly? Can they overlap? Without this information, I’m unclear what an epoch means?</p>
</li>
<li>
<p>Section 5.3: I’m confused about the different FAD scores. Why is * higher? Wouldn’t we expect * to be lower (i.e., better) since it includes the audio used for conditioning?</p>
</li>
<li>
<p>Section 5.4: The authors state that the approach “maintains text-control ability.” What is the evidence for this? It appears CLAP score drops quite a bit.</p>
</li>
<li>
<p>Section 5.4.1: The text in this section doesn’t seem to match what I see in Table 3.  As the number of trainable layers increases, Table 3 appears to show an increase in CLAP score, not a reduction as stated in the text. </p>
</li>
</ul></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper utilizes frame-wise chord, midi, and content representations as conditions for content-based controllable generation. It also proposes a Condition Adaptor to efficiently train large models with small datasets and employs an attention mechanism with condition prefixes to allow condition prefixes to effectively influence token prediction. The condition prefixes are learned through self-attention layers. As a result, the paper enables efficient content-based control using only 300 music tracks. Another new finding is that such content-based control further improves the fidelity of music (Table 2 FAD Score).
Despite these many new attempts and discoveries, the paper has some drawbacks:</p>
<ol>
<li>
<p>Lack of rigorous evaluation. The paper does not describe the inference method of the baseline MusicGen model. How did you infer MusicGen using the RWC-POP-100 data? Did you input chords, beat timing (i.e, [0.27, 0.88, 1.49, 2.06, 2.66, 3.26, 3.88, 4.49, 5.10]), and melody note of midi as textual representations separately? If you used text conditions such as the Main Category  of RWC-POP-100 (single word: pop), it would be meaning-less comparison. Furthermore, if this paper aims to strictly compare text vs. content-based generation, you should compare it to a model trained on natural language expressions of beat sequence text, chord sequences text, or melody note text.</p>
</li>
<li>
<p>Joint embedding. Can the concatenation and linear projection of different data be called joint embedding? According to the latest representation learning reference [1], defines joint embedding as "which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs." (in Chapter 2) The joint embedding mentioned in this paper seems closer to fusion or mixed representation.</p>
</li>
<li>
<p>Regarding the layer-wise functionality of the transformer layer claimed in line 359, it would be better to rigorously compare fine-tuning only the lower layers and fine-tuning only the higher layers separately. In this case, the influence of the fine-tuning parameters is added, lowering the validity of the message.</p>
</li>
<li>
<p>It is unclear whether the time-varying sequential conditions work because of the proposed condition prefix-based adaptor or simply due to the preserved sequence representation of condition. It is regrettable that a comparison with a sequence representation conditioned LoRA (the same conditioning method with MusicGen) was not reported.</p>
</li>
</ol>
<p>[1] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, MahmoudAssran et al. https://arxiv.org/pdf/2301.08243</p>
<hr />
<p>In light of this, I recommend a weak aceept, considering that the above limitations have little impact on content-based conditioning and can be addressed in camera-ready or follow-up research.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>The strengths of this paper is clear: (1) the idea of introducing musical context control for LLM based music generation systems (2) the use of LLaMA-Adapter type PEFT for this application (3) the presentation of methodology, implementation details are clear and solid.</p>
<p>The weaknesses for this paper are: (1) To make it more insightful, a research on the fine tuning dataset scale versus evaluation metrics can be helpful.
(2) Since nowadays music generation systems are usually not publicly available or easily reproducible due to data and compute, it's not possible to compare with other context based controllable systems such as diffusion based Music ControlNet or Mustango (which should be cited in the paper), but they should be cited, a comparison with Mustango under the same criteria is also a plus;<br />
(3) I'm interested in the masking scheme introduced in section 4.1.3, I'm not clear why only MIDI and acoustic embedding are masked and how would the ratio r will influence the final performance, there is no intuitive explanations or references for this justification;
(4) I wonder if the proposed system is able to "outpaint" on previous given controls so that improvise a bit and can remain consistency;
(5) In terms of writing, section 3 and be merged into one subsection of section 4.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Rephrase the methodology sec to make it clearer; Adding references in the related work sec; Correct typos in the exp sec.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;61&#39;, &#39;session&#39;: &#39;5&#39;, &#39;position&#39;: &#39;14&#39;, &#39;forum&#39;: &#39;61&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1G0Z008KDxw7aYe1eUq7Z1N53egh15jZT/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Content-based Controls for Music Large-scale Language Modeling&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Lin, Liwei*&#39;, &#39; Xia, Gus&#39;, &#39; Jiang, Junyan&#39;, &#39; Zhang, Yixiao&#39;], &#39;authors_and_affil&#39;: [&#39;Liwei Lin (New York University Shanghai)*&#39;, &#39; Gus Xia (New York University Shanghai)&#39;, &#39; Junyan Jiang (New York University Shanghai)&#39;, &#39; Yixiao Zhang (Queen Mary University of London)&#39;], &#39;keywords&#39;: [&#39;MIR tasks -&gt; music generation&#39;, &#39;Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; music synthesis and transformation&#39;], &#39;abstract&#39;: &#39;Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.&#39;, &#39;TLDR&#39;: &#39;Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1g1WMrdURcqIJpRyjJrpz4kpifCg57BZl/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;5&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1HutKNTlmekAao1OWMxx7J-4HFwD3EA0K/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1s9CKLuAQRybFBY1b20UyGI6-JLmKv9Lw/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UHCN1N8N&#39;, &#39;slack_channel&#39;: &#39;p5-13-content-based-controls&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#39;This paper presents an approach for learning content-based controls for the large open-source text to music transformer model, musicGen. The controls come from an example piece of music, from which automatic chord recognition, music transcription, and audio codec features are computed. Results are presented in terms of chord recognition and beat detection accuracy, however, I found it difficult to put the results in context, as only upper-bound ground truth results were provided without any baseline or lower-bound comparisons.\n\nStrengths:\n- The proposed conditioning mechanism is clearly explained \n- Makes strong use of off-the-shelf transcription and source separation tools\n\nWeakness:\n- Some of the design choices in the proposed conditioning scheme seem arbitrary\n- No baseline results (even unconditioned MusicGen) are included for chord and beat recognition accuracy\n- I found it difficult to take away reusable insights about what worked and didn’t work in the proposed approach. For example, training a model without the acoustic representation input would have been insightful, as then someone could condition the model without an audio example. The large drop in performance between full conditioning and chord only conditioning make it seem like the model is cheating quite a bit with the audio and melody inputs. Since MusicGen was already conditioned using audio and melody inputs, this makes it seem like the proposed extension to chord conditioning doesn’t actually work in practice.\n\nSpecific comments:\n- Table 1: It would be nice to have an example here where root and bass aren’t always the same, unless they’re required to always be the same, in which case, why do you need both?\n\n- Section 4.2: It was unclear to me why the condition prefix tokens are the same length as the generated encodec tokens? Doesn’t the number of generated encodec tokens change over time?\n\n- Equations (8)-(13) the notation for the W matrices appears to be overloaded, e.g., the same W symbols are used for (8) and (11), but they must certainly mean different things?\n\n- End of Section 4.2, it would be nice to list the symbols or refer to the equation numbers  to specify exactly what the trainable parameters are. For example, I’m unclear as to what the “joint embedding” trainable parameters are.\n\n- End of Section 4.2, assigning a random text description during training. Why do this, as opposed to no text conditioning at all? It seems like this could very negatively impact fine-tuning, and makes it difficult to trust this work without exploring this aspect more.\n\n- Table 2: why not include chord and beat accuracy results for unconditioned musicgen as a lower bound?\n\n- Table 3: Why are the CLAP score results so much higher than in Table 2 and never discussed? Is this a typo?\n\n- Section 5.2: How are the 20s samples chosen? Randomly? Can they overlap? Without this information, I’m unclear what an epoch means?\n\n- Section 5.3: I’m confused about the different FAD scores. Why is * higher? Wouldn’t we expect * to be lower (i.e., better) since it includes the audio used for conditioning?\n\n- Section 5.4: The authors state that the approach “maintains text-control ability.” What is the evidence for this? It appears CLAP score drops quite a bit.\n\n- Section 5.4.1: The text in this section doesn’t seem to match what I see in Table 3.  As the number of trainable layers increases, Table 3 appears to show an increase in CLAP score, not a reduction as stated in the text. &#39;, &#39;review_2&#39;: &#39;This paper utilizes frame-wise chord, midi, and content representations as conditions for content-based controllable generation. It also proposes a Condition Adaptor to efficiently train large models with small datasets and employs an attention mechanism with condition prefixes to allow condition prefixes to effectively influence token prediction. The condition prefixes are learned through self-attention layers. As a result, the paper enables efficient content-based control using only 300 music tracks. Another new finding is that such content-based control further improves the fidelity of music (Table 2 FAD Score).\nDespite these many new attempts and discoveries, the paper has some drawbacks:\n\n1. Lack of rigorous evaluation. The paper does not describe the inference method of the baseline MusicGen model. How did you infer MusicGen using the RWC-POP-100 data? Did you input chords, beat timing (i.e, [0.27, 0.88, 1.49, 2.06, 2.66, 3.26, 3.88, 4.49, 5.10]), and melody note of midi as textual representations separately? If you used text conditions such as the Main Category  of RWC-POP-100 (single word: pop), it would be meaning-less comparison. Furthermore, if this paper aims to strictly compare text vs. content-based generation, you should compare it to a model trained on natural language expressions of beat sequence text, chord sequences text, or melody note text.\n\n2. Joint embedding. Can the concatenation and linear projection of different data be called joint embedding? According to the latest representation learning reference [1], defines joint embedding as &#34;which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs.&#34; (in Chapter 2) The joint embedding mentioned in this paper seems closer to fusion or mixed representation.\n\n3. Regarding the layer-wise functionality of the transformer layer claimed in line 359, it would be better to rigorously compare fine-tuning only the lower layers and fine-tuning only the higher layers separately. In this case, the influence of the fine-tuning parameters is added, lowering the validity of the message.\n\n4. It is unclear whether the time-varying sequential conditions work because of the proposed condition prefix-based adaptor or simply due to the preserved sequence representation of condition. It is regrettable that a comparison with a sequence representation conditioned LoRA (the same conditioning method with MusicGen) was not reported.\n\n[1] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, MahmoudAssran et al. https://arxiv.org/pdf/2301.08243\n\n----------------------------------------\nIn light of this, I recommend a weak aceept, considering that the above limitations have little impact on content-based conditioning and can be addressed in camera-ready or follow-up research.&#39;, &#39;review_3&#39;: &#39;The strengths of this paper is clear: (1) the idea of introducing musical context control for LLM based music generation systems (2) the use of LLaMA-Adapter type PEFT for this application (3) the presentation of methodology, implementation details are clear and solid.\n\nThe weaknesses for this paper are: (1) To make it more insightful, a research on the fine tuning dataset scale versus evaluation metrics can be helpful.\n(2) Since nowadays music generation systems are usually not publicly available or easily reproducible due to data and compute, it\&#39;s not possible to compare with other context based controllable systems such as diffusion based Music ControlNet or Mustango (which should be cited in the paper), but they should be cited, a comparison with Mustango under the same criteria is also a plus;  \n(3) I\&#39;m interested in the masking scheme introduced in section 4.1.3, I\&#39;m not clear why only MIDI and acoustic embedding are masked and how would the ratio r will influence the final performance, there is no intuitive explanations or references for this justification;\n(4) I wonder if the proposed system is able to &#34;outpaint&#34; on previous given controls so that improvise a bit and can remain consistency;\n(5) In terms of writing, section 3 and be merged into one subsection of section 4.&#39;, &#39;meta_review&#39;: &#34;Reviewers agree that this paper is well written and clearly presented. Further, the code to be published along with the paper is a huge plus.\n\nIt is a logical approach to training controllable music generation with limited resources, using llama adaptors, and great to see a model that can take the range of modes of control that were presented. However, many reviewers point out that there is room for improvement in the evaluation. There are comments regarding the clarity around the description of the evaluation process, the presence of good baselines, and regarding limited insight into how useful each of the modes of control are to the model. Please take careful note of the each of the reviewer&#39;s comments and use them to address outstanding reviewers concerns.&#34;, &#39;author_changes&#39;: &#39;Rephrase the methodology sec to make it clearer; Adding references in the related work sec; Correct typos in the exp sec.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>