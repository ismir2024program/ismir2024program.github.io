

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Nercessian, Shahan*"
               class="text-muted"
            >Nercessian, Shahan*</a
            >,
            
            <a href="papers.html?filter=authors&search= Imort, Johannes"
               class="text-muted"
            > Imort, Johannes</a
            >,
            
            <a href="papers.html?filter=authors&search= Devis, Ninon"
               class="text-muted"
            > Devis, Ninon</a
            >,
            
            <a href="papers.html?filter=authors&search= Blang, Frederik"
               class="text-muted"
            > Blang, Frederik</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07UHCM7A9L" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p7-06-generating-sample-based</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; artistically-inspired generative tasks ; Generative Tasks -&gt; evaluation metrics"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; artistically-inspired generative tasks ; Generative Tasks -&gt; evaluation metrics</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; music and audio synthesis"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; music and audio synthesis</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1JSNa6kdSy1HbTaV90ziXODAfPvCMRS3W/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1cz6Qnzc3sW1F5EOUEFICXT1oEd6PMDyK/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1e2NU_vLLkHDZIDgUnV3coAUFXcYRkHDo/preview" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>All the reviewers agree that this is a novel and solid piece of work that should be presented at the conference.  The reviewers also mentioned a few points that the authors can take into account to further improve the quality of the paper.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This work addresses the task of generating a sampler instrument. That is, it applies a generative model to produce a set of timbrally coherent audio samples that can be mapped to a range of MIDI velocities and pitches in such a way that creates a playable instrument.</p>
<p>Orthogonal to my review of the work’s scientific merit, I want to commend the authors for focusing on a task that is of clear creative benefit to musicians, i.e. by creating playable instruments, as opposed to simple end-to-end generation of complete tracks. This is a valuable research direction and a strong example to the community of how work on generative AI can positively impact creative professionals.</p>
<p>It appears this paper is based on work previously presented at the NeurIPS workshop on machine learning for audio. While my view is that this is an excellent piece of work, it does mean that my recommendation really hinges on whether this ISMIR submission contains sufficient novelty to justify its acceptance as new work.</p>
<p>As far as I can tell from the authors stated contributions, the framing of the text-to-instrument task and the audio codec language model solution to the task are close to identical to what was presented in the workshop paper. However, the sample-to-instrument task appears to be novel. Further, the timbral consistency metric was used as a loss function in the workshop paper, but here it appears to be present only as a metric, and is no longer used for training. Further novel contributions include: (i) the adaptation to the average CLAP score metric; (ii) the three CLAP conditioning schemes, which account for the variance induced by pitch and velocity conditioning; and (iii) and the inclusion of experiments on non-autoregressive transformers. Further, the subjective evaluation is also an improvement on the workshop paper. In particular, the choice to use a MUSHRA-style test for the S2I task is arguably more robust.</p>
<p>I do note the absence of any particular discussion of the topic of sampler instrument design. Indeed, sample libraries and sampler instruments constitute a significant portion of the modern music technology industry, so I'm slightly surprised that the authors have not gone into any further detail about what developing a commercially viable sampler instrument actually entails. Considerations such as round robin sampling, separate attack/sustain samples, and the use of sample zones to trade off quality with disk IO and file size are all relevant, and should arguably play a role in determining whether or not the stated aim of the paper has been achieved.</p>
<p>Nonetheless, my overall impression is that this work contains sufficient novelty to be of value to the ISMIR community, and so I’m happy to recommend for acceptance.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper discusses the use of generative systems based on neural audio codec language models to address the task of generating sample-based instruments (i.e. generating several audio files corresponding to different musical notes with different pitches and velocities and with timbral consistency) given a text prompt or an audio example. To the best of our knowledge, the generation of sample-based instruments from text (T2I) or from audio (S2I) are new MIR tasks which are introduced in this paper. The authors provide an evaluation methodology which incorporates a novel metric to evaluate the timbral consistency of a set of sounds, which is a required quality of sample-based instrument sound sets. The authors also propose three variants of a system that address both T2I and S2I tasks, and carry out both quantitative and qualitative evaluations which result in the notion that the proposed systems can successfully propose solutions for the task but a trade-off exists between timbral consistency an expressivity (measured using FAD).</p>
<p>The paper is well structured and very well-written, with proper references  and providing great detail, however, it is not clear if code will be available to reproduce the results. Even though a link is provided to a companion website, this does seem to only include audio examples and no code so far.</p>
<p>Overall I think this is a relevant paper for the ISMIR community and therefore I recommend to accept it. What follows is a list of minor comments that authors could address to improve the paper:</p>
<ul>
<li>
<p>L74-80: Maybe rephrase these sentences? I think they are a bit confusing. I guess the point is to highlight the idea that using parametric synthesisers or DDSP-based approaches that would pre-define some parameters is out of the scope of the work because the authors consider this would severely limit the output space, but maybe this could be clarified?</p>
</li>
<li>
<p>L87: "We introduce the text-to-instrument..." Shouldn't "sample-to-instrument" task be also mentioned in this first point? Also the acronyms T2I and S2I should be first indicated here.</p>
</li>
<li>
<p>L107: Maybe start the paragraph with a connector? "The remaining of this paper is organized as follows. Section 2..."</p>
</li>
<li>
<p>L112: illustrate -&gt; illustrates (?)</p>
</li>
<li>
<p>L185: "The instrument family and source type (i.e., acoustic..." -&gt; I think a couple of examples of instrument families would be good  here.</p>
</li>
</ul></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>Strengths:
- The proposal of the T2I/S2I task is reasonably novel and would be of key interest to many at ISMIR
- The overall parameterization of the codec LMs are straightforward and well thought out, with the randomization of the CLAP conditioning in particular a useful insight for the task.</p>
<p>Weaknesses:
- The overall writing coherence and presentation is somewhat poor, which significantly impacts the ability to understand the paper in depth. In particular:
1) Notation for describing the underlying generative process for the codec LM (section 2.4) is rather uncharacteristic of previous work. While one can understand the desire to make notation generalizable between the AR and non-AR methods, the current notation runs is somewhat opaque and obfuscates parts of the generative process (i.e. that the AR model is trained through next-token prediction).
2) Most of Section 3 (and Figure 2) is extremely hard to parse. It is hard to tell what Figure 2 is referring to, given that it is both referenced as part of Section 3.2 and Section 3.3. As all definitions for each evaluation metric are exceedingly similar, the paper would be significantly improved by streamlining the legibility of this section and more effectively organizing the structure here.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Thank you for your acceptance and the insightful feedback on our work!</p>
<p>Now, we connect the paper to our preliminary work, which was a workshop presentation that was not officially published/archived as per mlforaudioworkshop.com. Relative to that, T2I is significantly expanded here (e.g., new CLAP conditioning variants, 2 new metrics, MAGNeT). While we are the first to use LMs for S2I, we refrain from claiming it as our own considering the cited works (DDSP, GANstrument).</p>
<p>While we acknowledge that generating specific parts of the fully assembled sample (e.g., attack/release samples) is indeed interesting, we consider the sampler design as industry-specific and omitted a discussion due to space constraints.</p>
<p>We aim to add more out-of-domain S2I examples to our demo page by ISMIR. Nonetheless, our existing test set results include samples and instruments not seen during training.</p>
<p>We have retained our notation, which we carefully considered prior to submission. With $\mathbf{x}_k(…)$, the args $(…)$ enable the selection of different waveforms $\mathbf{x}_k$. Equation 1 is defined to encapsulate both AR and non-AR processes. Although the equations may appear similar, they capture the subtleties of the various topics introduced in this paper, which are essential to our methodology.</p>
<p>Now, we note that DAC supports up to 9 codebooks. As listed in our future work, we have since fine-tuned DAC and trained corresponding LMs, and hope to provide examples on our site leading up to ISMIR.</p>
<p>Generally, we have addressed connectors, typos, and notation reminders, linking AR to next-token prediction for clarity. We add the T2I acronym upon its first use. We retain the original text in line 73-80 due to favorable comments from the meta-reviewer. While we aspire to provide code in the future, our current industry position precludes us from doing so at this time.</p>
<p>Thank you again for your constructive feedback and the opportunity to present our work at ISMIR 2024!</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;22&#39;, &#39;session&#39;: &#39;7&#39;, &#39;position&#39;: &#39;07&#39;, &#39;forum&#39;: &#39;22&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/19NV6asW0uyiNpY8JILdYGemXZ_T5vo53/view?usp=sharing&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Nercessian, Shahan*&#39;, &#39; Imort, Johannes&#39;, &#39; Devis, Ninon&#39;, &#39; Blang, Frederik&#39;], &#39;authors_and_affil&#39;: [&#39;Shahan Nercessian (Native Instruments)*&#39;, &#39; Johannes Imort (Native Instruments)&#39;, &#39; Ninon Devis (Native Instruments)&#39;, &#39; Frederik Blang (Native Instruments)&#39;], &#39;keywords&#39;: [&#39;Generative Tasks -&gt; artistically-inspired generative tasks ; Generative Tasks -&gt; evaluation metrics&#39;, &#39;Generative Tasks -&gt; music and audio synthesis&#39;], &#39;abstract&#39;: &#39;In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.&#39;, &#39;TLDR&#39;: &#39;In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1cz6Qnzc3sW1F5EOUEFICXT1oEd6PMDyK/view?usp=sharing&#39;, &#39;session&#39;: [&#39;7&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1JSNa6kdSy1HbTaV90ziXODAfPvCMRS3W/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1e2NU_vLLkHDZIDgUnV3coAUFXcYRkHDo/view?usp=sharing&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UHCM7A9L&#39;, &#39;slack_channel&#39;: &#39;p7-06-generating-sample-based&#39;, &#39;day&#39;: &#39;4&#39;, &#39;review_1&#39;: &#34;This work addresses the task of generating a sampler instrument. That is, it applies a generative model to produce a set of timbrally coherent audio samples that can be mapped to a range of MIDI velocities and pitches in such a way that creates a playable instrument.\n\nOrthogonal to my review of the work’s scientific merit, I want to commend the authors for focusing on a task that is of clear creative benefit to musicians, i.e. by creating playable instruments, as opposed to simple end-to-end generation of complete tracks. This is a valuable research direction and a strong example to the community of how work on generative AI can positively impact creative professionals.\n\nIt appears this paper is based on work previously presented at the NeurIPS workshop on machine learning for audio. While my view is that this is an excellent piece of work, it does mean that my recommendation really hinges on whether this ISMIR submission contains sufficient novelty to justify its acceptance as new work.\n\nAs far as I can tell from the authors stated contributions, the framing of the text-to-instrument task and the audio codec language model solution to the task are close to identical to what was presented in the workshop paper. However, the sample-to-instrument task appears to be novel. Further, the timbral consistency metric was used as a loss function in the workshop paper, but here it appears to be present only as a metric, and is no longer used for training. Further novel contributions include: (i) the adaptation to the average CLAP score metric; (ii) the three CLAP conditioning schemes, which account for the variance induced by pitch and velocity conditioning; and (iii) and the inclusion of experiments on non-autoregressive transformers. Further, the subjective evaluation is also an improvement on the workshop paper. In particular, the choice to use a MUSHRA-style test for the S2I task is arguably more robust.\n\nI do note the absence of any particular discussion of the topic of sampler instrument design. Indeed, sample libraries and sampler instruments constitute a significant portion of the modern music technology industry, so I&#39;m slightly surprised that the authors have not gone into any further detail about what developing a commercially viable sampler instrument actually entails. Considerations such as round robin sampling, separate attack/sustain samples, and the use of sample zones to trade off quality with disk IO and file size are all relevant, and should arguably play a role in determining whether or not the stated aim of the paper has been achieved.\n\nNonetheless, my overall impression is that this work contains sufficient novelty to be of value to the ISMIR community, and so I’m happy to recommend for acceptance.&#34;, &#39;review_2&#39;: &#39;This paper discusses the use of generative systems based on neural audio codec language models to address the task of generating sample-based instruments (i.e. generating several audio files corresponding to different musical notes with different pitches and velocities and with timbral consistency) given a text prompt or an audio example. To the best of our knowledge, the generation of sample-based instruments from text (T2I) or from audio (S2I) are new MIR tasks which are introduced in this paper. The authors provide an evaluation methodology which incorporates a novel metric to evaluate the timbral consistency of a set of sounds, which is a required quality of sample-based instrument sound sets. The authors also propose three variants of a system that address both T2I and S2I tasks, and carry out both quantitative and qualitative evaluations which result in the notion that the proposed systems can successfully propose solutions for the task but a trade-off exists between timbral consistency an expressivity (measured using FAD).\n\nThe paper is well structured and very well-written, with proper references  and providing great detail, however, it is not clear if code will be available to reproduce the results. Even though a link is provided to a companion website, this does seem to only include audio examples and no code so far.\n\nOverall I think this is a relevant paper for the ISMIR community and therefore I recommend to accept it. What follows is a list of minor comments that authors could address to improve the paper:\n\n* L74-80: Maybe rephrase these sentences? I think they are a bit confusing. I guess the point is to highlight the idea that using parametric synthesisers or DDSP-based approaches that would pre-define some parameters is out of the scope of the work because the authors consider this would severely limit the output space, but maybe this could be clarified?\n\n* L87: &#34;We introduce the text-to-instrument...&#34; Shouldn\&#39;t &#34;sample-to-instrument&#34; task be also mentioned in this first point? Also the acronyms T2I and S2I should be first indicated here.\n\n* L107: Maybe start the paragraph with a connector? &#34;The remaining of this paper is organized as follows. Section 2...&#34;\n\n* L112: illustrate -&gt; illustrates (?)\n\n* L185: &#34;The instrument family and source type (i.e., acoustic...&#34; -&gt; I think a couple of examples of instrument families would be good  here.&#39;, &#39;review_3&#39;: &#39;Strengths:\n- The proposal of the T2I/S2I task is reasonably novel and would be of key interest to many at ISMIR\n- The overall parameterization of the codec LMs are straightforward and well thought out, with the randomization of the CLAP conditioning in particular a useful insight for the task.\n\nWeaknesses:\n- The overall writing coherence and presentation is somewhat poor, which significantly impacts the ability to understand the paper in depth. In particular:\n1) Notation for describing the underlying generative process for the codec LM (section 2.4) is rather uncharacteristic of previous work. While one can understand the desire to make notation generalizable between the AR and non-AR methods, the current notation runs is somewhat opaque and obfuscates parts of the generative process (i.e. that the AR model is trained through next-token prediction).\n2) Most of Section 3 (and Figure 2) is extremely hard to parse. It is hard to tell what Figure 2 is referring to, given that it is both referenced as part of Section 3.2 and Section 3.3. As all definitions for each evaluation metric are exceedingly similar, the paper would be significantly improved by streamlining the legibility of this section and more effectively organizing the structure here.&#39;, &#39;meta_review&#39;: &#39;All the reviewers agree that this is a novel and solid piece of work that should be presented at the conference.  The reviewers also mentioned a few points that the authors can take into account to further improve the quality of the paper.&#39;, &#39;author_changes&#39;: &#39;Thank you for your acceptance and the insightful feedback on our work!\n\nNow, we connect the paper to our preliminary work, which was a workshop presentation that was not officially published/archived as per mlforaudioworkshop.com. Relative to that, T2I is significantly expanded here (e.g., new CLAP conditioning variants, 2 new metrics, MAGNeT). While we are the first to use LMs for S2I, we refrain from claiming it as our own considering the cited works (DDSP, GANstrument).\n\nWhile we acknowledge that generating specific parts of the fully assembled sample (e.g., attack/release samples) is indeed interesting, we consider the sampler design as industry-specific and omitted a discussion due to space constraints.\n\nWe aim to add more out-of-domain S2I examples to our demo page by ISMIR. Nonetheless, our existing test set results include samples and instruments not seen during training.\n\nWe have retained our notation, which we carefully considered prior to submission. With $\\mathbf{x}_k(…)$, the args $(…)$ enable the selection of different waveforms $\\mathbf{x}_k$. Equation 1 is defined to encapsulate both AR and non-AR processes. Although the equations may appear similar, they capture the subtleties of the various topics introduced in this paper, which are essential to our methodology.\n\nNow, we note that DAC supports up to 9 codebooks. As listed in our future work, we have since fine-tuned DAC and trained corresponding LMs, and hope to provide examples on our site leading up to ISMIR.\n\nGenerally, we have addressed connectors, typos, and notation reminders, linking AR to next-token prediction for clarity. We add the T2I acronym upon its first use. We retain the original text in line 73-80 due to favorable comments from the meta-reviewer. While we aspire to provide code in the future, our current industry position precludes us from doing so at this time.\n\nThank you again for your constructive feedback and the opportunity to present our work at ISMIR 2024!&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>