

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.", "abstract": "The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.", "author_changes": "We would like to thank all reviewers for their helpful comments and constructive feedback. \n\nWe have incorporated suggestions about the scope of the paper and additional detail about the tokenization method into the title and abstract (#1).\nAdditionally, we clarified the explanation of the conditioning and space tokens, moving them into Section 3.1.2. and Section 2.2, respectively (#4). \nThe tokenization section now includes more detail about handling polyphonic music, note ordering, and the encoding of musical time (#4 and meta #1).\nDataset statistics in Table 2 were recomputed in order to reconcile inconsistent preprocessing between different splits (#4).\nFurthermore, section 3.3 now contains additional information on how MusicXML scores were obtained for baseline methods (#4).\n\nDue to space restrictions, we were unable to insert a figure illustrating the tokenization in the paper (meta #1). However, clear code for all tokenization/detokenization steps will be available on GitHub, along with an illustration of a tokenization of a melody.\n\nWe also want to address our focus on classical music and the ASAP dataset (#3). To our knowledge, there are no other publicly available datasets which include paired performance-MIDI and MusicXML scores. CrestMusePEDB is the only other dataset in the literature with MusicXML scores, but it is smaller, not openly available, and also limited to classical music. To illustrate our method\u0027s generalization, the supplementary material includes examples of generations for out-of-genre music from in-the-wild performances. \n\nFinally, we fixed various typos and formatting issues (#4).\nTo adhere to the page limit, some changes required minor reworking of other text passages in the paper; however, no content was significantly altered.", "authors": ["Beyer, Tim*", " Dai, Angela"], "authors_and_affil": ["Tim Beyer (Technical University of Munich)*", " Angela Dai (Technical University of Munich)"], "channel_url": "https://ismir2024.slack.com/archives/C07UHCSSRSA", "day": "1", "keywords": ["MIR fundamentals and methodology -\u003e symbolic music processing; MIR tasks -\u003e music synthesis and transformation; Musical features and properties -\u003e rhythm, beat, tempo; Musical features and properties -\u003e structure, segmentation, and form", "MIR tasks -\u003e music transcription and annotation"], "long_presentation": "FALSE", "meta_review": "This paper describes end-to-end MIDI-to-score conversion, which has not been focused on in the MIR field.\n\nThe paper is written very well. The proposed method is technically solid and its effectiveness was clearly shown by the well-designed experiments.", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/10tX3QWkVV5ALUq3aD0QwXSJ4JP9dmD1B/view?usp=sharing", "poster_pdf": "https://drive.google.com/file/d/1X8aKfd0wZiXnLgeiCqNYOBWXzxHWIDLY/view?usp=drive_link", "review_1": "Overall, the paper is very clear and well-written. The methodological decisions are motivated, and the evaluation is rigorous, using standard metrics, comparing with the state-of-the-art works, and with an ablation study for key design decisions. In addition, the code and pre-trained models will be released upon publication.\n\nSince the dataset used for training and evaluation only includes piano performances (ASAP), I suggest including this aspect in the title (e.g., \"End-to-end piano performance-MIDI...\") and in the abstract.\n\n", "review_2": "This work presents an approach for obtaining a musical score from a performance MIDI file based on Transformers. The authors consider a reference dataset for the experiments and compare against existing approaches (both commercial and research-oriented) with several evaluation metrics and conclude the superiority of their proposal.\n\nThe manuscript is well written and sounds technical. Also, while the authors are commited to share the code and data if the work is accepted for publication, they also share as supplementary material to the conference some examples of the results that the approach may obtain.\n\nMy only concern in the work is the exclusive use of the ASAP dataset (as labeled data). In this sense, have the authors consider any other alternative dataset, at least for a particular experiment, to assess the generalization capabilities of the approach?", "review_3": "This paper proposed a new end-to-end approach to performance-MIDI to score conversion using the Transformers architecture. The proposed method is well-motivated (there is no one-to-one mapping between a performance MIDI sequence and a music score note sequence), achieves good results (beating the state-of-the-art), well-explained, and supported with rigorous experiments.\n\nApart from the key contributions stated in the paper\u0027s introduction, the biggest advantage of this paper, from my point of view, is the detailed technical considerations presented in the paper. The use of data (labelled and unpaired), the tokenization of the music score data (11 attributes derived from the MusicXML score, and the use of space token to fulfill beat-level alignment), the data augmentation (especially the duration jitter and onset jitter to simulate human performance), and the detailed ablation study makes the paper technically strong and provides plenty of useful insights to people working on related field. I especially like the way the paper describes data-related design choices by supporting them with comparisons to previous/other settings (e.g. Section 3.1.2 Unpaired data and Table 5). Furthermore, it\u0027s great to see the ethics statement which mentions future research for other music genres.\n\nOverall, I think it\u0027s a good paper that should be presented at ISMIR, which will have good audiences and can be very helpful to others working on related tasks.\n\nBelow are only some minor comments.\n\n* L27: \"significant downstream applications\" =\u003e significant number of downstream applications?\n* L147: \"Eq. (4)\" =\u003e Eq. (5)\n* L178: \"ml_j stores the preceding measure\u0027s length for downbeat notes or is set to false otherwise.\" =\u003e I\u0027m still not very sure what ml_j is precisely. Is a downbeat note a note those onset is on a downbeat? How is it related to the measure\u0027s length?\n* L205: \"an inner dimension of 3072 for the position-wise feed-forward network\" =\u003e It is said at the beginning of this paragraph that \"the backbone model follows the original architecture described by Vaswani et al.\" But the original feed-forward dimension is 2048.\n* L211: After reading this paragraph, it\u0027s still not clear to me what are the \"conditioning token\" and the \"space token\". These are made clear after I finished reading Section 3.1.2. It may be helpful to polish this paragraph a bit, and add a reference to Section 3.1.2.\n* L234: \"space token steam\" =\u003e stream\n* **Table 2**: The total number of distinct pieces, P-MIDI notes, and score notes are not equal to the corresponding sum over the train/valid/test splits.\n* Table 4: Which SOTA is it in the table? Add a reference?\n* Table 5: MIDI scores do not actually have stem directions. How are the stems obtained for the MIDI scores (e.g. by importing to MuseScore or Finale)? It will be helpful to mention that to improve reproducibility. Or to annotate it as a \"-\" indicating there is no stem prediction.\n* L387: How are the barlines predicted after removing the time signature module? And similarly, how are the predictions converted into MusicXML format for evaluation?\n* References: Some minor formatting issues for items [6, 19, 20, 25, 26, 27, 29, 40, 41], mostly related to capitalisation.\n", "session": ["2"], "slack_channel": "p2-15-end-to-end", "slides_pdf": "https://drive.google.com/file/d/1pUyUcFoY15nX2VCZt-JRn96v0JjrecFC/view?usp=drive_link", "title": "End-to-end Piano Performance-MIDI to Score Conversion with Transformers", "video": "https://drive.google.com/file/d/11SgGetPXGSIFCwFpgwiUIefA0Hy3jLjk/view?usp=drive_link"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            End-to-end Piano Performance-MIDI to Score Conversion with Transformers
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Tim Beyer (Technical University of Munich)*"
               class="text-muted"
            >Tim Beyer (Technical University of Munich)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Angela Dai (Technical University of Munich)"
               class="text-muted"
            > Angela Dai (Technical University of Munich)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UHCSSRSA" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p2-15-end-to-end</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; rhythm, beat, tempo; Musical features and properties -&gt; structure, segmentation, and form"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; rhythm, beat, tempo; Musical features and properties -&gt; structure, segmentation, and form</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; music transcription and annotation"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; music transcription and annotation</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/10tX3QWkVV5ALUq3aD0QwXSJ4JP9dmD1B/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1X8aKfd0wZiXnLgeiCqNYOBWXzxHWIDLY/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/11SgGetPXGSIFCwFpgwiUIefA0Hy3jLjk/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://drive.google.com/file/d/1pUyUcFoY15nX2VCZt-JRn96v0JjrecFC/preview?usp=drive_link" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>This paper describes end-to-end MIDI-to-score conversion, which has not been focused on in the MIR field.</p>
<p>The paper is written very well. The proposed method is technically solid and its effectiveness was clearly shown by the well-designed experiments.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Overall, the paper is very clear and well-written. The methodological decisions are motivated, and the evaluation is rigorous, using standard metrics, comparing with the state-of-the-art works, and with an ablation study for key design decisions. In addition, the code and pre-trained models will be released upon publication.</p>
<p>Since the dataset used for training and evaluation only includes piano performances (ASAP), I suggest including this aspect in the title (e.g., "End-to-end piano performance-MIDI...") and in the abstract.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This work presents an approach for obtaining a musical score from a performance MIDI file based on Transformers. The authors consider a reference dataset for the experiments and compare against existing approaches (both commercial and research-oriented) with several evaluation metrics and conclude the superiority of their proposal.</p>
<p>The manuscript is well written and sounds technical. Also, while the authors are commited to share the code and data if the work is accepted for publication, they also share as supplementary material to the conference some examples of the results that the approach may obtain.</p>
<p>My only concern in the work is the exclusive use of the ASAP dataset (as labeled data). In this sense, have the authors consider any other alternative dataset, at least for a particular experiment, to assess the generalization capabilities of the approach?</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper proposed a new end-to-end approach to performance-MIDI to score conversion using the Transformers architecture. The proposed method is well-motivated (there is no one-to-one mapping between a performance MIDI sequence and a music score note sequence), achieves good results (beating the state-of-the-art), well-explained, and supported with rigorous experiments.</p>
<p>Apart from the key contributions stated in the paper's introduction, the biggest advantage of this paper, from my point of view, is the detailed technical considerations presented in the paper. The use of data (labelled and unpaired), the tokenization of the music score data (11 attributes derived from the MusicXML score, and the use of space token to fulfill beat-level alignment), the data augmentation (especially the duration jitter and onset jitter to simulate human performance), and the detailed ablation study makes the paper technically strong and provides plenty of useful insights to people working on related field. I especially like the way the paper describes data-related design choices by supporting them with comparisons to previous/other settings (e.g. Section 3.1.2 Unpaired data and Table 5). Furthermore, it's great to see the ethics statement which mentions future research for other music genres.</p>
<p>Overall, I think it's a good paper that should be presented at ISMIR, which will have good audiences and can be very helpful to others working on related tasks.</p>
<p>Below are only some minor comments.</p>
<ul>
<li>L27: "significant downstream applications" =&gt; significant number of downstream applications?</li>
<li>L147: "Eq. (4)" =&gt; Eq. (5)</li>
<li>L178: "ml_j stores the preceding measure's length for downbeat notes or is set to false otherwise." =&gt; I'm still not very sure what ml_j is precisely. Is a downbeat note a note those onset is on a downbeat? How is it related to the measure's length?</li>
<li>L205: "an inner dimension of 3072 for the position-wise feed-forward network" =&gt; It is said at the beginning of this paragraph that "the backbone model follows the original architecture described by Vaswani et al." But the original feed-forward dimension is 2048.</li>
<li>L211: After reading this paragraph, it's still not clear to me what are the "conditioning token" and the "space token". These are made clear after I finished reading Section 3.1.2. It may be helpful to polish this paragraph a bit, and add a reference to Section 3.1.2.</li>
<li>L234: "space token steam" =&gt; stream</li>
<li><strong>Table 2</strong>: The total number of distinct pieces, P-MIDI notes, and score notes are not equal to the corresponding sum over the train/valid/test splits.</li>
<li>Table 4: Which SOTA is it in the table? Add a reference?</li>
<li>Table 5: MIDI scores do not actually have stem directions. How are the stems obtained for the MIDI scores (e.g. by importing to MuseScore or Finale)? It will be helpful to mention that to improve reproducibility. Or to annotate it as a "-" indicating there is no stem prediction.</li>
<li>L387: How are the barlines predicted after removing the time signature module? And similarly, how are the predictions converted into MusicXML format for evaluation?</li>
<li>References: Some minor formatting issues for items [6, 19, 20, 25, 26, 27, 29, 40, 41], mostly related to capitalisation.</li>
</ul></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We would like to thank all reviewers for their helpful comments and constructive feedback. </p>
<p>We have incorporated suggestions about the scope of the paper and additional detail about the tokenization method into the title and abstract (#1).
Additionally, we clarified the explanation of the conditioning and space tokens, moving them into Section 3.1.2. and Section 2.2, respectively (#4). 
The tokenization section now includes more detail about handling polyphonic music, note ordering, and the encoding of musical time (#4 and meta #1).
Dataset statistics in Table 2 were recomputed in order to reconcile inconsistent preprocessing between different splits (#4).
Furthermore, section 3.3 now contains additional information on how MusicXML scores were obtained for baseline methods (#4).</p>
<p>Due to space restrictions, we were unable to insert a figure illustrating the tokenization in the paper (meta #1). However, clear code for all tokenization/detokenization steps will be available on GitHub, along with an illustration of a tokenization of a melody.</p>
<p>We also want to address our focus on classical music and the ASAP dataset (#3). To our knowledge, there are no other publicly available datasets which include paired performance-MIDI and MusicXML scores. CrestMusePEDB is the only other dataset in the literature with MusicXML scores, but it is smaller, not openly available, and also limited to classical music. To illustrate our method's generalization, the supplementary material includes examples of generations for out-of-genre music from in-the-wild performances. </p>
<p>Finally, we fixed various typos and formatting issues (#4).
To adhere to the page limit, some changes required minor reworking of other text passages in the paper; however, no content was significantly altered.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;158&#39;, &#39;session&#39;: &#39;2&#39;, &#39;position&#39;: &#39;16&#39;, &#39;forum&#39;: &#39;158&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1WEIyNwDfEfHyazJa093LiyjPY5310QN-/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;End-to-end Piano Performance-MIDI to Score Conversion with Transformers&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Beyer, Tim*&#39;, &#39; Dai, Angela&#39;], &#39;authors_and_affil&#39;: [&#39;Tim Beyer (Technical University of Munich)*&#39;, &#39; Angela Dai (Technical University of Munich)&#39;], &#39;keywords&#39;: [&#39;MIR fundamentals and methodology -&gt; symbolic music processing; MIR tasks -&gt; music synthesis and transformation; Musical features and properties -&gt; rhythm, beat, tempo; Musical features and properties -&gt; structure, segmentation, and form&#39;, &#39;MIR tasks -&gt; music transcription and annotation&#39;], &#39;abstract&#39;: &#39;The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.&#39;, &#39;TLDR&#39;: &#39;The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1X8aKfd0wZiXnLgeiCqNYOBWXzxHWIDLY/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;2&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/10tX3QWkVV5ALUq3aD0QwXSJ4JP9dmD1B/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/11SgGetPXGSIFCwFpgwiUIefA0Hy3jLjk/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UHCSSRSA&#39;, &#39;slack_channel&#39;: &#39;p2-15-end-to-end&#39;, &#39;slides_pdf&#39;: &#39;https://drive.google.com/file/d/1pUyUcFoY15nX2VCZt-JRn96v0JjrecFC/view?usp=drive_link&#39;, &#39;day&#39;: &#39;1&#39;, &#39;review_1&#39;: &#39;Overall, the paper is very clear and well-written. The methodological decisions are motivated, and the evaluation is rigorous, using standard metrics, comparing with the state-of-the-art works, and with an ablation study for key design decisions. In addition, the code and pre-trained models will be released upon publication.\n\nSince the dataset used for training and evaluation only includes piano performances (ASAP), I suggest including this aspect in the title (e.g., &#34;End-to-end piano performance-MIDI...&#34;) and in the abstract.\n\n&#39;, &#39;review_2&#39;: &#39;This work presents an approach for obtaining a musical score from a performance MIDI file based on Transformers. The authors consider a reference dataset for the experiments and compare against existing approaches (both commercial and research-oriented) with several evaluation metrics and conclude the superiority of their proposal.\n\nThe manuscript is well written and sounds technical. Also, while the authors are commited to share the code and data if the work is accepted for publication, they also share as supplementary material to the conference some examples of the results that the approach may obtain.\n\nMy only concern in the work is the exclusive use of the ASAP dataset (as labeled data). In this sense, have the authors consider any other alternative dataset, at least for a particular experiment, to assess the generalization capabilities of the approach?&#39;, &#39;review_3&#39;: &#39;This paper proposed a new end-to-end approach to performance-MIDI to score conversion using the Transformers architecture. The proposed method is well-motivated (there is no one-to-one mapping between a performance MIDI sequence and a music score note sequence), achieves good results (beating the state-of-the-art), well-explained, and supported with rigorous experiments.\n\nApart from the key contributions stated in the paper\&#39;s introduction, the biggest advantage of this paper, from my point of view, is the detailed technical considerations presented in the paper. The use of data (labelled and unpaired), the tokenization of the music score data (11 attributes derived from the MusicXML score, and the use of space token to fulfill beat-level alignment), the data augmentation (especially the duration jitter and onset jitter to simulate human performance), and the detailed ablation study makes the paper technically strong and provides plenty of useful insights to people working on related field. I especially like the way the paper describes data-related design choices by supporting them with comparisons to previous/other settings (e.g. Section 3.1.2 Unpaired data and Table 5). Furthermore, it\&#39;s great to see the ethics statement which mentions future research for other music genres.\n\nOverall, I think it\&#39;s a good paper that should be presented at ISMIR, which will have good audiences and can be very helpful to others working on related tasks.\n\nBelow are only some minor comments.\n\n* L27: &#34;significant downstream applications&#34; =&gt; significant number of downstream applications?\n* L147: &#34;Eq. (4)&#34; =&gt; Eq. (5)\n* L178: &#34;ml_j stores the preceding measure\&#39;s length for downbeat notes or is set to false otherwise.&#34; =&gt; I\&#39;m still not very sure what ml_j is precisely. Is a downbeat note a note those onset is on a downbeat? How is it related to the measure\&#39;s length?\n* L205: &#34;an inner dimension of 3072 for the position-wise feed-forward network&#34; =&gt; It is said at the beginning of this paragraph that &#34;the backbone model follows the original architecture described by Vaswani et al.&#34; But the original feed-forward dimension is 2048.\n* L211: After reading this paragraph, it\&#39;s still not clear to me what are the &#34;conditioning token&#34; and the &#34;space token&#34;. These are made clear after I finished reading Section 3.1.2. It may be helpful to polish this paragraph a bit, and add a reference to Section 3.1.2.\n* L234: &#34;space token steam&#34; =&gt; stream\n* **Table 2**: The total number of distinct pieces, P-MIDI notes, and score notes are not equal to the corresponding sum over the train/valid/test splits.\n* Table 4: Which SOTA is it in the table? Add a reference?\n* Table 5: MIDI scores do not actually have stem directions. How are the stems obtained for the MIDI scores (e.g. by importing to MuseScore or Finale)? It will be helpful to mention that to improve reproducibility. Or to annotate it as a &#34;-&#34; indicating there is no stem prediction.\n* L387: How are the barlines predicted after removing the time signature module? And similarly, how are the predictions converted into MusicXML format for evaluation?\n* References: Some minor formatting issues for items [6, 19, 20, 25, 26, 27, 29, 40, 41], mostly related to capitalisation.\n&#39;, &#39;meta_review&#39;: &#39;This paper describes end-to-end MIDI-to-score conversion, which has not been focused on in the MIR field.\n\nThe paper is written very well. The proposed method is technically solid and its effectiveness was clearly shown by the well-designed experiments.&#39;, &#39;author_changes&#39;: &#34;We would like to thank all reviewers for their helpful comments and constructive feedback. \n\nWe have incorporated suggestions about the scope of the paper and additional detail about the tokenization method into the title and abstract (#1).\nAdditionally, we clarified the explanation of the conditioning and space tokens, moving them into Section 3.1.2. and Section 2.2, respectively (#4). \nThe tokenization section now includes more detail about handling polyphonic music, note ordering, and the encoding of musical time (#4 and meta #1).\nDataset statistics in Table 2 were recomputed in order to reconcile inconsistent preprocessing between different splits (#4).\nFurthermore, section 3.3 now contains additional information on how MusicXML scores were obtained for baseline methods (#4).\n\nDue to space restrictions, we were unable to insert a figure illustrating the tokenization in the paper (meta #1). However, clear code for all tokenization/detokenization steps will be available on GitHub, along with an illustration of a tokenization of a melody.\n\nWe also want to address our focus on classical music and the ASAP dataset (#3). To our knowledge, there are no other publicly available datasets which include paired performance-MIDI and MusicXML scores. CrestMusePEDB is the only other dataset in the literature with MusicXML scores, but it is smaller, not openly available, and also limited to classical music. To illustrate our method&#39;s generalization, the supplementary material includes examples of generations for out-of-genre music from in-the-wild performances. \n\nFinally, we fixed various typos and formatting issues (#4).\nTo adhere to the page limit, some changes required minor reworking of other text passages in the paper; however, no content was significantly altered.&#34;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>