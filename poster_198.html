

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            MidiCaps: A Large-scale MIDI Dataset with Text Captions
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Melechovsky, Jan"
               class="text-muted"
            >Melechovsky, Jan</a
            >,
            
            <a href="papers.html?filter=authors&search= Roy, Abhinaba*"
               class="text-muted"
            > Roy, Abhinaba*</a
            >,
            
            <a href="papers.html?filter=authors&search= Herremans, Dorien"
               class="text-muted"
            > Herremans, Dorien</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UPUEUDV1" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p6-05-midicaps-a-large</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; lyrics and other textual data; MIR fundamentals and methodology -&gt; symbolic music processing; MIR fundamentals and methodology -&gt; web mining, and natural language processing; Musical features and properties -&gt; representations of music"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; lyrics and other textual data; MIR fundamentals and methodology -&gt; symbolic music processing; MIR fundamentals and methodology -&gt; web mining, and natural language processing; Musical features and properties -&gt; representations of music</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases"
                    class="text-secondary text-decoration-none"
            >Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/18ltPsN6Gp-anmdtvXVkfsuFWG-tTAvmF/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/16b061WIusDZCvVrtgezh5AiyCE9ElkdT/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1qIrcmvqJRIO-Ov3kpzGRrxay8Mh_jzom/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The paper was commended for the new task to be introduced and on the good presentation and writing. There were specific comments on the motivation, the need for additional evaluations, and a lack of clarity on the features used. Although there are some minor comments on the manuscript itself, these can be addressed in the camera-ready version, and the manuscript can be recommended for acceptance.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Overall, the paper is well-written, and the topic itself is quite new, so I think this dataset can be used in future researches well. However, an illustration of the details about the used features is limited and especially the evaluation of the pseudo-generated MIDI captioning dataset is also very limited. </p>
<p>Listening study: looking at the 5 audio and caption examples in the web demo page, the quality seems decent. However, the listening study reported in the paper doesn't give much information about the quality of the captioning dataset.</p>
<p>It would be helpful for the readers if the authors can provide detailed explanation about the used feature extraction algorithms. In this paper, the characteristics about the each of feature extraction algorithms plays critical role in the pseudo generated MIDI captions, however, in the paper, what the authors reported regarding the used feature extraction algorithms were only about which library they have used. If the authors can report the 1) performance of the each feature extraction algorithm, 2) the category of the output fields (e.g. if it's tempo extraction algorithm, then what would be the output of this algorithm, it can be numbers or some text. I can see it from result table, but it is necessary to illustrate it as detail as possible), and some more information, these would be really beneficial for readers who would want to work on pseudo MIDI captioning or other related tasks.</p>
<p>I also think the author can add small paragraph explaining what is the in-context learning for the readers. </p>
<p>And, if the author can provide some possible list of applications using this dataset (like text-to-MIDI generation, MIDI-to-text captioning task, etc), it would be also nice for the readers. </p>
<p>Also, if the authors can pick one of those tasks and show baseline result, it would compensate the lack of the thorough listening study. However, I understand that it cannot be easily added during reviewing phase. </p>
<p>In conclusion, I think enhancing 1) details about the feature extraction algorithm, 2) listening study, and 3) providing possible tasks using this dataset are necessary to be published as a good paper. Currently, the dataset is novel, but the insights the readers can get from this paper is very limited.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>I must confess that after my first reading, I was very sceptical about this approach of using traditional MIR algorithms to generate some annotation that then a LLM would convert into text captioning. Given the fact that the long-term motivation of the dataset introduced in this paper is to enable a different field of prompts-to-music generative models, especially symbolic music generation, I was expecting an approach that would be more human-centred for captioning the data. One idea could have been to use one of many sources of data available across various websites where people comment and describe how they feel about a particular music. This allowed among other approaches the text prompt-to-music audio field to achieve good results in the past years, especially when targeting a broad human audience. However, choosing the field of symbolic music generation somehow already implies that the target is not a random human, but more especially a musician, and therefore it is indeed legitimate to approach the captioning of music with semantic descriptors that someone skilled in music theory would use.
It is then indeed logical then to rely on the state-of-the-art MIR algorithms to extract such descriptors and then make use of LLMs to generate such caption. However, it would have been useful to discuss a little bit more the limitation of this approach, especially in opposition to a real-world musician captioning; and a small experiment could also have been useful to justify further the usefulness of generated captioning vs human captioning.</p>
<p>The pipeline for the generation of the dataset is clearly explained, with details on every different annotation extracted from the MIDI files of the Lakh Dataset. It would have been interesting to deliver the scripts implementing this pipeline on the repository but is not mandatory.</p>
<p>The two listening study that were conducted to validate the hypothesis that this pipeline would generate caption accurate to musicians are interesting. It would be probably pertinent to detail this section further, especially explaining why the PsyToolkit allows an accurate evaluation of the quality of the generated captioning. Also, again on the topic of comparing generated captioning with human hand-made captioning, it would have been useful to introduce a few human caption examples and compare if the listening results shows significant differences between those two groups.</p>
<p>Finally, it would have been interesting to making use of the dataset applied on a simple experiment. It is understood that this is probably the next step of the research, but this would have helped to justify further the usefulness of this dataset.</p>
<p>In bref;</p>
<p>Strength: First Dataset of MIDI Files Text Captions, Detailed and Reusable Pipeline, Fairly Musically Accurate Captions, Validated with Listening Study</p>
<p>Weakness: No Experiment to demonstrate the Applications of the Dataset, Not enough discussion of Generated Captions vs Human Caption, No Discussion on Future Directions for Improvements of the Captioning, Not enough Discussion on the weaknesses of the Approach</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>The paper is well-written and the motivation is clear, but the way they create the dataset isn't that novel since they mostly use existing models and tools. But we shouldn't expect a dataset paper to be super innovative.
What's great is that the MidiCaps dataset is really big and the automatically generated captions are pretty good quality. Putting together such a large dataset of MIDI files with matching text is valuable in itself, because it allows for new kinds of research combining symbolic music and language processing.
Thus, I think this paper should be accepted.
Typos: Section 3.2: tweek should be tweak</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We updated the title to use colon ":" instead of a dash "-".
Abstract was updated to help save space.
Acknowledgement was added.</p>
<p>We addressed reviewers’ comments, in specific:
We updated our evaluation section by running an extended listening study that included both human and AI-annotated captions to show the similarity between these two. Both general audience and music experts were invited to participate. The section is followed by a discussion on the results and shortcoming of our work, illustrating possibilities for future work.
We also corrected a couple of typos and added an explanation on what is meant by “in-context learning”.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;198&#39;, &#39;session&#39;: &#39;6&#39;, &#39;position&#39;: &#39;06&#39;, &#39;forum&#39;: &#39;198&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1oM1Zo-J2dI_yHQeh3Op6J7Bc3eC4oS6W/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;MidiCaps: A Large-scale MIDI Dataset with Text Captions&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Melechovsky, Jan&#39;, &#39; Roy, Abhinaba*&#39;, &#39; Herremans, Dorien&#39;], &#39;authors_and_affil&#39;: [&#39;Jan Melechovsky (Singapore University of Technology and Design)&#39;, &#39; Abhinaba Roy (SUTD)*&#39;, &#39; Dorien Herremans (Singapore University of Technology and Design)&#39;], &#39;keywords&#39;: [&#39;Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; lyrics and other textual data; MIR fundamentals and methodology -&gt; symbolic music processing; MIR fundamentals and methodology -&gt; web mining, and natural language processing; Musical features and properties -&gt; representations of music&#39;, &#39;Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases&#39;], &#39;abstract&#39;: &#39;Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.&#39;, &#39;TLDR&#39;: &#39;Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/16b061WIusDZCvVrtgezh5AiyCE9ElkdT/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;6&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/18ltPsN6Gp-anmdtvXVkfsuFWG-tTAvmF/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1qIrcmvqJRIO-Ov3kpzGRrxay8Mh_jzom/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UPUEUDV1&#39;, &#39;slack_channel&#39;: &#39;p6-05-midicaps-a-large&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#34;Overall, the paper is well-written, and the topic itself is quite new, so I think this dataset can be used in future researches well. However, an illustration of the details about the used features is limited and especially the evaluation of the pseudo-generated MIDI captioning dataset is also very limited. \n\nListening study: looking at the 5 audio and caption examples in the web demo page, the quality seems decent. However, the listening study reported in the paper doesn&#39;t give much information about the quality of the captioning dataset.\n\nIt would be helpful for the readers if the authors can provide detailed explanation about the used feature extraction algorithms. In this paper, the characteristics about the each of feature extraction algorithms plays critical role in the pseudo generated MIDI captions, however, in the paper, what the authors reported regarding the used feature extraction algorithms were only about which library they have used. If the authors can report the 1) performance of the each feature extraction algorithm, 2) the category of the output fields (e.g. if it&#39;s tempo extraction algorithm, then what would be the output of this algorithm, it can be numbers or some text. I can see it from result table, but it is necessary to illustrate it as detail as possible), and some more information, these would be really beneficial for readers who would want to work on pseudo MIDI captioning or other related tasks.\n\nI also think the author can add small paragraph explaining what is the in-context learning for the readers. \n\nAnd, if the author can provide some possible list of applications using this dataset (like text-to-MIDI generation, MIDI-to-text captioning task, etc), it would be also nice for the readers. \n\nAlso, if the authors can pick one of those tasks and show baseline result, it would compensate the lack of the thorough listening study. However, I understand that it cannot be easily added during reviewing phase. \n\nIn conclusion, I think enhancing 1) details about the feature extraction algorithm, 2) listening study, and 3) providing possible tasks using this dataset are necessary to be published as a good paper. Currently, the dataset is novel, but the insights the readers can get from this paper is very limited.&#34;, &#39;review_2&#39;: &#39;I must confess that after my first reading, I was very sceptical about this approach of using traditional MIR algorithms to generate some annotation that then a LLM would convert into text captioning. Given the fact that the long-term motivation of the dataset introduced in this paper is to enable a different field of prompts-to-music generative models, especially symbolic music generation, I was expecting an approach that would be more human-centred for captioning the data. One idea could have been to use one of many sources of data available across various websites where people comment and describe how they feel about a particular music. This allowed among other approaches the text prompt-to-music audio field to achieve good results in the past years, especially when targeting a broad human audience. However, choosing the field of symbolic music generation somehow already implies that the target is not a random human, but more especially a musician, and therefore it is indeed legitimate to approach the captioning of music with semantic descriptors that someone skilled in music theory would use.\nIt is then indeed logical then to rely on the state-of-the-art MIR algorithms to extract such descriptors and then make use of LLMs to generate such caption. However, it would have been useful to discuss a little bit more the limitation of this approach, especially in opposition to a real-world musician captioning; and a small experiment could also have been useful to justify further the usefulness of generated captioning vs human captioning.\n\nThe pipeline for the generation of the dataset is clearly explained, with details on every different annotation extracted from the MIDI files of the Lakh Dataset. It would have been interesting to deliver the scripts implementing this pipeline on the repository but is not mandatory.\n\nThe two listening study that were conducted to validate the hypothesis that this pipeline would generate caption accurate to musicians are interesting. It would be probably pertinent to detail this section further, especially explaining why the PsyToolkit allows an accurate evaluation of the quality of the generated captioning. Also, again on the topic of comparing generated captioning with human hand-made captioning, it would have been useful to introduce a few human caption examples and compare if the listening results shows significant differences between those two groups.\n\nFinally, it would have been interesting to making use of the dataset applied on a simple experiment. It is understood that this is probably the next step of the research, but this would have helped to justify further the usefulness of this dataset.\n\nIn bref;\n\nStrength: First Dataset of MIDI Files Text Captions, Detailed and Reusable Pipeline, Fairly Musically Accurate Captions, Validated with Listening Study\n\nWeakness: No Experiment to demonstrate the Applications of the Dataset, Not enough discussion of Generated Captions vs Human Caption, No Discussion on Future Directions for Improvements of the Captioning, Not enough Discussion on the weaknesses of the Approach&#39;, &#39;review_3&#39;: &#34;The paper is well-written and the motivation is clear, but the way they create the dataset isn&#39;t that novel since they mostly use existing models and tools. But we shouldn&#39;t expect a dataset paper to be super innovative.\nWhat&#39;s great is that the MidiCaps dataset is really big and the automatically generated captions are pretty good quality. Putting together such a large dataset of MIDI files with matching text is valuable in itself, because it allows for new kinds of research combining symbolic music and language processing.\nThus, I think this paper should be accepted.\nTypos: Section 3.2: tweek should be tweak&#34;, &#39;meta_review&#39;: &#39;The paper was commended for the new task to be introduced and on the good presentation and writing. There were specific comments on the motivation, the need for additional evaluations, and a lack of clarity on the features used. Although there are some minor comments on the manuscript itself, these can be addressed in the camera-ready version, and the manuscript can be recommended for acceptance.&#39;, &#39;author_changes&#39;: &#39;We updated the title to use colon &#34;:&#34; instead of a dash &#34;-&#34;.\nAbstract was updated to help save space.\nAcknowledgement was added.\n\nWe addressed reviewers’ comments, in specific:\nWe updated our evaluation section by running an extended listening study that included both human and AI-annotated captions to show the similarity between these two. Both general audience and music experts were invited to participate. The section is followed by a discussion on the results and shortcoming of our work, illustrating possibilities for future work.\nWe also corrected a couple of typos and added an explanation on what is meant by “in-context learning”.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>