[
  {
    "content": {
      "TLDR": "In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",
      "abstract": "In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",
      "authors": [
        "Flexer, Arthur*"
      ],
      "authors_and_affil": [
        "Arthur Flexer (Johannes Kepler University Linz)*"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> web mining, and natural language processing; MIR tasks -> similarity metrics; Philosophical and ethical discussions -> philosophical and methodological foundations",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1dA6YiGRw7WLmcSBG-dCbmqGRjp7DpgnE/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1VAG-uC-9QPHuPzJsxziqRpipsaG-LYLI/view?usp=sharing",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "On the validity of employing ChatGPT for distant reading of music similarity",
      "video": "https://drive.google.com/file/d/1XuSiWBgkUfBCnZLa-s5uAkZIua7E-p0C/view?usp=drive_link"
    },
    "forum": "1",
    "id": "1",
    "pic_id": "https://drive.google.com/file/d/1bDKJUha7CfprIh1jVxLcMZ1aUzb7_kJK/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "abstract": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "authors": [
        "Ryu, Jiwoo",
        " Dong, Hao-Wen",
        " Jung, Jongmin",
        " Jeong, Dasaem*"
      ],
      "authors_and_affil": [
        "Jiwoo Ryu (Sogang University)",
        " Hao-Wen Dong (University of Michigan)",
        " Jongmin Jung (Sogang University)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1E8CTFJvSUaksQAHwn5DzFUtPTiuSNZ4L/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/19ZkRhN2wiJdL1LnQt8dfOMXD__2YV6LW/view?usp=sharing",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
      "video": "https://drive.google.com/file/d/1d4vhwOeI7Ve1w0niVr5Y-qRko_ONQcoN/view?usp=share_link"
    },
    "forum": "5",
    "id": "5",
    "pic_id": "https://drive.google.com/file/d/1ulzf3JND3fCeQwGLr8ueKujOh5F-arSI/view?usp=sharing",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In Music Information Retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automatic music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio\u2013audio and audio\u2013score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",
      "abstract": "In Music Information Retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automatic music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio\u2013audio and audio\u2013score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",
      "authors": [
        "Zeitler, Johannes*",
        " Maman, Ben",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        " Ben Maman (Tel Aviv University)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ovrLChf92BfHq_ilyOqp6VIm6DOYTKMZ/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1Yeeif-eM52-Vv73rbePu4_580JjPKdNH/view?usp=sharing",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models",
      "video": "https://drive.google.com/file/d/1YObvTral3sonXEVYqTR-gIs_grqizI72/view?usp=sharing"
    },
    "forum": "8",
    "id": "8",
    "pic_id": "https://drive.google.com/file/d/1Xhbn8tKt43J3OItlISBo2JOUww4rs3AS/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score typesetting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",
      "abstract": "This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score typesetting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",
      "authors": [
        "Foscarin, Francesco*",
        " Karystinaios, Emmanouil",
        " Nakamura, Eita",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Emmanouil Karystinaios (Johannes Kepler University)",
        " Eita Nakamura (Kyoto University)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1PmF0u_7o7ZK5P0SYyGiQOs18Xog2OOlS/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PamUFD3n7kGV__DCVZPZd3TVHjgpDztW/view?usp=sharing",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving",
      "video": "https://drive.google.com/file/d/1QAMVD9519WvHgz5Dlh39j8YCZtjUZ62O/view?usp=sharing"
    },
    "forum": "9",
    "id": "9",
    "pic_id": "https://drive.google.com/file/d/17pv5SIysX3ao3sQ187USvJryc5unEp4Q/view?usp=share_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets -- including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations -- and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",
      "abstract": "We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets -- including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations -- and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",
      "authors": [
        "Foscarin, Francesco*",
        " Schl\u00fcter, Jan",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Jan Schl\u00fcter (JKU Linz)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1h9AImUGC_47nMT_rV1AYJOGvFkdoAEu8/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/15AS9uKpRbx36lXTLVpiOlfpfXnXAw-tY/view?usp=sharing",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Beat this! Accurate beat tracking without DBN postprocessing",
      "video": "https://drive.google.com/file/d/1-ll3kj92H0jJTdQ851VIO8lXCBDw_qge/view?usp=sharing"
    },
    "forum": "10",
    "id": "10",
    "pic_id": "https://drive.google.com/file/d/18I-8CqusUouwV1uqsWd8r2pq_NrqQR-X/view?usp=sharing",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",
      "abstract": "This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",
      "authors": [
        "Chen, Tsung-Ping*",
        " Yoshii, Kazuyoshi"
      ],
      "authors_and_affil": [
        "Tsung-Ping Chen (Kyoto University)*",
        " Kazuyoshi Yoshii (Kyoto University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Musical features and properties -> structure, segmentation, and form",
        "Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13xy02rQcrgv5C8TZKQF17dUUO3CkQP52/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14RwLEkmKADHV1DAkWTXs6rCRe0KDXOoa/view?usp=sharing",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Learning Multifaceted Self-Similarity over Time and Frequency for Music Structure Analysis",
      "video": "https://drive.google.com/file/d/112O5L8XUnDxGENRx4lG4MJSImunVRW5p/view?usp=sharing"
    },
    "forum": "12",
    "id": "12",
    "pic_id": "https://drive.google.com/file/d/1gw9zu9cfPC_Z8Ar-nB8AiYEhOUKOWORL/view?usp=sharing",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",
      "abstract": "Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",
      "authors": [
        "Arguello, Giulia",
        " Lanzendoerfer, Luca A*",
        " Wattenhofer, Roger"
      ],
      "authors_and_affil": [
        "Giulia Arguello (ETH Zurich)",
        " Luca A Lanzendoerfer (ETH Zurich)*",
        " Roger Wattenhofer (ETH Zurich)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> pattern matching and detection"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1qAbrZKi-ItKh6F5IcxY9SVbtROgDQR2G/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1-MPgcWlKym7T6k4lLhTrt7VIa016_Eqq/view?usp=sharing",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Cue Point Estimation using Object Detection",
      "video": "https://drive.google.com/file/d/1MLy__nYjpuhpzC8A7JoXcbv2S3G-h8V0/view?usp=sharing"
    },
    "forum": "19",
    "id": "19",
    "pic_id": "https://drive.google.com/file/d/1qOroLJ5UcefxgrALmSFTpQtjy1xmlqcU/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "abstract": "In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "authors": [
        "Nercessian, Shahan*",
        " Imort, Johannes",
        " Devis, Ninon",
        " Blang, Frederik"
      ],
      "authors_and_affil": [
        "Shahan Nercessian (Native Instruments)*",
        " Johannes Imort (Native Instruments)",
        " Ninon Devis (Native Instruments)",
        " Frederik Blang (Native Instruments)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1JSNa6kdSy1HbTaV90ziXODAfPvCMRS3W/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1cz6Qnzc3sW1F5EOUEFICXT1oEd6PMDyK/view?usp=sharing",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "video": "https://drive.google.com/file/d/1e2NU_vLLkHDZIDgUnV3coAUFXcYRkHDo/view?usp=sharing"
    },
    "forum": "22",
    "id": "22",
    "pic_id": "https://drive.google.com/file/d/19NV6asW0uyiNpY8JILdYGemXZ_T5vo53/view?usp=sharing",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contempor\u00e0nia de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",
      "abstract": "We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contempor\u00e0nia de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",
      "authors": [
        "Haki, Behzad",
        " Evans, Nicholas*",
        " G\u00f3mez, Daniel",
        " Jord\u00e0, Sergi"
      ],
      "authors_and_affil": [
        "Behzad Haki (Universitat Pompeu Fabra)",
        " Nicholas Evans (Universitat Pompeu Fabra)*",
        " Daniel G\u00f3mez (MTG)",
        " Sergi Jord\u00e0 (Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Musical features and properties -> rhythm, beat, tempo",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1HWtECEdTCXBadHE1SZX61uFCqSA5w3Zr/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1_XdGgImckqE1SPlOhdFutotqmWzKJBvF/view?usp=sharing",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "El Bongosero: A Crowd-sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired with Drum Patterns",
      "video": "https://drive.google.com/file/d/13GqBs9WeVeweyL3J4_42sBlSgEES6eMq/view?usp=sharing"
    },
    "forum": "24",
    "id": "24",
    "pic_id": "https://drive.google.com/file/d/1unWOlZ-Q98Qpc3py14DSmOx_u_Sz8G8q/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet \u2014 at only 24.9 M trainable parameters \u2014 performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",
      "abstract": "Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet \u2014 at only 24.9 M trainable parameters \u2014 performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",
      "authors": [
        "Watcharasupat, Karn N*",
        " Lerch, Alexander"
      ],
      "authors_and_affil": [
        "Karn N Watcharasupat (Georgia Institute of Technology)*",
        " Alexander Lerch (Georgia Institute of Technology)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> indexing and querying; Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1o7k0dD9lJ6aP0DDtX2SxXA-UO8QwSQXs/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1-A7wIS_Eo0VTqp6yVFjC7x_nyyhnTvW_/view?usp=drive_link",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
      "video": "https://drive.google.com/file/d/1mujQfs5mHI_SB15pzwPf0CUSTuoPyDXg/view?usp=drive_link"
    },
    "forum": "32",
    "id": "32",
    "pic_id": "https://drive.google.com/file/d/1x1wJNvbQ6PwtUhqMOxJVfLOhPviARX2I/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "We introduce a project that revives a piece of 15th-century Korean court music, Chwipunghyeong, composed upon the poem 'Songs of the Dragon Flying to Heaven'. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chwipunghyeong was evaluated by experts and is scheduled to be performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",
      "abstract": "We introduce a project that revives a piece of 15th-century Korean court music, Chwipunghyeong, composed upon the poem 'Songs of the Dragon Flying to Heaven'. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chwipunghyeong was evaluated by experts and is scheduled to be performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",
      "authors": [
        "Han, Danbinaerin",
        " Gotham, Mark R H",
        " Kim, DongMin",
        " Park, Hannah",
        " Lee, Sihun",
        " Jeong, Dasaem*"
      ],
      "authors_and_affil": [
        "Danbinaerin Han (KAIST)",
        " Mark R H Gotham (Durham)",
        " DongMin Kim (Sogang University)",
        " Hannah Park (Sogang University)",
        " Sihun Lee (Sogang University)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Applications -> music composition, performance, and production",
        "Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1DeBMx3GiwXq6Jss-UNAHqg3knxh3NT38/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ToZXB9UB1VjG19bVSX2NzE1xLd10d4E-/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding",
      "video": "https://drive.google.com/file/d/1niUE0tEkH0PMP_GE9EhMe1FyckKUHpSr/view?usp=drive_link"
    },
    "forum": "35",
    "id": "35",
    "pic_id": "https://drive.google.com/file/d/1zQ_ErYB9wyqKxrPflKBkMw0Y-cGPqtZc/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",
      "abstract": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",
      "authors": [
        "Suda, Hitoshi*",
        " Yoshida, Shunsuke",
        " Nakamura, Tomohiko",
        " Fukayama, Satoru",
        " Ogata, Jun"
      ],
      "authors_and_affil": [
        "Hitoshi Suda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Shunsuke Yoshida (The University of Tokyo)",
        " Tomohiko Nakamura (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Satoru Fukayama (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Jun Ogata (AIST)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Musical features and properties -> timbre, instrumentation, and singing voice",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1tP9dkZCAXSH3EY2iZk5znhBUFDXitWFr/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/14XSOt9VO4srbVQIc08K1PqxXeJsczxbX/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs",
      "video": "https://drive.google.com/file/d/1uO7oXRmT8aqi3TqoO7X9bXHMi77WpUiP/view?usp=drive_link"
    },
    "forum": "38",
    "id": "38",
    "pic_id": "https://drive.google.com/file/d/1wEaOrGd12pgHCnwduMjbyzjuChxdbBK1/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.",
      "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.",
      "authors": [
        "Rouard, Simon*",
        " Defossez, Alexandre",
        " Adi, Yossi",
        " Copet, Jade",
        " Roebel, Axel"
      ],
      "authors_and_affil": [
        "Simon Rouard (Meta AI Research)*",
        " Alexandre Defossez (Kyutai)",
        " Yossi Adi (Facebook AI Research )",
        " Jade Copet (Meta AI Research)",
        " Axel Roebel (IRCAM)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1AXwd4qZXBWY7jJ12ASDW13nKpzPeW910/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1I1TzCEWABu-BD52VU9EDr0hjPFr3Cg1k/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features",
      "video": "https://drive.google.com/file/d/1C_XUyhaIMcDv04fFALbIhQemHpVAyYw9/view?usp=drive_link"
    },
    "forum": "41",
    "id": "41",
    "pic_id": "https://drive.google.com/file/d/1KIzQlQbxxz8C4TrXwWVAgSnKHl5QL_vU/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "It is well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.   In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning.  As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",
      "abstract": "It is well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.   In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning.  As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",
      "authors": [
        "Kim, Hyon*",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Hyon Kim (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing; Musical features and properties -> expression and performative aspects of music",
        "MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1j-WrXG6w8826Xd_mmCnsdDVdCBcg_AjI/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Roqh2RqoMLQfxwRgH-Wtxocb58su2ODH/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "A Method for MIDI Velocity Estimation for Piano Performance by a U-Net with Attention and FiLM",
      "video": "https://drive.google.com/file/d/1T_Ldu-OZ5QK6O68vFquQHNZWz_x0lzAd/view?usp=drive_link"
    },
    "forum": "42",
    "id": "42",
    "pic_id": "https://drive.google.com/file/d/1aOPRZRHwkysFf7uyhlBGLG2HTJhJeEMN/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference, and ambivalence were preferred. In contrast, themes such as liberation, familiarity, and activity are not as favored. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",
      "abstract": "Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference, and ambivalence were preferred. In contrast, themes such as liberation, familiarity, and activity are not as favored. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",
      "authors": [
        "Chowdary, Pavani B*",
        " Singh, Bhavyajeet",
        " Agarwal, Rajat",
        " Alluri, Vinoo"
      ],
      "authors_and_affil": [
        "Pavani B Chowdary (International Institute of Information Technology, Hyderabad)*",
        " Bhavyajeet Singh (International Institute of Information Technology, Hyderabad )",
        " Rajat Agarwal (International Institute of Information Technology)",
        " Vinoo  Alluri (IIIT - Hyderabad)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Human-centered MIR",
        "Applications -> music and health, well-being and therapy; Human-centered MIR -> user behavior analysis and mining, user modeling; Human-centered MIR -> user-centered evaluation; MIR fundamentals and methodology -> lyrics and other textual data; Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1h1h8IbGas1Eq65yRk8IoohDgh6ZVh67e/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1LokDJCDPPXaforbEr7DhcxuOGm3Pcg_U/view?usp=drive_link",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk",
      "video": "https://drive.google.com/file/d/1WqcSFrn16ZcX2us_cF3Y4JbxaWEfnJ5c/view?usp=drive_link"
    },
    "forum": "43",
    "id": "43",
    "pic_id": "https://drive.google.com/file/d/1CTC9XD66wpDs9bwasfXRg6nleMZu2zs8/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",
      "abstract": "The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",
      "authors": [
        "Luna-Barahona, Noelia N",
        " Rosell\u00f3, Adri\u00e1n",
        " Alfaro-Contreras, Mar\u00eda",
        " Rizo, David",
        " Calvo-Zaragoza, Jorge*"
      ],
      "authors_and_affil": [
        "Noelia N Luna-Barahona (Universidad de Alicante)",
        " Adri\u00e1n Rosell\u00f3 (Universidad de Alicante)",
        " Mar\u00eda Alfaro-Contreras (University of Alicante)",
        " David Rizo (University of Alicante. Instituto Superior de Ense\u00f1anzas Art\u00edsrticas de la Comunidad Valenciana)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Applications -> digital libraries and archives",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1uAPEcIw9skchg5UQxmgCkLc1BsClOgFF/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1bzCCw1CM6f0GbA-yFkX4XjLxGClfb6P0/view?usp=drive_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition",
      "video": "https://drive.google.com/file/d/17uyevRjXv8eJp77erGQKQP9Z9AzlI89B/view?usp=drive_link"
    },
    "forum": "45",
    "id": "45",
    "pic_id": "https://drive.google.com/file/d/1pS1if-khfQb_HZLoWqkpYsGqsWCTeT5J/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods are capable of adequately performing this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. For that, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create dynamic class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently outperforms them. This highlights the interest in CL methods tailored specifically for music classification.",
      "abstract": "Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods are capable of adequately performing this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. For that, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create dynamic class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently outperforms them. This highlights the interest in CL methods tailored specifically for music classification.",
      "authors": [
        "Gonz\u00e1lez-Barrachina, Pedro",
        " Alfaro-Contreras, Mar\u00eda",
        " Calvo-Zaragoza, Jorge*"
      ],
      "authors_and_affil": [
        "Pedro Gonz\u00e1lez-Barrachina (University of Alicante)",
        " Mar\u00eda Alfaro-Contreras (University of Alicante)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR tasks -> automatic classification",
        "MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1C62NxDl4MWhctcXKlDNq2cBbsRMCLFYQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1UmdgpGdCSDy7rDeRGwyHYH_Qj4_ZiD_S/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Continual Learning for Music Classification",
      "video": "https://drive.google.com/file/d/1j8PN64wnrXGHAQ8QN5rDLoRpqGPhoDNn/view?usp=drive_link"
    },
    "forum": "46",
    "id": "46",
    "pic_id": "https://drive.google.com/file/d/1BUDLfutqRkVEJFy1z2culI2uv_G1lrYL/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",
      "abstract": "Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",
      "authors": [
        "Deng, Jiajun*",
        " Ju, Yaolong",
        " Yang, Jing",
        " Lui, Simon",
        " Liu, Xunying"
      ],
      "authors_and_affil": [
        "Jiajun Deng (The Chinese University of HongKong)*",
        " Yaolong Ju (Huawei)",
        " Jing Yang (Huawei 2012 Labs)",
        " Simon Lui (Huawei)",
        " Xunying Liu (The Chinese University of Hong Kong)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Musical features and properties -> rhythm, beat, tempo",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1CG6ZsCT7eqBT-FHt5XzVXU6JleGoe-FQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1TJ7tKyht_8XhKfcKFU6X-yl5rINUj9Eo/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "EFFICIENT ADAPTER TUNING FOR JOINT SINGING VOICE BEAT AND DOWNBEAT TRACKING WITH SELF-SUPERVISED LEARNING FEATURES",
      "video": "https://drive.google.com/file/d/1_QEmBe1S2hLH83IqJlc_z89AlwAbSMjq/view?usp=drive_link"
    },
    "forum": "48",
    "id": "48",
    "pic_id": "https://drive.google.com/file/d/1qLg3J2L7_BQfBfCtWQe9vWm8s7pHWwcB/view?usp=drive_link",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",
      "abstract": "We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",
      "authors": [
        "Malandro, Martin E*"
      ],
      "authors_and_affil": [
        "Martin E Malandro (Sam Houston State University)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Creativity -> human-ai co-creativity; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1tjF6ljH2xbNTv6enbQfeU4UecpR-C3vH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1yI-Ig00wI16BRqVhS7cqLuLwPHqXoiNy/view?usp=drive_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Composer's Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control",
      "video": "https://drive.google.com/file/d/1PDuCcmOrjiQU0Gxzm0JIy3C6jCases84/view?usp=drive_link"
    },
    "forum": "60",
    "id": "60",
    "pic_id": "https://drive.google.com/file/d/1GIhTiaj-zJipZdmgeIm2H8uoBPfU77vZ/view?usp=drive_link",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.",
      "abstract": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.",
      "authors": [
        "Lin, Liwei*",
        " Xia, Gus",
        " Jiang, Junyan",
        " Zhang, Yixiao"
      ],
      "authors_and_affil": [
        "Liwei Lin (New York University Shanghai)*",
        " Gus Xia (New York University Shanghai)",
        " Junyan Jiang (New York University Shanghai)",
        " Yixiao Zhang (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1HutKNTlmekAao1OWMxx7J-4HFwD3EA0K/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1g1WMrdURcqIJpRyjJrpz4kpifCg57BZl/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Content-based Controls for Music Large-scale Language Modeling",
      "video": "https://drive.google.com/file/d/1s9CKLuAQRybFBY1b20UyGI6-JLmKv9Lw/view?usp=drive_link"
    },
    "forum": "61",
    "id": "61",
    "pic_id": "https://drive.google.com/file/d/1G0Z008KDxw7aYe1eUq7Z1N53egh15jZT/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",
      "abstract": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",
      "authors": [
        "Tal, Or*",
        " Ziv, Alon",
        " Kreuk, Felix",
        " Gat, Itai",
        " Adi, Yossi"
      ],
      "authors_and_affil": [
        "Or Tal (The Hebrew University of Jerusalem)*",
        " Alon Ziv (The Hebrew University of Jerusalem)",
        " Felix Kreuk (Bar-Ilan University)",
        " Itai Gat (Meta)",
        " Yossi Adi (The Hebrew University of Jerusalem)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis; MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation",
      "video": "https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG"
    },
    "forum": "65",
    "id": "65",
    "pic_id": "https://drive.google.com/file/d/17bUEpA6WnKZn6bcPrHmY82DqvQ8_VXrd/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for \"variation success\", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",
      "abstract": "Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for \"variation success\", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",
      "authors": [
        "Gao, Chenyu*",
        " Reuben, Federico",
        " Collins, Tom"
      ],
      "authors_and_affil": [
        "Chenyu Gao (University of York)*",
        " Federico Reuben (University of York)",
        " Tom Collins (University of York",
        " MAIA, Inc.)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1N4ey26D4sZQphIzi74uNjr9fQNtuBCSx/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1vPlg5tf9-nwKr5mi20Xp1qt44iIR6ZDO/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Variation Transformer: New datasets, models, and comparative evaluation for symbolic music variation generation",
      "video": "https://drive.google.com/file/d/14-NxP7Q7c5tEO4UaKWGkEhVLwtTjgT0E/view?usp=drive_link"
    },
    "forum": "72",
    "id": "72",
    "pic_id": "https://drive.google.com/file/d/1ZtDDsSuSB4YVng0Vj5L8X30Pa4TYFeIA/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.  Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",
      "abstract": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.  Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",
      "authors": [
        "Ramoneda, Pedro*",
        " Rocamora, Mart\u00edn",
        " Akama, Taketo"
      ],
      "authors_and_affil": [
        "Pedro Ramoneda (Universitat Pompeu Fabra)*",
        " Mart\u00edn Rocamora (Universidad de la Rep\u00fablica)",
        " Taketo Akama (Sony CSL)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Creativity; Creativity -> creativity and learning; Generative Tasks; Generative Tasks -> interactions; MIR tasks -> music generation",
        "Creativity -> human-ai co-creativity"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ia2OxA9wgadP1SiWWYN7clsiaz9kJbmA/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1VGJLTy1natiGB5LEP1zOBPUmO6oVdVFm/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context",
      "video": "https://drive.google.com/file/d/1Um81VNHusxHqoh84RQlMumaZmMxjGKSX/view?usp=drive_link"
    },
    "forum": "77",
    "id": "77",
    "pic_id": "https://drive.google.com/file/d/1MMgr2dhkmNjdfdcLItJes-4fwbYPNM3a/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human-AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest teams. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP contributes to advancing understanding of human-AI co-creation from the users' perspective. Furthermore, we outline potential use cases for the dataset, ranging from analyzing AI tools utilized in songwriting to gaining insights into users' ethical considerations and expanding creative possibilities. This can help to inform both scholarly inquiry and practical applications in music composition and beyond.",
      "abstract": "The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human-AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest teams. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP contributes to advancing understanding of human-AI co-creation from the users' perspective. Furthermore, we outline potential use cases for the dataset, ranging from analyzing AI tools utilized in songwriting to gaining insights into users' ethical considerations and expanding creative possibilities. This can help to inform both scholarly inquiry and practical applications in music composition and beyond.",
      "authors": [
        "Morris, Lidia J*",
        " Leger, Rebecca",
        " Newman, Michele",
        " Burgoyne, John Ashley",
        " Groves, Ryan",
        " Mangal, Natasha",
        " Lee, Jin Ha"
      ],
      "authors_and_affil": [
        "Lidia J Morris (University of Washington)*",
        " Rebecca Leger (Fraunhofer IIS)",
        " Michele Newman (University of Washington)",
        " John Ashley Burgoyne (University of Amsterdam)",
        " Ryan Groves (Self-employed)",
        " Natasha Mangal (CISAC)",
        " Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Creativity -> computational creativity; Creativity -> creative practice involving MIR or generative technology ; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Creativity -> human-ai co-creativity"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1VjFIWqMwUxnsBKdPsG3jok97RngcMBSQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1GHUgD19TlzgmMjZSe60cgR2uMBZqGWqv/view?usp=drive_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Human-AI Music Process: A Dataset of AI-Supported Songwriting Processes from the AI Song Contest",
      "video": "https://drive.google.com/file/d/1Ple4x4zVPxCMeHRgnjA-WEE5OQt-epk3/view?usp=drive_link"
    },
    "forum": "79",
    "id": "79",
    "pic_id": "https://drive.google.com/file/d/1G7LXNeVNfho8g_TVWUYUFRe93En4BY-P/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as \"purposeful play\". Furthermore, we discuss its implications for creative MIR.",
      "abstract": "The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as \"purposeful play\". Furthermore, we discuss its implications for creative MIR.",
      "authors": [
        "Newman, Michele*",
        " Morris, Lidia J",
        " Kato, Jun",
        " Goto, Masataka",
        " Yip, Jason",
        " Lee, Jin Ha"
      ],
      "authors_and_affil": [
        "Michele Newman (University of Washington)*",
        " Lidia Morris (University of Washington)",
        " Jun Kato (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Jason Yip (University of Washington)",
        " Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Creativity -> creativity and learning",
        "Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; Human-centered MIR -> user-centered evaluation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Iq4_tkuSSKZdUphnPm-8DtyKE4jFWk2N/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/16URMaG7w64HLOG6njAm9UEc1GseMVYX1/view?usp=share_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications with Children",
      "video": "https://drive.google.com/file/d/1mP85ID1OC1_QFZreMCh-HxUQOwbNZLQp/view?usp=share_link"
    },
    "forum": "83",
    "id": "83",
    "pic_id": "https://drive.google.com/file/d/11W7-Grx5vqLGGNfNTkRSlmack3-92kQj/view?usp=share_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song\u2019s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their spectral characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch\u2019s method prioritizes the spectral properties of the bass drum over traditional pitch values, enhancing bass response and overall sound quality. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary music production.",
      "abstract": "The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song\u2019s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their spectral characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch\u2019s method prioritizes the spectral properties of the bass drum over traditional pitch values, enhancing bass response and overall sound quality. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary music production.",
      "authors": [
        "Deruty, Emmanuel*"
      ],
      "authors_and_affil": [
        "Emmanuel Deruty (Sony Computer Science Laboratories)*"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Applications -> music composition, performance, and production; Computational musicology -> digital musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> musical style and genre",
        "Computational musicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/13eZzr4F6FtWq7Kb7YCHicXXkl4N3zA8R/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/1O2570jxaug06VoO0xsDhykKHpqJiMlV4/view?usp=share_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Harmonic and Transposition Constraints Arising from the Use of the Roland TR-808 Bass Drum",
      "video": "https://drive.google.com/file/d/1-7SRb1F67xRtNMhbnUuc06tTkUX2YJ8N/view?usp=share_link"
    },
    "forum": "86",
    "id": "86",
    "pic_id": "https://drive.google.com/file/d/1qualw556L5tVvL-2lA3CeS-tWGvHDswT/view?usp=share_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on MelodyHub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",
      "abstract": "In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on MelodyHub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",
      "authors": [
        "Wu, Shangda",
        " Wang, Yashan",
        " Li, Xiaobing",
        " Yu, Feng",
        " Sun, Maosong*"
      ],
      "authors_and_affil": [
        "Shangda Wu (Central Conservatory of Music)",
        " Yashan Wang (Central Conservatory of Music)",
        " Xiaobing Li (Central Conservatory of Music)",
        " Feng Yu (Central Conservatory of Music)",
        " Maosong Sun (Tsinghua University)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music generation; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1I72iajjKOPsQMmee8CDFiNCSIkQdArvP/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/1jZrxMhcXPZU7iMAQID3nf_BI246t2GNs/view?usp=share_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing",
      "video": "https://drive.google.com/file/d/1GaJwHFJhCRNmFLeZGYLMhi9EKvnr7JV6/view?usp=share_link"
    },
    "forum": "90",
    "id": "90",
    "pic_id": "https://drive.google.com/file/d/1KZHABImBHV5T-_TK4OEVb1glgv-Xpbs5/view?usp=share_link",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of it\u0002erations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realis\u0002tic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30\u00d7 faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of appli\u0002cations (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we in\u0002terpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",
      "abstract": "Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of it\u0002erations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realis\u0002tic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30\u00d7 faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of appli\u0002cations (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we in\u0002terpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",
      "authors": [
        "Comunita, Marco",
        " Zhong, Zhi*",
        " Takahashi, Akira",
        " Yang, Shiqi",
        " Zhao, Mengjie",
        " Saito, Koichi",
        " Ikemiya, Yukara",
        " Shibuya, Takashi",
        " Takahashi, Shusuke",
        " Mitsufuji, Yuki"
      ],
      "authors_and_affil": [
        "Marco Comunit\u00e0 (Queen Mary University of London)",
        " Zhi Zhong (Sony Group Corporation)*",
        " Akira Takahashi (Sony Group Corporation)",
        " Shiqi Yang (Sony)",
        " Mengjie Zhao (Sony Group Corporation)",
        " Koichi Saito (Sony Gruop Corporation)",
        " Yukara Ikemiya (Sony Research)",
        " Takashi Shibuya (Sony AI)",
        " Shusuke Takahashi (Sony Group Corporation)",
        " Yuki Mitsufuji (Sony AI)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "Generative Tasks -> interactions; Generative Tasks -> real-time considerations; Generative Tasks -> transformations; MIR tasks -> automatic classification; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Vinmy5AVoQSPJOvA-qYFu32kPhx8_r8x/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1YZkyRX_07zw72sOvTA9eoaEjcDtPfVIm/view?usp=sharing",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond",
      "video": "https://drive.google.com/file/d/1GvfnJaoT73dkCHy0CsV5WcrTSjfomUFe/view?usp=sharing"
    },
    "forum": "96",
    "id": "96",
    "pic_id": "https://drive.google.com/file/d/1h34o3dFNby363faQnn4IYC8mFwPWu4fc/view?usp=sharing",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",
      "abstract": "Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",
      "authors": [
        "Peter, Silvan*",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Silvan David Peter (JKU)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1GHRxv4pvcpny1pO3Lgx1W1u7kbGoUXnn/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PfmicdgUqlBQIlc7QFRllz3Km0qc3w_D/view?usp=sharing",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "TheGlueNote: Learned Representations for Robust and Flexible Note Alignment",
      "video": "https://drive.google.com/file/d/12FVcGsJpZaS3qKKGy82vNAFXsaaoTm_9/view?usp=sharing"
    },
    "forum": "103",
    "id": "103",
    "pic_id": "https://drive.google.com/file/d/1CcnPsI0Fq-S-7zkqwYY6WIRd5K3wrNAn/view?usp=share_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017\u20132023 that introduce an AMG model, and 2) ISMIR papers from the years 2022\u20132023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",
      "abstract": "The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017\u20132023 that introduce an AMG model, and 2) ISMIR papers from the years 2022\u20132023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",
      "authors": [
        "Holzapfel, Andre*",
        " Kaila, Anna-Kaisa",
        " J\u00e4\u00e4skel\u00e4inen, Petra"
      ],
      "authors_and_affil": [
        "Andre Holzapfel (KTH Royal Institute of Technology in Stockholm)*",
        " Anna-Kaisa Kaila (KTH Royal Institute of Technology, Stockholm)",
        " Petra J\u00e4\u00e4skel\u00e4inen (KTH)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Philosophical and ethical discussions -> legal and societal aspects of MIR",
        "Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1rAepoJk1U2R4g3S_AcxdqWDGzRGd7xPg/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1cYKFcKBYn5ndyPg95kzhNTzvaUouhemE/view?usp=share_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Green MIR? Investigating computational cost of recent music-Ai research in ISMIR",
      "video": "https://drive.google.com/file/d/1FIrhz2X1ejLKxNdDNhi3msM28OhC1MA3/view?usp=share_link"
    },
    "forum": "113",
    "id": "113",
    "pic_id": "https://drive.google.com/file/d/1tPtxxZyBA0nvUGkMXkTlkGIyxkIwzxft/view?usp=share_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",
      "abstract": "Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",
      "authors": [
        "Tan, Chih-Pin*",
        " Ai, Hsin",
        " Chang, Yi-Hsin",
        " Guan, Shuen-Huei",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Chih-Pin Tan (National Taiwan University)*",
        " Hsin Ai (National Taiwan University)",
        " Yi-Hsin Chang (National Taiwan University)",
        " Shuen-Huei Guan (KKCompany Techonologies)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production; Generative Tasks -> transformations; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music generation; Musical features and properties -> musical style and genre",
        "Generative Tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZtVeAwZ4H40Ed2JSPt2uKGBJNjhMvXcP/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1YM75UhJ5geuLqXYH111Zk8vm9diDLPtd/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data",
      "video": "https://drive.google.com/file/d/1GN7GywKGpF4j-DxhbAJey0_uLccSOJre/view?usp=drive_link"
    },
    "forum": "114",
    "id": "114",
    "pic_id": "https://drive.google.com/file/d/1c8Y5WtQiVYHB4GSufL3261GJ2Y00-rWZ/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.cusing on an under-explored genre.",
      "abstract": "Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.cusing on an under-explored genre.",
      "authors": [
        "Sarmento, Pedro Pereira",
        " Loth, Jackson J*",
        " Barthet, Mathieu"
      ],
      "authors_and_affil": [
        "Pedro Pereira Sarmento (Centre for Digital Music)",
        " Jackson J Loth (Queen Mary University of London)*",
        " Mathieu Barthet (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Creativity -> human-ai co-creativity; Creativity -> humanistic discussions; Musical features and properties -> musical style and genre; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies; Philosophical and ethical discussions -> philosophical and methodological foundations",
        "Generative Tasks -> qualitative evaluations"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1YtkoSFwjRZRWBoBSyzxItdPMH0HS2-gj/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1xI02FsNtZTNZsYLy4xH-1-lK30ok51vy/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
      "video": "https://drive.google.com/file/d/1Gz6u9QdoBF6iREW6k4742hjVStT5sNMH/view?usp=drive_link"
    },
    "forum": "131",
    "id": "131",
    "pic_id": "https://drive.google.com/file/d/1p4iM4QJ49Hr4TaPrP9qkgUISHLKvhf39/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks -- pitch spelling and cadence detection -- demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https://github.com/manoskary/graphmuse",
      "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks -- pitch spelling and cadence detection -- demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https://github.com/manoskary/graphmuse",
      "authors": [
        "Karystinaios, Emmanouil*",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Emmanouil Karystinaios (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1v3xnltIXVjl686mPvV34AsC3eN_vNlXr/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1OBlsXp-7Cbh0dH5M1H7HEpVbFp0bsDAt/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "GraphMuse: A Library for Symbolic Music Graph Processing",
      "video": "https://drive.google.com/file/d/1hsWV5uz5Y3sBhXcOUIYbhFU5CzFdMBdl/view?usp=drive_link"
    },
    "forum": "142",
    "id": "142",
    "pic_id": "https://drive.google.com/file/d/1yKYiFaUKBy94b4hVCUKY5VxPMavNYv5w/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems.  We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",
      "abstract": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems.  We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",
      "authors": [
        "Bjare, Mathias Rose*",
        " Lattner, Stefan",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Mathias Rose Bjare (Johannes Kepler University Linz)*",
        " Stefan Lattner (Sony Computer Science Laboratories, Paris)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR tasks -> music generation",
        "Applications -> music composition, performance, and production; Creativity -> computational creativity; Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1OfyMa9IiQxA-tXZ3Z8mIu3EQosT7b0ke/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1IuShjd5Ve4Rlu8GXnRA-6E5RrjWD3Qxm/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Controlling Surprisal in Music Generation via Information Content Curve Matching",
      "video": "https://drive.google.com/file/d/1C5iStI9QSKT53hU1edU3-s5IPpl-WLto/view?usp=drive_link"
    },
    "forum": "143",
    "id": "143",
    "pic_id": "https://drive.google.com/file/d/1peWDFb7ZP5WXiPBECA59VmMawalX4-hC/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",
      "abstract": "Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",
      "authors": [
        "Nakatsuka, Takayuki*",
        " Hamasaki, Masahiro",
        " Goto, Masataka"
      ],
      "authors_and_affil": [
        "Takayuki Nakatsuka (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "MIR and machine learning for musical acoustics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/179hgCXSyqxXxz1erf68XrB8Qy09KbX_R/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1xRye0jC3dcCoaocwUr0RzfrK0pcEw4dY/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval",
      "video": "https://drive.google.com/file/d/1C_MVMWoqeur5iulba_NfP21N647EL0W8/view?usp=drive_link"
    },
    "forum": "155",
    "id": "155",
    "pic_id": "https://drive.google.com/file/d/1Qv2AWSlvU5TxzQTTBP6tFCISYjdfYNpA/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",
      "abstract": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",
      "authors": [
        "Muluneh, Mequanent Argaw*",
        " Peng, Yan-Tsung",
        " Su, Li"
      ],
      "authors_and_affil": [
        "Mequanent Argaw Muluneh (Academia Sinica",
        " National Chengchi University",
        " Debre Markos University)*",
        " Yan-Tsung Peng (National Chengchi University)",
        " Li Su (Academia Sinica)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR",
        "Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants",
      "video": "https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link"
    },
    "forum": "160",
    "id": "160",
    "pic_id": "https://drive.google.com/file/d/1VdrhCgcONuBQ_PGqf-4qf3l7isvYMLVN/view?usp=drive_link",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.",
      "abstract": "Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.",
      "authors": [
        "Araz, Recep Oguz*",
        " Serra, Xavier",
        " Bogdanov, Dmitry"
      ],
      "authors_and_affil": [
        "Recep Oguz Araz (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR tasks",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> fingerprinting; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZLoWu-jji5KbPxEsQtWLJIxxYGASZ919/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/11b84MPrhbqy3NfjtwJRNsJnNKlPLExfe/view?usp=drive_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata",
      "video": "https://drive.google.com/file/d/1S6tj2j1ocTlGctPUCW7SIr6CuhdifpwX/view?usp=drive_link"
    },
    "forum": "166",
    "id": "166",
    "pic_id": "https://drive.google.com/file/d/1LBJWnWvXdGPgEicEIqG9NF6PkNRNM1cl/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the model first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). In this paper, we focus on the MUSCIMA++ v2.0 dataset. It represents musical notation as a graph, where the pairwise relationships among detected music objects are predicted. Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stage in a more holistic way. These findings are an important step toward a more complete OMR solution.",
      "abstract": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the model first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). In this paper, we focus on the MUSCIMA++ v2.0 dataset. It represents musical notation as a graph, where the pairwise relationships among detected music objects are predicted. Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stage in a more holistic way. These findings are an important step toward a more complete OMR solution.",
      "authors": [
        "Yang, Guang*",
        " Zhang, Muru",
        " Qiu, Lin",
        " Wan, Yanming",
        " Smith, Noah A"
      ],
      "authors_and_affil": [
        "Guang Yang (University of Washington)*",
        " Muru Zhang (University of Washington)",
        " Lin Qiu (University of Washington)",
        " Yanming Wan (University of Washington)",
        " Noah A Smith (University of Washington and Allen Institute for AI)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR tasks -> optical music recognition",
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ymF8sx8o5Krxh8EGMA2UwmhRX0ov0YkR/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1S0-dUNVs10dALNj86ORBmU5EaH9IV-79/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Toward a More Complete OMR Solution",
      "video": "https://drive.google.com/file/d/161aBb4xSrTI7akF0_bTI2SPwTzI4zunK/view?usp=drive_link"
    },
    "forum": "193",
    "id": "193",
    "pic_id": "https://drive.google.com/file/d/1py7A8U7NlmRV53-cKnyykr626eHooiwL/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",
      "abstract": "Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",
      "authors": [
        "Melechovsky, Jan",
        " Roy, Abhinaba*",
        " Herremans, Dorien"
      ],
      "authors_and_affil": [
        "Jan Melechovsky (Singapore University of Technology and Design)",
        " Abhinaba Roy (SUTD)*",
        " Dorien Herremans (Singapore University of Technology and Design)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> symbolic music processing; MIR fundamentals and methodology -> web mining, and natural language processing; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/18ltPsN6Gp-anmdtvXVkfsuFWG-tTAvmF/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/16b061WIusDZCvVrtgezh5AiyCE9ElkdT/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "MidiCaps: A Large-scale MIDI Dataset with Text Captions",
      "video": "https://drive.google.com/file/d/1qIrcmvqJRIO-Ov3kpzGRrxay8Mh_jzom/view?usp=drive_link"
    },
    "forum": "198",
    "id": "198",
    "pic_id": "https://drive.google.com/file/d/1oM1Zo-J2dI_yHQeh3Op6J7Bc3eC4oS6W/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting templates and the music-theoretical relevance of the identified repetitions.",
      "abstract": "Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting templates and the music-theoretical relevance of the identified repetitions.",
      "authors": [
        "Ren, Zeng*",
        " Rammos, Yannis",
        " Rohrmeier, Martin A"
      ],
      "authors_and_affil": [
        "Zeng Ren (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)*",
        " Yannis Rammos (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Computational musicology -> mathematical music theory; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> representations of music; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> structure, segmentation, and form",
        "Computational musicology"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1mL_I29Y9OXfstFXm0C2EVB8xozviBLhL/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1g-xHPS7iaP2x5Gbq3DlGXcaiNs5NAXRC/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Formal Modeling of Structural Repetition using Tree Compression",
      "video": "https://drive.google.com/file/d/1bXs3DMiL--96DfbHYoJfdmp4cQe4bIEr/view?usp=drive_link"
    },
    "forum": "207",
    "id": "207",
    "pic_id": "https://drive.google.com/file/d/14P2-03D3SKJsWef0kto2uDsHpqCh4-3h/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",
      "abstract": "Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",
      "authors": [
        "Strahl, Sebastian*",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Sebastian Strahl (International Audio Laboratories Erlangen)*",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music",
        "MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1CHhf2YqFLE4yhviOEnoiWPkquE-MqLBy/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1qYZX0b77rH_vb-cNHkkztkV7ggpFtA3e/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques",
      "video": "https://drive.google.com/file/d/1wLdS2VgE1tyDmEQWvyI0OaVOt-ZAJvL3/view?usp=drive_link"
    },
    "forum": "214",
    "id": "214",
    "pic_id": "https://drive.google.com/file/d/1hvjjP5pOg0rHA1HL7ua4uD2K767PJ1_m/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",
      "abstract": "Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",
      "authors": [
        "Batlle-Roca, Roser*",
        " Liao, Wei-Hsiang",
        " Serra, Xavier",
        " Mitsufuji, Yuki",
        " Gomez, Emilia"
      ],
      "authors_and_affil": [
        "Roser Batlle-Roca (Universitat Pompeu Fabra)*",
        " Wei-Hsiang Liao (Sony Group Corporation)",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Yuki Mitsufuji (Sony AI)",
        " Emilia Gomez (Joint Research Centre, European Commission & Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Generative Tasks -> evaluation metrics; MIR tasks -> music generation; MIR tasks -> similarity metrics",
        "Evaluation, datasets, and reproducibility -> evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1VxJq1lT9F1gJ64Cvb10en0WeVNUeV0hk/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1WbKAV-UY56QfYuDJhsB4Vzm6A7oUvrp9/view?usp=sharing",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio",
      "video": "https://drive.google.com/file/d/1TWr7oLMJb2Q7ENmhfPF6rpZeDhwSDqMI/view?usp=sharing"
    },
    "forum": "216",
    "id": "216",
    "pic_id": "https://drive.google.com/file/d/1yVUzCS6SITQJSFhIqk1PjpCsML0cAtko/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6\\% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples available at: https://conference-demo-2024.github.io/demo/",
      "abstract": "The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6\\% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples available at: https://conference-demo-2024.github.io/demo/",
      "authors": [
        "Haseeb, Muhammad Taimoor*",
        " Hammoudeh, Ahmad",
        " Xia, Gus"
      ],
      "authors_and_affil": [
        "Muhammad Taimoor Haseeb (Mohamed bin Zayed University of Artificial Intelligence)*",
        " Ahmad Hammoudeh (Mohamed bin Zayed University of Artificial Intelligence)",
        " Gus Xia (Mohamed bin Zayed University of Artificial Intelligence)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Applications -> music retrieval systems; Creativity -> human-ai co-creativity; Creativity -> tools for artists; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1V7_pi-xl8w6NaSXtx19TnfXuNi70nsF-/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/17LX_eqTjhyJwTVsoLxihZWu02qHnwpnY/view?usp=sharing",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "DEEP RECOMBINANT TRANSFORMER: ENHANCING LOOP COMPATIBILITY IN DIGITAL MUSIC PRODUCTION",
      "video": "https://drive.google.com/file/d/15TzFNZPcOT9P5p_Kqe1lpvfH_j5duHDh/view?usp=sharing"
    },
    "forum": "231",
    "id": "231",
    "pic_id": "https://drive.google.com/file/d/1M3c7JEvlngUOURE-mbKB3vSuUBxqHUE3/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",
      "abstract": "We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",
      "authors": [
        "Sowula, Robert*",
        " Knees, Peter"
      ],
      "authors_and_affil": [
        "Robert Sowula (TU Wien)*",
        " Peter Knees (TU Wien)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Creativity -> tools for artists",
        "Applications -> music recommendation and playlist generation; Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> music interfaces and services; MIR tasks -> similarity metrics; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1c_OY_5sI5pR0HLwObXqRUuE8RYipUOpU/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/14PHc1b8L-XG5xKNVewIRd7dXPUUp78Rw/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-based Stem Modification And Precise Beat-Grid Estimation",
      "video": "https://drive.google.com/file/d/1GZUMFnXUsYbkjjFZsAJkMCERw70Ra7wJ/view?usp=drive_link"
    },
    "forum": "261",
    "id": "261",
    "pic_id": "https://drive.google.com/file/d/1JtyOe4i6Vkr8-5X1FAVecxObcdRjA_8N/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately 3 hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to existing multi-track guitar datasets. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the suitability of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",
      "abstract": "Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately 3 hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to existing multi-track guitar datasets. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the suitability of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",
      "authors": [
        "Glytsos, Marios*",
        " Garoufis, Christos",
        " Zlatintsi, Athanasia",
        " Maragos, Petros"
      ],
      "authors_and_affil": [
        "Marios Glytsos (National Technical University of Athens)*",
        " Christos Garoufis (Athena Research Center)",
        " Athanasia Zlatintsi (Athena Research Center)",
        " Petros Maragos (National Technical University of Athens)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> evaluation metrics; MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1hmHE0nv8wZsj51ajCsSdN_UehrQKrMDt/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1wq-3SzXdmDQmELX5hNjycKnF-RSLLc2_/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Classical Guitar Duet Separation using GuitarDuets - a Dataset of Real and Synthesized Guitar Recordings",
      "video": "https://drive.google.com/file/d/13Iw_NtHQmtN_Z4iend_Py8r4_fUG9F2x/view?usp=drive_link"
    },
    "forum": "262",
    "id": "262",
    "pic_id": "https://drive.google.com/file/d/1tT0xYZ7MVSJG6-Dj4asIYA6Ewm_d1oIz/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",
      "abstract": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",
      "authors": [
        "Desblancs, Dorian*",
        " Meseguer Brocal, Gabriel",
        " Hennequin, Romain",
        " Moussallam, Manuel"
      ],
      "authors_and_affil": [
        "Dorian Desblancs (Deezer Research)*",
        " Gabriel Meseguer Brocal (Deezer)",
        " Romain Hennequin (Deezer Research)",
        " Manuel Moussallam (Deezer)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR tasks -> automatic classification",
        "Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> music signal processing; MIR tasks -> sound source separation; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "From Real to Cloned Singer Identification",
      "video": "https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/view?usp=drive_link"
    },
    "forum": "271",
    "id": "271",
    "pic_id": "https://drive.google.com/file/d/1QDJhWyLpD5Oheyeq0z7cFInOtFksVAxr/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",
      "abstract": "Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",
      "authors": [
        "C\u00edfka, Ond\u0159ej*",
        " Schreiber, Hendrik",
        " Miner, Luke",
        " St\u00f6ter, Fabian-Robert"
      ],
      "authors_and_affil": [
        "Ond\u0159ej C\u00edfka (AudioShake)*",
        " Hendrik Schreiber (AudioShake)",
        " Luke Miner (AudioShake)",
        " Fabian-Robert St\u00f6ter (AudioShake)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/18DaiaPQ46l9FPfs_IX5hI12y0WiaaZg0/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1GbM--7-vFptu10l8mbUFdaf2aYAsuYSy/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Lyrics Transcription for Humans: A Readability-Aware Benchmark",
      "video": "https://drive.google.com/file/d/1aO3TyMqL8iw7MRan4-Z4PEMnpLMu7YOy/view?usp=drive_link"
    },
    "forum": "272",
    "id": "272",
    "pic_id": "https://drive.google.com/file/d/1KeD6qyfmisYMpKuxn_HOed85pqGOg-J2/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.",
      "abstract": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.",
      "authors": [
        "Vasilakis, Yannis*",
        " Bittner, Rachel",
        " Pauwels, Johan"
      ],
      "authors_and_affil": [
        "Yannis Vasilakis (Queen Mary University of London)*",
        " Rachel Bittner (Spotify)",
        " Johan Pauwels (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> automatic classification",
        "MIR fundamentals and methodology -> multimodality"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition",
      "video": "https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/view?usp=drive_link"
    },
    "forum": "275",
    "id": "275",
    "pic_id": "https://drive.google.com/file/d/1Ku9p2tOaHugEDAx1tH0ervgBcpre2E4M/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent text-to-music models have enabled users to generate realistic audio music with a simple command. However, editing music audios remains challenging due to conflicting desiderata: performing fine-grained alterations on the audio while maintaining a simplistic user interface. To address this challenge, we propose Audio Prompt Adapter (or AP Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion text-to-music model. With only 22M trainable parameters, AP Adapter empowers users to harness both global (e.g., style and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP Adapter on three tasks: timbre transfer, style transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",
      "abstract": "Recent text-to-music models have enabled users to generate realistic audio music with a simple command. However, editing music audios remains challenging due to conflicting desiderata: performing fine-grained alterations on the audio while maintaining a simplistic user interface. To address this challenge, we propose Audio Prompt Adapter (or AP Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion text-to-music model. With only 22M trainable parameters, AP Adapter empowers users to harness both global (e.g., style and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP Adapter on three tasks: timbre transfer, style transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",
      "authors": [
        "Tsai, Fang Duo*",
        " Wu, Shih-Lun",
        " Kim, Haven",
        " Chen, Bo-Yu",
        " Cheng, Hao-Chung",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Fang Duo Tsai (National Taiwan University)*",
        " Shih-Lun Wu (Carnegie Mellon University)",
        " Haven Kim (University of California San Diego)",
        " Bo-Yu Chen (National Taiwan University, Rhythm Culture Corporation)",
        " Hao-Chung Cheng (National Taiwan University)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Creativity -> computational creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR and machine learning for musical acoustics; MIR tasks -> music generation",
        "MIR tasks -> music synthesis and transformation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1dDdDgd5TVYl0UorNvzj4f__AftQ9RJRw/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/13QtDnOhEOCD40rDFaCuChfIwGpIuCqys/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning",
      "video": "https://drive.google.com/file/d/1PkwkRsbws4zUxxy_vp0pMi6LWueUjzTY/view?usp=drive_link"
    },
    "forum": "278",
    "id": "278",
    "pic_id": "https://drive.google.com/file/d/1QvWRJ4PyIWLrEsovRVVVOJyKHTSIkvWw/view?usp=drive_link",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the \"main tempo,\" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes \"renotations\" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",
      "abstract": "Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the \"main tempo,\" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes \"renotations\" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",
      "authors": [
        "Condit-Schultz, Nathaniel*"
      ],
      "authors_and_affil": [
        "Nathaniel Condit-Schultz (Georgia Institute of Technology)*"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Computational musicology",
        "Evaluation, datasets, and reproducibility -> annotation protocols; Knowledge-driven approaches to MIR -> cognitive MIR; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR tasks -> music transcription and annotation; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/15s3hLyFhSpdkSLDpThGQ5teU1LuPotu_/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Nrh_OGw3wV0r-3FnXVEaoTD_PFEQXib4/view?usp=drive_link",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow",
      "video": "https://drive.google.com/file/d/1SdhM--el63CU_awSBoQ1atUIbIxzplxz/view?usp=drive_link"
    },
    "forum": "280",
    "id": "280",
    "pic_id": "https://drive.google.com/file/d/12WK4b1sZvF38l3zGomcJAZIixSt8sPvH/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking.   Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not.   We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions).   We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",
      "abstract": "In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking.   Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not.   We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions).   We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",
      "authors": [
        "Gagner\u00e9, Antonin*",
        " Essid, Slim",
        " Peeters, Geoffroy"
      ],
      "authors_and_affil": [
        "Antonin Gagner\u00e9 (LTCI - T\u00e9l\u00e9com Paris, IP Paris)*",
        " Slim Essid (  LTCI - T\u00e9l\u00e9com Paris, IP Paris)",
        " Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZCbNqs3QV7vhfH-HGAD6iaFzzZRvch_g/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1RWkcjSZM-yUW2tDEPU8_q42I7t6igv2Z/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "A Contrastive Self-Supervised Learning scheme for beat tracking amenable to few-shot learning",
      "video": "https://drive.google.com/file/d/1dBlcd_jNwcnBDoKOC1YZOkYYb90I1aSE/view?usp=drive_link"
    },
    "forum": "283",
    "id": "283",
    "pic_id": "https://drive.google.com/file/d/1BPmJ_BnweNHfHsrmFpmCzPF3Q7SdoMh9/view?usp=drive_link",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",
      "abstract": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",
      "authors": [
        "G\u00e9r\u00e9, L\u00e9o*",
        " Audebert, Nicolas",
        " Rigaux, Philippe"
      ],
      "authors_and_affil": [
        "L\u00e9o G\u00e9r\u00e9 (Cnam)*",
        " Nicolas Audebert (IGN)",
        " Philippe Rigaux (Cnam)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR tasks -> automatic classification",
        "Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "Improved symbolic drum style classification with grammar-based hierarchical representations",
      "video": "https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/view?usp=drive_link"
    },
    "forum": "293",
    "id": "293",
    "pic_id": "https://drive.google.com/file/d/14RvMqnf57mymZFZ5vRsTRTR4aCVFpJ5Z/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",
      "abstract": "Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",
      "authors": [
        "Koops, Hendrik Vincent*",
        " Micchi, Gianluca",
        " Quinton, Elio"
      ],
      "authors_and_affil": [
        "Hendrik Vincent Koops (Universal Music Group)*",
        " Gianluca Micchi (Universal Music Group)",
        " Elio Quinton (Universal Music Group)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR tasks -> automatic classification",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1a7Gf7efa-MPguct1Cfnz6Dnqr_adqbMH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1azt9YjZHFULXu2BWk6FQ1413hSq3EIIu/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Robust lossy audio compression identification",
      "video": "https://drive.google.com/file/d/1X8L6NyjPn0YTHVnGCtWLxm4UCZHDXQlU/view?usp=drive_link"
    },
    "forum": "304",
    "id": "304",
    "pic_id": "https://drive.google.com/file/d/10BWHAZtK81FDktXcLFQLNUXNaBiBla4J/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pretraining on unlabeled data is therefore a promising means of improving performance on these tasks, especially because, during pretraining, a model may be expected to acquire latent representations of musical abstractions like keys and chords. However, existing deep learning models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks not involving explicit music theory, like composer or genre classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to predict labels for each note, then aggregate the predictions of simultaneous notes to obtain a single label at each time step. Conditioning the chord predictions on key predictions gives more coherent labels. The resulting model outperforms previous roman numeral analysis models by a substantial margin.",
      "abstract": "Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pretraining on unlabeled data is therefore a promising means of improving performance on these tasks, especially because, during pretraining, a model may be expected to acquire latent representations of musical abstractions like keys and chords. However, existing deep learning models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks not involving explicit music theory, like composer or genre classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to predict labels for each note, then aggregate the predictions of simultaneous notes to obtain a single label at each time step. Conditioning the chord predictions on key predictions gives more coherent labels. The resulting model outperforms previous roman numeral analysis models by a substantial margin.",
      "authors": [
        "Sailor, Malcolm*"
      ],
      "authors_and_affil": [
        "Malcolm Sailor (Yale University)*"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Computational musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; Musical features and properties -> harmony, chords and tonality"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Gqyk2QoOrzJXOPd_LBgchAsnBDJxG6cz/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1shpkSE2PSIr-u-vcBxTryredYcCwPBRu/view?usp=drive_link",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis",
      "video": "https://drive.google.com/file/d/101uolZhDyW2dpOhoPqlgfgy_uM_kyBIu/view?usp=drive_link"
    },
    "forum": "322",
    "id": "322",
    "pic_id": "https://drive.google.com/file/d/1wLkwNpEi0tli-WbJLSHEaU8B3vOOW9zn/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",
      "abstract": "Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",
      "authors": [
        "Preniqi, Vjosa*",
        " Ghinassi, Iacopo",
        " Ive, Julia",
        " Kalimeri, Kyriaki",
        " Saitis, Charalampos"
      ],
      "authors_and_affil": [
        "Vjosa Preniqi (Queen Mary University of London)*",
        " Iacopo Ghinassi (Queen Mary University of London)",
        " Julia Ive (Queen Mary University of London)",
        " Kyriaki Kalimeri (ISI Foundation)",
        " Charalampos Saitis (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> web mining, and natural language processing",
        "MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1nTVfYNUTeLcioLm6T1-A3cCtG9ElEAVF/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1p0hDd03EtOchCgOQqB92j0bjd7Jy64Zy/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Automatic Detection of Moral Values in Music Lyrics",
      "video": "https://drive.google.com/file/d/1EfbWaqUcxnOqSdVEvL2HZbeoWe2FN7sA/view?usp=drive_link"
    },
    "forum": "326",
    "id": "326",
    "pic_id": "https://drive.google.com/file/d/1Eiz0NkthEE7ejBxQ_Z6CA558SfE8LCbr/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",
      "abstract": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",
      "authors": [
        "Manco, Ilaria*",
        " Salamon, Justin",
        " Nieto, Oriol"
      ],
      "authors_and_affil": [
        "Ilaria Manco (Queen Mary University of London)*",
        " Justin Salamon (Adobe)",
        " Oriol Nieto (Adobe)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Applications -> music retrieval systems; Applications -> music videos, multimodal music systems; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",
        "Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1QU9KGnGv96vGy6Jvh-OgdfcY2fPMdXHT/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14QtD9aUQH8b5oBxNlgMwsBMZZnfVywFD/view?usp=sharing",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning",
      "video": "https://drive.google.com/file/d/1lqBFpk6ik1wOBxaBmTtFuFqfSnooydtj/view?usp=sharing"
    },
    "forum": "336",
    "id": "336",
    "pic_id": "https://drive.google.com/file/d/1-ozbdUylrOzfa6KpGiSK5JzLVYCzkie1/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",
      "abstract": "Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",
      "authors": [
        "Fang, Kun*",
        " Wang, Ziyu",
        " Xia, Gus",
        " Fujinaga, Ichiro"
      ],
      "authors_and_affil": [
        "Kun Fang (McGill University)*",
        " Ziyu Wang (NYU Shanghai)",
        " Gus Xia (New York University Shanghai)",
        " Ichiro Fujinaga (McGill University)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo",
        "Creativity -> creative practice involving MIR or generative technology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1faSCXbSbmPlzCZxPF14QMZT8EALvlXf4/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1aCrLu7Ribt416pix7FTkXvv5OuzEzgXr/view?usp=sharing",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Exploring GPT's Ability as a Judge in Music Understanding",
      "video": "https://drive.google.com/file/d/1HUb7E9n3UhZXhSkVTqGJ6sIgazG9fxxv/view?usp=sharing"
    },
    "forum": "345",
    "id": "345",
    "pic_id": "https://drive.google.com/file/d/1vlIVKi9CQqPQwyrkA7BXPjpj-CebbHA7/view?usp=sharing",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",
      "abstract": "The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",
      "authors": [
        "Carvalho, Hugo T*",
        " Li, Min Susan",
        " Di Luca, Massimiliano",
        " Wing, Alan M."
      ],
      "authors_and_affil": [
        "Hugo T. Carvalho (Federal University of Rio de Janeiro)*",
        " Min S. Li (University of Birmingham)",
        " Massimiliano Di Luca (University of Birmingham)",
        " Alan M. Wing (University of Birmingham)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Musical features and properties",
        "MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> expression and performative aspects of music; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1xbuQojY3c7U5aBISEmYQbcgdSrHE7IMA/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PaMyuIzsGJRpviccxxaaSnHvOku7Wjg3/view?usp=sharing",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "A Kalman Filter model for synchronization in musical ensembles",
      "video": "https://drive.google.com/file/d/1YKoc_qGkB-_kV3iYWly-RBG5QGwl3cnE/view?usp=sharing"
    },
    "forum": "347",
    "id": "347",
    "pic_id": "https://drive.google.com/file/d/1Q_1EwrIcYXlMDQR39wgg7zMQEd4ECInT/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",
      "abstract": "Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",
      "authors": [
        "Sivasankar, Adithi Shankar*",
        " Plaja-Roglans, Gen\u00eds",
        " Nuttall, Thomas",
        " Rocamora, Mart\u00edn",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Adithi Shankar (Music Technology Group- Universitat Pompeu Fabra)*",
        " Gen\u00eds Plaja-Roglans (Music Technology Group)",
        " Thomas Nuttall (Universitat Pompeu Fabra, Barcelona)",
        " Mart\u00edn Rocamora (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Computational musicology",
        "Applications -> music videos, multimodal music systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> computational ethnomusicology; MIR tasks -> pattern matching and detection; MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1EQXLNuxVj3e60R2Eovj8U4saxajWlXac/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1dSina5Drt9bNlrD4KEr4FXYmpbSTOkyh/view?usp=sharing",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Saraga Audiovisual: a large multimodal open data collection for the analysis of Carnatic Music",
      "video": "https://drive.google.com/file/d/1Hn3rqQxlbgnb69IGcw8hZj53CRzNeqYm/view?usp=share_link"
    },
    "forum": "352",
    "id": "352",
    "pic_id": "https://drive.google.com/file/d/1WLVY2eot7RHjB-PrJXrM5Q-JEQ2kt0uB/view?usp=share_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to fine-tuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",
      "abstract": "Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to fine-tuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",
      "authors": [
        "Vaidyanathapuram Krishnan, Venkatakrishnan*",
        " Alben, Noel",
        " Nair , Anish  A",
        " Condit-Schultz, Nathaniel"
      ],
      "authors_and_affil": [
        "Venkatakrishnan Vaidyanathapuram Krishnan (Georgia Institute of Technology)*",
        " Noel Alben (Georgia Institute Of Technology)",
        " Anish Nair (Georgia Institute of Technology)",
        " Nathaniel Condit-Schultz (Georgia Institute of Technology)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR tasks -> sound source separation",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1xxZ4RvR9q_4mIj3QBAs3Ey32bl6g2Bqg/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1JuGwSJvFXexbfTXoILktGru4ABzJctxR/view?usp=sharing",
      "session": [
        "5"
      ],
      "slack_channel": "",
      "title": "Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music",
      "video": "https://drive.google.com/file/d/1I5hfnKGshAW6HBgw2YkUG2a9yttdxHN8/view?usp=sharing"
    },
    "forum": "354",
    "id": "354",
    "pic_id": "https://drive.google.com/file/d/1cs7FlC0Wky9WGM0FiKQaryvD70Q5qXAX/view?usp=sharing",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs\u2014this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% \u2192 82%).",
      "abstract": "We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs\u2014this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% \u2192 82%).",
      "authors": [
        "Bukey, Irmak*",
        " Feffer, Michael",
        " Donahue, Chris"
      ],
      "authors_and_affil": [
        "Irmak Bukey (Carnegie Mellon University)*",
        " Michael Feffer (Carnegie Mellon University)",
        " Chris Donahue (CMU)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality; MIR tasks -> optical music recognition; MIR tasks -> pattern matching and detection",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1UvKKzTuptPSWcUG8eLtoRAW27nLrJMCU/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/10SNBKkgmZ93IS5WNXThWLvpxRXXQpTk_/view?usp=sharing",
      "session": [
        "7"
      ],
      "slack_channel": "",
      "title": "Just Label the Repeats for In-The-Wild Audio-to-Score Alignment",
      "video": "https://drive.google.com/file/d/1w4trgsz-MIdKaRspq3X0EEUYFMWbYweP/view?usp=sharing"
    },
    "forum": "357",
    "id": "357",
    "pic_id": "https://drive.google.com/file/d/11zO68fEWu30iyUTulZET7OrLb0i5Aa32/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 496 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. In this paper we also propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",
      "abstract": "Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 496 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. In this paper we also propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",
      "authors": [
        "Yu, Huiran*",
        " Duan, Zhiyao"
      ],
      "authors_and_affil": [
        "Huiran Yu (University of Rochester)*",
        " Zhiyao Duan (University of Rochester)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Guv-I2ac5iDOWntRs6M5MdZs2FiPLK48/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1eKvu4ZSNFuwX4KW91A-KyGFHRCYjjZrt/view?usp=drive_link",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Note-Level Transcription of Choral Music",
      "video": "https://drive.google.com/file/d/1tcFjNk0p0z14T5IvU-1h-kK3Ms3LR3Wq/view?usp=drive_link"
    },
    "forum": "364",
    "id": "364",
    "pic_id": "https://drive.google.com/file/d/1sTC_5dpgBbtQ6UWcsolKyXFrZwIQlJSp/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcription-based lyrics embeddings (TLE). These estimate `ground-truth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, `ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",
      "abstract": "The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcription-based lyrics embeddings (TLE). These estimate `ground-truth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, `ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",
      "authors": [
        "Kim, Jaehun*",
        " Henkel, Florian",
        " Landau, Camilo",
        " Sandberg, Samuel E.",
        " Ehmann, Andreas F."
      ],
      "authors_and_affil": [
        "Jaehun Kim (Pandora / SiriusXM)*",
        " Florian Henkel (SiriusXM + Pandora)",
        " Camilo Landau (Pandora / SiriusXM)",
        " Samuel E. Sandberg (SiriusXM + Pandora)",
        " Andreas F. Ehmann (SiriusXM + Pandora)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> multimodality; MIR tasks -> automatic classification; Musical features and properties -> representations of music",
        "MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13bjL3Kt-vHf_PaWqoTbIxu6DNnD7W3PK/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1x7bDU2EIR_cbGPvGTegxp6deDMf933BJ/view?usp=drive_link",
      "session": [
        "2"
      ],
      "slack_channel": "",
      "title": "Transcription-based lyrics embeddings: simple extraction of effective lyrics embeddings from audio",
      "video": "https://drive.google.com/file/d/1hX06JcKiPBAFV2-1O6VsUpYluNbqE6WP/view?usp=drive_link"
    },
    "forum": "365",
    "id": "365",
    "pic_id": "https://drive.google.com/file/d/1javSHd7p_gz4nV107bjWb4al7WrnltSH/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",
      "abstract": "The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",
      "authors": [
        "Buisson, Morgan*",
        " McFee, Brian",
        " Essid, Slim"
      ],
      "authors_and_affil": [
        "Morgan Buisson (Telecom-Paris)*",
        " Brian McFee (New York University)",
        " Slim Essid (Telecom Paris - Institut Polytechnique de Paris)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "",
        "Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1J7VODLY0uirvo78S5oTCZ2Sj1uZWwc8H/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14wfben86kWD1tsG3bRS-8e_RcM5kjQ6V/view?usp=sharing",
      "session": [
        "1"
      ],
      "slack_channel": "",
      "title": "Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis",
      "video": "https://drive.google.com/file/d/1ItBVYLES5QsKMp0kVJ6DAfRvT3JgHrqa/view?usp=drive_link"
    },
    "forum": "405",
    "id": "405",
    "pic_id": "https://drive.google.com/file/d/1DZNsWdPBl_rb51o_SxePiDnzqOt7bPY4/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Yigitcan \u00d6zer, Simon Schw\u00e4r, Vlora Arifi-M\u00fcller, Jeremy Lawrence, Emre Sen, and Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Yigitcan \u00d6zer, Simon Schw\u00e4r, Vlora Arifi-M\u00fcller, Jeremy Lawrence, Emre Sen, and Meinard M\u00fcller*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/160/files/6501b44ff32d9.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1kIG7BA9W4UW50walYN7YM4-XdJ2BUnWq/view?usp=drive_link",
      "session": [
        "3"
      ],
      "slack_channel": "",
      "title": "Piano Concerto Dataset (PCD): A Multitrack Dataset of Piano Concertos",
      "video": "https://drive.google.com/file/d/1Sa42785sl1JOyLRjWGbr4ACCeY43lp1A/view?usp=drive_link"
    },
    "forum": "512",
    "id": "512",
    "pic_id": "https://drive.google.com/file/d/1ruV3sh4cOKTG8JRcs2HYbyVsvfbKOHhH/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Simon Schw\u00e4r, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Simon Schw\u00e4r*, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard M\u00fcller"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/166/files/65d89725d616e.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1CYaVjfKHx3wKkEBvak4wmi4tq-YXcEit/view?usp=drive_link",
      "session": [
        "4"
      ],
      "slack_channel": "",
      "title": "A Dataset of Larynx Microphone Recordings for Singing Voice Reconstruction",
      "video": "https://drive.google.com/file/d/1Kri3iUu08qgw67fAQw4VzPZXXzTxuGcB/view?usp=drive_link"
    },
    "forum": "513",
    "id": "513",
    "pic_id": "https://drive.google.com/file/d/1enDWiGur_9rUoPcshOAsAKkfaUjz5XOV/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Gen\u00eds Plaja-Roglans, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron"
      ],
      "authors_and_affil": [
        "Gen\u00eds Plaja-Roglans*, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/137/files/6499954ec1b0b.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1c0Csr3IZrEEsF8klvIve7SypcBMUssWL/view?usp=drive_link",
      "session": [
        "6"
      ],
      "slack_channel": "",
      "title": "Repertoire-Specific Vocal Pitch Data Generation for Improved Melodic Analysis of Carnatic Music",
      "video": "https://drive.google.com/file/d/14GqI2ULCkTiWLrm3iUpAdkqLV0t4fyW6/view?usp=drive_link"
    },
    "forum": "515",
    "id": "515",
    "pic_id": "https://drive.google.com/file/d/1kxxmWxBNn2foiu-p7gMGOiR37tnskogh/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  }
]
