[
  {
    "content": {
      "TLDR": "In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",
      "abstract": "In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",
      "author_changes": "Reviewer #3:\n\n\"The paper only tested with 6 participants and 90 songs.\"\n\nResponse: The study with only 6 participants is prior work that is only being referenced and scrutinized in our paper.\n\n\"The study's focus solely on the GPT series ... potentially overlooks the capabilities of other types of LLMs ...\"\n\nResponse: This limitation of our study is already being discussed at the end of section 8.\n\n\"Furthermore, the absence of detailed information on the experimental LLM prompts and a comparison of responses under different conditions (e.g., with and without Chain of Thought) leaves a gap ...\"\n\nResponse: The exact LLM prompts are provided in section 3.2. Usage of different prompts is discussed in section 7.\n\nReviewer #4:\n\n\"It seems very apparent that a LLM is not the correct tool for the task, there being a host of applications that are able to \"hear\" the raw audio ...\"\n\nResponse: Other work on extracting psychophysical information from text (see [11] and [12]) shows that this kind of work is worth conducting.\n\n\"This means that other than statistical conclusion validity ... the other aspects remain on the abstract, speculative realm.\"\n\nResponse: Throughout the paper additional experiments are being conducted, hence the article is not only \"speculative\".\n\nMeta reviewer:\n\n\"What would be wrong about using the genre as a guiding information for music similarity?\"\n\nResponse: The complicated relations between genre and music similarity are being discussed in section 5.\n\n\"A shortcoming of the paper may be that the influence of varying prompting on the ratings was not investigated.\"\n\nResponse: Usage of different prompts is discussed in section 7.\n\n\"The authors could provide a bit more elaboration of the lack of symbol grounding.\"\n\nResponse: We have the feeling that \"symbol grounding\" is such an elaborate problem, having been discussed for decades in AI research, that going beyond a brief, but important, mentioning in our paper would go beyond the scope of our work.",
      "authors": [
        "Flexer, Arthur*"
      ],
      "authors_and_affil": [
        "Arthur Flexer (Johannes Kepler University Linz)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGCGWSY",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility",
        "Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> web mining, and natural language processing; MIR tasks -> similarity metrics; Philosophical and ethical discussions -> philosophical and methodological foundations"
      ],
      "long_presentation": "FALSE",
      "meta_review": "For this paper, the meta review faces the challenge that two reviews voted for accept (including the initial meta review), and two reviews vote for rejection (both weak reject). None of the reviewers provided any input to the discussion phase despite my repeated encouragement for discussion. This leaves me as the meta reviewer with the texts of the individual initial reviews.\n\nAmong the reviews, I would argue that R3 has made crucial misjudgments and should not be taken into account strongly: R3 points out that \"only few experiments were conducted\". This is a complete misunderstanding: the related experiments had been conducted in the context of published research [10], and the submitted paper applies a different, more theoretical framework to it. Therefore, opinions about the experimental design of [10] must not be imposed on the review judgement for the present paper. Also, R3s argument related to \"Task Representation\" see inappropriate, as I would argue that the paper uses music similarity because it is a typical MIR task, and actually discusses in detail how this interacts with many other cultural constructs such as genre. Therefore, I as the meta-reviewer see little substance in the motivations for R3 to reject the paper.\n\nR4, on the other hand, raises a series of valid concerns, such as that \"no further investigation is made which could actually be rather trivial (like checking the results of alternative LLMs)\". Overall, the critique of the paper by R4 reads similar to R2 and the initial meta-review, with the difference that R4 recommends weak reject.\n\nIn total, I would therefore argue to accept this paper as an outcome of a majority vote of three valid reviews (meta-review, R2, R4). In contrast to the other - more experimental - papers that I reviewed, I would like to emphasize my impression that reviewers seem to find it harder to establish common standards for reviews for such rather conceptual papers. \n\nIn case the paper is indeed accepted I encourage the authors to more carefully emphasize limitations and scop of the paper, to address the concerns expressed in the reviews.",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1dA6YiGRw7WLmcSBG-dCbmqGRjp7DpgnE/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1VAG-uC-9QPHuPzJsxziqRpipsaG-LYLI/view?usp=sharing",
      "review_1": "This paper focuses on using ChatGPT/LLMs to do ``distant reading'', a musicological task pertaining to the analysis of a corpora of data related to music. The primary motivation behind this is that LLMs appear to be designed to be good at this task. \n\nThe prevailing hypothesis here appears to be that ChatGPT's ability to estimate similarity is a sufficient surrogate for human perception of similarity. \n\nThe experimental setup uses a listening study, where they ask people to rate pairs of songs. This is followed up with asking ChatGPT how similar the song pairings are. The paper thoroughly explores the validity of the results through the lens of the statistical, internal, external and construct validity. At the surface, they find that there was a correlation between listener ratings of similarity and ChatGPT's reported understanding of similarity. However, looking at the construct and external validity, the usefulness of ChatGPT appears to fall. \n\nThis paper formulates a fairly straightforward experiment that evaluates the feasibility of using ChatGPT for distant reading. However, as their own experiments show, it is not a particularly useful tool for this purpose. To some extent, it can be argued that this is an expected result. While the underlying LLM models used by ChatGPT and the like are certainly trained on stupendous amounts of data, it is unlikely that they are capable of knowing if song A by artist X is similar to song B by artist Y. \n\nWorks like this are necessary to caution anyone who might assume that ChatGPT, with its infinitely large dataset and ability to answer queries, might be able an appropriate tool for music related tasks directly. I think the rigorous analysis of the validity of running experiments such as this one should be a useful blueprint for anyone seeking to run experiments probing large language models on musical concepts.\n\nThere is a minor reference issue with [3] (Graphs, maps, trees: abstract models).",
      "review_2": "**Strengths:**\n\n1. The article applies LLMs to distant reading in the field of literary studies.\n\n**Weaknesses:**\n\n1. **Model Diversity**: The study's focus solely on the GPT series (GPT-4.0 and GPT-3.5) potentially overlooks the capabilities of other types of LLMs or AI systems like Gemini, Qwen, Claude. This limited selection might skew the generalizability of the results, suggesting the need for broader testing across different models to validate the conclusions.\n\n2. **Task Representation**: The study restricts its evaluation to music similarity tasks to draw conclusions about the broader domain of music knowledge discovery. This narrow focus might not fully encapsulate the complex and multifaceted nature of music understanding, such as emotional recognition or thematic analysis, which could provide a different assessment of LLM capabilities.\n\n3. **Participant and Methodological**: The paper only tested with 6 participants and 90 songs. Furthermore, the absence of detailed information on the experimental LLM prompts and a comparison of responses under different conditions (e.g., with and without Chain of Thought) leaves a gap in understanding how different interaction modes with LLMs might affect the outcomes.\n\n4. **Interpretability and Bias**: The argument that LLMs' \"black box\" nature inherently limits their usefulness in tasks requiring interpretability is notable. However, this critique could also apply to human cognition and subjective biases in music appreciation. The lack of a comparative analysis between human and AI biases and the ability to guide LLM responses with tailored prompts suggest an area for further exploration and potentially undermines the conclusion that LLMs are unsuitable for music knowledge discovery based solely on interpretability issues.",
      "review_3": "\nThis is a very clearly written and revised article that researches the validity (considered from four formal perspectives) of a Large Language Model (namely in this case OpenAIs ChatGPT 3.5) for distant reading of music similarity. This is mainly done via a comparative test with human raters and it is consistently shown that inter-human consistency is larger than human-LLM consistency. The design of the paper is quite comprehensive and it draws from previously vetted material. There is a sense that the research came up with a negative result (which the authors acknowledge was expected) and thus the article became something of a speculation and justification of why this could be so. While this could be an interesting idea, there are two large problems with this endeavour: \n- It seems very apparent that a LLM is not the correct tool for the task, there being a host of applications that are able to \"hear\" the raw audio, which would be the only task that would yield a fair comparison. I do understand that the authors want to look into \"distant reading\", obviously, but there are so many apparent drawbacks from the start, that getting to the actual negative result seems quite pointless. For example, in real-world applications, it would probably be the case that most of the inputs given to the LLM were novel, and thus never considered in the training dataset, or that they would be unlabelled (audio with no name).\n- More importantly, when considering the different aspects of validity, no further investigation is made which could actually be rather trivial (like checking the results of alternative LLMs). This means that other than statistical conclusion validity, which is objectively measured, the other aspects remain on the abstract, speculative realm. Were this a philosophical article, not actually based on the conclusions of an experiment, and it could have been written with this approach (which would need to be very different from the current state). As it is, it is very incomplete by design.\nSo, while this is an interesting read, very clear and well written (no suggestions in terms of language, no typos found, very well-structured), it is too speculative and based on non-proven assumptions for the nature of what it promises to deliver.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-05-on-the-validity",
      "title": "On the validity of employing ChatGPT for distant reading of music similarity",
      "video": "https://drive.google.com/file/d/1XuSiWBgkUfBCnZLa-s5uAkZIua7E-p0C/view?usp=drive_link"
    },
    "forum": "1",
    "id": "1",
    "pic_id": "https://drive.google.com/file/d/1bDKJUha7CfprIh1jVxLcMZ1aUzb7_kJK/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "abstract": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "author_changes": "",
      "authors": [
        "Ryu, Jiwoo",
        " Dong, Hao-Wen",
        " Jung, Jongmin",
        " Jeong, Dasaem*"
      ],
      "authors_and_affil": [
        "Jiwoo Ryu (Sogang University)",
        " Hao-Wen Dong (University of Michigan)",
        " Jongmin Jung (Sogang University)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2ML0VSM",
      "day": "2",
      "keywords": [
        "MIR tasks -> music generation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1E8CTFJvSUaksQAHwn5DzFUtPTiuSNZ4L/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/19ZkRhN2wiJdL1LnQt8dfOMXD__2YV6LW/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-12-nested-music-transformer",
      "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
      "video": "https://drive.google.com/file/d/1d4vhwOeI7Ve1w0niVr5Y-qRko_ONQcoN/view?usp=share_link"
    },
    "forum": "5",
    "id": "5",
    "pic_id": "https://drive.google.com/file/d/1ulzf3JND3fCeQwGLr8ueKujOh5F-arSI/view?usp=sharing",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In Music Information Retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automatic music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio\u2013audio and audio\u2013score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",
      "abstract": "In Music Information Retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automatic music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio\u2013audio and audio\u2013score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",
      "author_changes": "",
      "authors": [
        "Zeitler, Johannes*",
        " Maman, Ben",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        " Ben Maman (Tel Aviv University)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU6SEN7",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ovrLChf92BfHq_ilyOqp6VIm6DOYTKMZ/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1Yeeif-eM52-Vv73rbePu4_580JjPKdNH/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-09-robust-and-accurate",
      "title": "Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models",
      "video": "https://drive.google.com/file/d/1YObvTral3sonXEVYqTR-gIs_grqizI72/view?usp=sharing"
    },
    "forum": "8",
    "id": "8",
    "pic_id": "https://drive.google.com/file/d/1Xhbn8tKt43J3OItlISBo2JOUww4rs3AS/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score typesetting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",
      "abstract": "This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score typesetting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",
      "author_changes": "",
      "authors": [
        "Foscarin, Francesco*",
        " Karystinaios, Emmanouil",
        " Nakamura, Eita",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Emmanouil Karystinaios (Johannes Kepler University)",
        " Eita Nakamura (Kyoto University)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9EY9AAK",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1PmF0u_7o7ZK5P0SYyGiQOs18Xog2OOlS/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PamUFD3n7kGV__DCVZPZd3TVHjgpDztW/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-01-cluster-and-separate",
      "title": "Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving",
      "video": "https://drive.google.com/file/d/1QAMVD9519WvHgz5Dlh39j8YCZtjUZ62O/view?usp=sharing"
    },
    "forum": "9",
    "id": "9",
    "pic_id": "https://drive.google.com/file/d/17pv5SIysX3ao3sQ187USvJryc5unEp4Q/view?usp=share_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets -- including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations -- and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",
      "abstract": "We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets -- including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations -- and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",
      "author_changes": "",
      "authors": [
        "Foscarin, Francesco*",
        " Schl\u00fcter, Jan",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Jan Schl\u00fcter (JKU Linz)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5S4VJR",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1h9AImUGC_47nMT_rV1AYJOGvFkdoAEu8/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/15AS9uKpRbx36lXTLVpiOlfpfXnXAw-tY/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-19-beat-this-accurate",
      "title": "Beat this! Accurate beat tracking without DBN postprocessing",
      "video": "https://drive.google.com/file/d/1-ll3kj92H0jJTdQ851VIO8lXCBDw_qge/view?usp=sharing"
    },
    "forum": "10",
    "id": "10",
    "pic_id": "https://drive.google.com/file/d/18I-8CqusUouwV1uqsWd8r2pq_NrqQR-X/view?usp=sharing",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",
      "abstract": "This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",
      "author_changes": "",
      "authors": [
        "Chen, Tsung-Ping*",
        " Yoshii, Kazuyoshi"
      ],
      "authors_and_affil": [
        "Tsung-Ping Chen (Kyoto University)*",
        " Kazuyoshi Yoshii (Kyoto University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCQWBJ3A",
      "day": "1",
      "keywords": [
        "Musical features and properties -> representations of music",
        "Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13xy02rQcrgv5C8TZKQF17dUUO3CkQP52/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14RwLEkmKADHV1DAkWTXs6rCRe0KDXOoa/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-18-learning-multifaceted-self",
      "title": "Learning Multifaceted Self-Similarity over Time and Frequency for Music Structure Analysis",
      "video": "https://drive.google.com/file/d/112O5L8XUnDxGENRx4lG4MJSImunVRW5p/view?usp=sharing"
    },
    "forum": "12",
    "id": "12",
    "pic_id": "https://drive.google.com/file/d/1gw9zu9cfPC_Z8Ar-nB8AiYEhOUKOWORL/view?usp=sharing",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Deep learning has significantly advanced music source separation (MSS), aiming to decompose music recordings into individual tracks corresponding to singing or specific instruments. Typically, results are evaluated using quantitative measures like signal-to-distortion ratio (SDR) computed for entire excerpts or songs. As the main contribution of this article, we introduce a novel evaluation approach that decomposes an audio track into musically meaningful sound events and applies the evaluation metric based on these units. In a case study, we apply this strategy to the challenging task of separating piano concerto recordings into piano and orchestra tracks. To assess piano separation quality, we use a score-informed nonnegative matrix factorization approach to decompose the reference and separated piano tracks into notewise sound events. In our experiments assessing various MSS systems, we demonstrate that our notewise evaluation, which takes into account factors such as pitch range and musical complexity, enhances the comprehension of both the results of source separation and the intricacies within the underlying music.",
      "abstract": "Deep learning has significantly advanced music source separation (MSS), aiming to decompose music recordings into individual tracks corresponding to singing or specific instruments. Typically, results are evaluated using quantitative measures like signal-to-distortion ratio (SDR) computed for entire excerpts or songs. As the main contribution of this article, we introduce a novel evaluation approach that decomposes an audio track into musically meaningful sound events and applies the evaluation metric based on these units. In a case study, we apply this strategy to the challenging task of separating piano concerto recordings into piano and orchestra tracks. To assess piano separation quality, we use a score-informed nonnegative matrix factorization approach to decompose the reference and separated piano tracks into notewise sound events. In our experiments assessing various MSS systems, we demonstrate that our notewise evaluation, which takes into account factors such as pitch range and musical complexity, enhances the comprehension of both the results of source separation and the intricacies within the underlying music.",
      "author_changes": "",
      "authors": [
        "\u00d6zer, Yigitcan*",
        " Berendes, Hans-Ulrich",
        " Arifi-M\u00fcller, Vlora",
        " St\u00f6ter, Fabian-Robert",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Yigitcan \u00d6zer (International Audio Laboratories Erlangen)*",
        " Hans-Ulrich Berendes (International Audio Laboratories Erlangen)",
        " Vlora Arifi-M\u00fcller (International Audio Laboratories Erlangen )",
        " Fabian-Robert St\u00f6ter (AudioShake, Inc.)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1C8K5Y",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> sound source separation",
        "Evaluation, datasets, and reproducibility -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1oJhm0Gj0qHwbdpcwU5kuagau9KlttYKn/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1I-RG-YNaUPQlj5PR26JqkwFtNroKfMXf/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-05-notewise-evaluation-of",
      "title": "Notewise Evaluation of Source Separation: A Case Study For Separated Piano Tracks",
      "video": "https://drive.google.com/file/d/1zvXE3Zc-RS1CJEoRxMBhb4vsW6K-FXzZ/view?usp=drive_link"
    },
    "forum": "13",
    "id": "13",
    "pic_id": "https://drive.google.com/file/d/1GZzsLWg_k1nqfpWHE63d07ZcfwtJw1yu/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper describes a streaming audio-to-MIDI piano transcription approach that aims to sequentially translate a music signal into a sequence of note onset and offset events. The sequence-to-sequence nature of this task may call for the computationally-intensive transformer model for better performance, which has recently been used for offline transcription benchmarks and could be extended for streaming transcription with causal attention mechanisms. We assume that the performance limitation of this naive approach lies in the decoder. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive Transformer decoder detecting a variable number of onset events and another decoder detecting the offset events for the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.",
      "abstract": "This paper describes a streaming audio-to-MIDI piano transcription approach that aims to sequentially translate a music signal into a sequence of note onset and offset events. The sequence-to-sequence nature of this task may call for the computationally-intensive transformer model for better performance, which has recently been used for offline transcription benchmarks and could be extended for streaming transcription with causal attention mechanisms. We assume that the performance limitation of this naive approach lies in the decoder. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive Transformer decoder detecting a variable number of onset events and another decoder detecting the offset events for the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.",
      "author_changes": "We truly appreciate the valuable comments and suggestions and have made corresponding modifications on the original manuscript as much as possible.\n\nTo Reviewer 1:\n\n1. We have updated Figure 2 to illustrate how encoder features are fed to the decoder, and clarified this process in Section 3.1.\n\n2. Figure 2 has been revised to demonstrate the relationship between layer normalization and other layers.\n\n3. We've added Table 1 to showcase the detection of sustain pedal events and the resulting output sequences.\n\n4. In Section 3.1, we now emphasize that the offset decoder does not perform sequence prediction, but instead predicts the offset for each detected onset simultaneously.\n\n5. The caption for Algorithm 1 now specifies that index 0 in the offset sequence represents tokens for the sustain pedal.\n\nTo Reviewers 2 and 3:\n\n1. We have corrected the spelling mistakes throughout the paper.\n\n2. The \"while True\" loop has been removed from Algorithm 1 to improve clarity.\n\n3. To avoid confusion, we've replaced \"frame-level accuracy\" with \"frame-level transcription performance\" throughout the paper.\n\nTo Meta Reviewer:\n\n1. Section 4.1.3 now includes an explanation for maintaining the full vocabulary for all decoders.\n\n2. We've added an explanation for consistent onset and offset decoding in Section 3.4.\n\n3. We've included the missing references:\n   - Kwon et al., \"Polyphonic piano transcription using autoregressive multi-state note model,\" ISMIR 2020.\n   - Dasaem Jeong, \"Real-time Automatic Piano Music Transcription System,\" ISMIR LBD 2020.\n\n4. We acknowledge that Automatic Music Transcription (AMT) and Automatic Speech Recognition (ASR) face different challenges.\n\nTo All Reviewers:\n\nDue to limited research resources, we did not conduct more extensive model analysis. As noted in our conclusion, we recognize that decoding every frame may lead to unnecessary computations. We plan to address these issues and improve our model in future work and open-source our model in the future.",
      "authors": [
        "Wei, Weixing*",
        " Zhao, Jiahao",
        " Wu, Yulun",
        " Yoshii, Kazuyoshi"
      ],
      "authors_and_affil": [
        "Weixing Wei (Kyoto University)*",
        " Jiahao Zhao (Kyoto University)",
        " Yulun Wu (Fudan University)",
        " Kazuyoshi Yoshii (Kyoto University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGD286Q",
      "day": "3",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper received generally good ratings from the four reviewers, as the proposed model achieves high performance while working in real time, which has been less studied. The main weakness is the lack of clarity in the model descriptions. In particular, reviewer #1 pointed out a lot of missing details that make it difficult to understand the exact model operation, and thus recommended a weak rejection. Through discussion, the reviewers concluded that many of the issues can be addressed in the camera-ready version and that the main contributions of the paper deserve to be presented at the ISMIR conference. Therefore, the authors are strongly encouraged to incorporate all comments in the revision and improve the clarity.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1W4gBY_EmjutsJSn1xUUT4YikW3Nmbe1G/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/16lZk-YBqSk_ypmZkZlSXqRroFivWVxY3/view?usp=sharing",
      "review_1": "This paper proposes a streaming-capable automatic piano transcription method. Automatic piano transcription is a well-known task with a somewhat standardized evaluation method used since the Onsets and Frames paper (Hawthorne 2018). To make the model streaming-capable, the authors improve upon the sequence-to-sequence piano transcription model (Hawthorne 2021) so that rather than generating a full sequence of MIDI-like tokens, the decoder concerns onset or offset events in a single time frame only, at each time. The onset decoder autoregressively predicts the sequence of note onsets, and the offset decoder takes the set of active onsets and non-autoregressively predicts if any of them reached the offset.\n\nThe results in Table 1-2 show that the proposed method performs competitively with other SOTA models, while achieving 380 milliseconds of streaming latency, which I consider a strong result. While real-time / streaming capabilities are often overlooked in academic settings, a streaming-capable model can have a huge impact on the usability and interactive applications.\n\nThe proposed method section could be improved a lot to have clearer descriptions of the model:\n\n- some details of the model architecture are missing, such as how the encoder features are fed to the decoders; I could infer from Figure 2 that it's using cross attention (aka encoder-decoder attention) on the features with a causal mask to enable streaming, but this should have been explained in the main body of the text.\n- In Figure 2, each decoder layer seems to have layernorms in both the beginning and the end of the block, which is unconventional. (For context, the original transformer paper had layernorms after attention and mlp, but most implementations including T5 put the layernorms before them).\n- There should be a more detailed description on the sustain pedal detection, which in the title. I'm guessing from the output vocabulary that the pedal states are predicted as part of the onset decoder's prediction, but I can't tell if the tokens represent the states (presence or absence, as written in Section 4.1.3) or the state changes (press / release).\n- In that sense, it'd be helpful to include some example output sequences, to clarify if there is a preferred order of the tokens, e.g. BOS, any pedal tokens followed by notes lower to higher, and  then EOS.\n- The offset decoder is described to be operating non-autoregressively, which makes sense because it just needs to determine if each active onset note ended or not at each frame, but it's easy to (mis?)understand from the paper that offset decoder also does sequence prediction, from the notations in Section 3.1 and 3.3.\n- I find the notation in Algorithm 1 is a little inconsistent. I have to guess that the superscript notations like `1:k_1` and `0:n_1` denote the variable-length onset and offset sequences in each time frame, but it's unclear why `Y_t^{0:k_t}` was computed but only `Y_t^{1:k_t}` is outputted. Maybe it is to exclude EOS, or the pedal token? In any case, a caption to the algorithms block would be nice, to provide a high-level overview of the algorithm explaining the overall flow. Lines 19-26 is pretty standard greedy LM decoding, so it could become a single line.\n- Given all the intricacies like the above, an open-source code and model release would be able to foster faster and wider adoption of the streaming piano transcription model.\n\nGiven that we have more space, further analyses like the following would have made it a stronger paper.\n\n- A piano roll representation of the predicted MIDI or the \"posteriorgrams\" could be useful, space permitting, to show a qualitative visualization of the model behavior.\n- Latency analysis: while 380ms is the systematic latency of the streaming architecture, the real-world latency would depend on the time between the actual and predicted onsets/offset events. A histogram of those latencies would be useful for understanding the real-world latency and its variability.\n- A bonus point if a video of a piano performance getting transcribed in real time, showing the player and animated piano roll representation on the screen.\n\n\nTo conclude, I really like the strong results and would love to see the paper accepted and the model becoming available for streaming piano transcription, but as above, I find the overall quality of the text to be below ISMIR's acceptance bar.\n",
      "review_2": "The paper is well presented and the evualuation gaves promising results.\n\nIt would be interesting to comment on the choice of using the same number of frames before and after for the input of the encoders after to reduce latency.\n\nL.5 \u201cmay call for a transformer\u201d <- for using a Transformer model?\nL.52 rephrase to avoid repeat\nL.67 \u201cunder these circumstances\u201d -> to overcome these limitations?\nL.87 \u201crelated *work*\"\nL.384 \"relying on long-term dependency *of* acoustic features\"",
      "review_3": "I found the paper to be clear, well-written, and easy to read. Congratulations to the authors. I don't have a lot of comments, and I think that this paper meets the requirements for publication at ISMIR.\n\nComments:\n- It would be nice to know if the model is actually usable in real-world applications, i.e. if the model is already ready for streaming transcription. An experiment made by the authors in real conditions could be a nice addition to the paper.\n- I find inelegant the use of a \"while True\" loop in the pseudo-code of Algorithm 1. In my opinion, the code could be rewritten to remove this condition while keeping the same behavior (using a while y!= EOS condition and initializing y out of the loop).\n- The code should be made open-source. If the authors intend to make the code open-source, they should mention it (for instance with a placeholder instead of the real link for double-blind review).\n- Line 87: the word \"work\" is missing in \"this section review related WORK\".\n- Line 96: I think there is a word missing after mainstream in \"In APT, the framewise transcription approach has still been the mainstream due to [...]\". Maybe mainstream \"use\" or something of the like?\n- The definitions of \"frame-level accuracy\" and \"note-level accuracy\" are not given in Section 2.1.",
      "session": [
        "6"
      ],
      "slack_channel": "p6-12-streaming-piano-transcription",
      "title": "Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection",
      "video": "https://drive.google.com/file/d/1_iXRxgXqrGNFVHh5XESpnD5jm169EMfM/view?usp=sharing"
    },
    "forum": "18",
    "id": "18",
    "pic_id": "https://drive.google.com/file/d/1ORPgbshIbOPrG7xibK3s_sUx0GZGmTDU/view?usp=sharing",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",
      "abstract": "Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",
      "author_changes": "",
      "authors": [
        "Arguello, Giulia",
        " Lanzendoerfer, Luca A*",
        " Wattenhofer, Roger"
      ],
      "authors_and_affil": [
        "Giulia Arguello (ETH Zurich)",
        " Luca A Lanzendoerfer (ETH Zurich)*",
        " Roger Wattenhofer (ETH Zurich)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU7B599",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> pattern matching and detection"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1qAbrZKi-ItKh6F5IcxY9SVbtROgDQR2G/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1-MPgcWlKym7T6k4lLhTrt7VIa016_Eqq/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-05-cue-point-estimation",
      "title": "Cue Point Estimation using Object Detection",
      "video": "https://drive.google.com/file/d/1MLy__nYjpuhpzC8A7JoXcbv2S3G-h8V0/view?usp=sharing"
    },
    "forum": "19",
    "id": "19",
    "pic_id": "https://drive.google.com/file/d/1qOroLJ5UcefxgrALmSFTpQtjy1xmlqcU/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "abstract": "In this paper, we propose and investigate the use of neural audio codec language models for automatic generation of sample-based musical instruments based on text or reference audio prompts.  Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes.  We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "author_changes": "Thank you for your acceptance and the insightful feedback on our work!\n\nNow, we connect the paper to our preliminary work, which was a workshop presentation that was not officially published/archived as per mlforaudioworkshop.com. Relative to that, T2I is significantly expanded here (e.g., new CLAP conditioning variants, 2 new metrics, MAGNeT). While we are the first to use LMs for S2I, we refrain from claiming it as our own considering the cited works (DDSP, GANstrument).\n\nWhile we acknowledge that generating specific parts of the fully assembled sample (e.g., attack/release samples) is indeed interesting, we consider the sampler design as industry-specific and omitted a discussion due to space constraints.\n\nWe aim to add more out-of-domain S2I examples to our demo page by ISMIR. Nonetheless, our existing test set results include samples and instruments not seen during training.\n\nWe have retained our notation, which we carefully considered prior to submission. With $\\mathbf{x}_k(\u2026)$, the args $(\u2026)$ enable the selection of different waveforms $\\mathbf{x}_k$. Equation 1 is defined to encapsulate both AR and non-AR processes. Although the equations may appear similar, they capture the subtleties of the various topics introduced in this paper, which are essential to our methodology.\n\nNow, we note that DAC supports up to 9 codebooks. As listed in our future work, we have since fine-tuned DAC and trained corresponding LMs, and hope to provide examples on our site leading up to ISMIR.\n\nGenerally, we have addressed connectors, typos, and notation reminders, linking AR to next-token prediction for clarity. We add the T2I acronym upon its first use. We retain the original text in line 73-80 due to favorable comments from the meta-reviewer. While we aspire to provide code in the future, our current industry position precludes us from doing so at this time.\n\nThank you again for your constructive feedback and the opportunity to present our work at ISMIR 2024!",
      "authors": [
        "Nercessian, Shahan*",
        " Imort, Johannes",
        " Devis, Ninon",
        " Blang, Frederik"
      ],
      "authors_and_affil": [
        "Shahan Nercessian (Native Instruments)*",
        " Johannes Imort (Native Instruments)",
        " Ninon Devis (Native Instruments)",
        " Frederik Blang (Native Instruments)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCM7A9L",
      "day": "4",
      "keywords": [
        "Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "All the reviewers agree that this is a novel and solid piece of work that should be presented at the conference.  The reviewers also mentioned a few points that the authors can take into account to further improve the quality of the paper.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1JSNa6kdSy1HbTaV90ziXODAfPvCMRS3W/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1cz6Qnzc3sW1F5EOUEFICXT1oEd6PMDyK/view?usp=sharing",
      "review_1": "This work addresses the task of generating a sampler instrument. That is, it applies a generative model to produce a set of timbrally coherent audio samples that can be mapped to a range of MIDI velocities and pitches in such a way that creates a playable instrument.\n\nOrthogonal to my review of the work\u2019s scientific merit, I want to commend the authors for focusing on a task that is of clear creative benefit to musicians, i.e. by creating playable instruments, as opposed to simple end-to-end generation of complete tracks. This is a valuable research direction and a strong example to the community of how work on generative AI can positively impact creative professionals.\n\nIt appears this paper is based on work previously presented at the NeurIPS workshop on machine learning for audio. While my view is that this is an excellent piece of work, it does mean that my recommendation really hinges on whether this ISMIR submission contains sufficient novelty to justify its acceptance as new work.\n\nAs far as I can tell from the authors stated contributions, the framing of the text-to-instrument task and the audio codec language model solution to the task are close to identical to what was presented in the workshop paper. However, the sample-to-instrument task appears to be novel. Further, the timbral consistency metric was used as a loss function in the workshop paper, but here it appears to be present only as a metric, and is no longer used for training. Further novel contributions include: (i) the adaptation to the average CLAP score metric; (ii) the three CLAP conditioning schemes, which account for the variance induced by pitch and velocity conditioning; and (iii) and the inclusion of experiments on non-autoregressive transformers. Further, the subjective evaluation is also an improvement on the workshop paper. In particular, the choice to use a MUSHRA-style test for the S2I task is arguably more robust.\n\nI do note the absence of any particular discussion of the topic of sampler instrument design. Indeed, sample libraries and sampler instruments constitute a significant portion of the modern music technology industry, so I'm slightly surprised that the authors have not gone into any further detail about what developing a commercially viable sampler instrument actually entails. Considerations such as round robin sampling, separate attack/sustain samples, and the use of sample zones to trade off quality with disk IO and file size are all relevant, and should arguably play a role in determining whether or not the stated aim of the paper has been achieved.\n\nNonetheless, my overall impression is that this work contains sufficient novelty to be of value to the ISMIR community, and so I\u2019m happy to recommend for acceptance.",
      "review_2": "This paper discusses the use of generative systems based on neural audio codec language models to address the task of generating sample-based instruments (i.e. generating several audio files corresponding to different musical notes with different pitches and velocities and with timbral consistency) given a text prompt or an audio example. To the best of our knowledge, the generation of sample-based instruments from text (T2I) or from audio (S2I) are new MIR tasks which are introduced in this paper. The authors provide an evaluation methodology which incorporates a novel metric to evaluate the timbral consistency of a set of sounds, which is a required quality of sample-based instrument sound sets. The authors also propose three variants of a system that address both T2I and S2I tasks, and carry out both quantitative and qualitative evaluations which result in the notion that the proposed systems can successfully propose solutions for the task but a trade-off exists between timbral consistency an expressivity (measured using FAD).\n\nThe paper is well structured and very well-written, with proper references  and providing great detail, however, it is not clear if code will be available to reproduce the results. Even though a link is provided to a companion website, this does seem to only include audio examples and no code so far.\n\nOverall I think this is a relevant paper for the ISMIR community and therefore I recommend to accept it. What follows is a list of minor comments that authors could address to improve the paper:\n\n* L74-80: Maybe rephrase these sentences? I think they are a bit confusing. I guess the point is to highlight the idea that using parametric synthesisers or DDSP-based approaches that would pre-define some parameters is out of the scope of the work because the authors consider this would severely limit the output space, but maybe this could be clarified?\n\n* L87: \"We introduce the text-to-instrument...\" Shouldn't \"sample-to-instrument\" task be also mentioned in this first point? Also the acronyms T2I and S2I should be first indicated here.\n\n* L107: Maybe start the paragraph with a connector? \"The remaining of this paper is organized as follows. Section 2...\"\n\n* L112: illustrate -> illustrates (?)\n\n* L185: \"The instrument family and source type (i.e., acoustic...\" -> I think a couple of examples of instrument families would be good  here.",
      "review_3": "Strengths:\n- The proposal of the T2I/S2I task is reasonably novel and would be of key interest to many at ISMIR\n- The overall parameterization of the codec LMs are straightforward and well thought out, with the randomization of the CLAP conditioning in particular a useful insight for the task.\n\nWeaknesses:\n- The overall writing coherence and presentation is somewhat poor, which significantly impacts the ability to understand the paper in depth. In particular:\n1) Notation for describing the underlying generative process for the codec LM (section 2.4) is rather uncharacteristic of previous work. While one can understand the desire to make notation generalizable between the AR and non-AR methods, the current notation runs is somewhat opaque and obfuscates parts of the generative process (i.e. that the AR model is trained through next-token prediction).\n2) Most of Section 3 (and Figure 2) is extremely hard to parse. It is hard to tell what Figure 2 is referring to, given that it is both referenced as part of Section 3.2 and Section 3.3. As all definitions for each evaluation metric are exceedingly similar, the paper would be significantly improved by streamlining the legibility of this section and more effectively organizing the structure here.",
      "session": [
        "7"
      ],
      "slack_channel": "p7-06-generating-sample-based",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "video": "https://drive.google.com/file/d/1e2NU_vLLkHDZIDgUnV3coAUFXcYRkHDo/view?usp=sharing"
    },
    "forum": "22",
    "id": "22",
    "pic_id": "https://drive.google.com/file/d/19NV6asW0uyiNpY8JILdYGemXZ_T5vo53/view?usp=sharing",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contempor\u00e0nia de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",
      "abstract": "We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contempor\u00e0nia de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",
      "author_changes": "",
      "authors": [
        "Haki, Behzad",
        " Evans, Nicholas*",
        " G\u00f3mez, Daniel",
        " Jord\u00e0, Sergi"
      ],
      "authors_and_affil": [
        "Behzad Haki (Universitat Pompeu Fabra)",
        " Nicholas Evans (Universitat Pompeu Fabra)*",
        " Daniel G\u00f3mez (MTG)",
        " Sergi Jord\u00e0 (Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9EYS9QF",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1HWtECEdTCXBadHE1SZX61uFCqSA5w3Zr/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1_XdGgImckqE1SPlOhdFutotqmWzKJBvF/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-05-el-bongosero-a",
      "title": "El Bongosero: A Crowd-sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired with Drum Patterns",
      "video": "https://drive.google.com/file/d/13GqBs9WeVeweyL3J4_42sBlSgEES6eMq/view?usp=sharing"
    },
    "forum": "24",
    "id": "24",
    "pic_id": "https://drive.google.com/file/d/1unWOlZ-Q98Qpc3py14DSmOx_u_Sz8G8q/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process.  This study conducts a thorough investigation of LLMs' capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs' responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not a free lunch for LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.",
      "abstract": "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process.  This study conducts a thorough investigation of LLMs' capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs' responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not a free lunch for LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.",
      "author_changes": "",
      "authors": [
        "Zhou, Ziya*",
        " Wu, Yuhang",
        " Wu, Zhiyue",
        " Zhang, Xinyue",
        " Yuan, Ruibin",
        " MA, Yinghao",
        " Wang, Lu",
        " Benetos, Emmanouil",
        " Xue, Wei",
        " Guo, Yike"
      ],
      "authors_and_affil": [
        "Ziya Zhou (HKUST)*",
        " Yuhang Wu (Multimodal Art Projection)",
        " Zhiyue Wu (Shenzhen University)",
        " Xinyue Zhang (Multimodal Art Projection)",
        " Ruibin Yuan (CMU)",
        " Yinghao MA (Queen Mary University of London)",
        " Lu Wang (Shenzhen University)",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Wei Xue (The Hong Kong University of Science and Technology)",
        " Yike Guo (Hong Kong University of Science and Technology)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCQWS056",
      "day": "1",
      "keywords": [
        "Creativity -> computational creativity; Creativity -> human-ai co-creativity; Human-centered MIR -> human-computer interaction; Human-centered MIR -> user-centered evaluation",
        "Generative Tasks -> artistically-inspired generative tasks"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1WF46yo_uNSFyI1eaPBltBD7KXN_H9Rp2/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Kf0_BAAu_F9nHnu5naMyzlHcUsLZYZK0/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-07-can-llms-reason",
      "title": "Can LLMs \"Reason\" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation",
      "video": "https://drive.google.com/file/d/1sN3MNgFzS8TKqubiNFncKUlVPjsvpF02/view?usp=sharing"
    },
    "forum": "31",
    "id": "31",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet \u2014 at only 24.9 M trainable parameters \u2014 performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",
      "abstract": "Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet \u2014 at only 24.9 M trainable parameters \u2014 performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",
      "author_changes": "We would like to thank the reviewers for their detailed reviews. \n\nThe changes made to the paper compared other the initial submission are as follows.\n\n- Additional relevant work have been included. (Wang et al., ICASSP 2022 was already included as [20] in the initial submission).\n- The discussion on Moises\u2019 system has been cut down and other commercial systems were mentioned. Our intention was to highlight inflexibility in existing commercial systems, but we could understand that this might be misinterpreted.\n- Thank you to R4 for spotting the typo in Sec 3.1. We fixed it to NFFT = 2(F-1) = 2048.\n- We added a paragraph motivating the use of embedding-based query instead of class-labled query in S3.3.\n- Alt text have been added to the figures.\n- Acknowledgement and ethics statement have been added.\n",
      "authors": [
        "Watcharasupat, Karn N*",
        " Lerch, Alexander"
      ],
      "authors_and_affil": [
        "Karn N Watcharasupat (Georgia Institute of Technology)*",
        " Alexander Lerch (Georgia Institute of Technology)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9EZ2NH5",
      "day": "4",
      "keywords": [
        "MIR tasks -> sound source separation",
        "Creativity -> tools for artists; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> indexing and querying; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper prompted a lively discussion amongst the reviewers, ultimately reaching a consensus that it should be accepted.\n\nWe ask that the authors make minor revisions to better-motivate the use of a query-based system (as opposed to instrument labels). The advantages of the query-based approach are a bit unclear, given that the paper does not study generalization to instruments unseen during training (which would obviously preclude a label-based approach).\n\nPlease also incorporate the missing references identified in the reviews.\n\nFinally, the comments about Moises around Line 85 could be interpreted as advertisement, which has no place in an academic paper. Please consider toning down this discussion of Moises, or adding commentary on other >4 stem commercial systems (e.g., Lalal.ai, audioshake).",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1o7k0dD9lJ6aP0DDtX2SxXA-UO8QwSQXs/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1-A7wIS_Eo0VTqp6yVFjC7x_nyyhnTvW_/view?usp=drive_link",
      "review_1": "This paper presents a system called Banquet, which aims to separate musical audio into different parts (stems) using a single decoder. The system is designed to handle more than the usual four stems (vocals, drums, bass, and other) and uses a query-based approach to identify which instrument to separate. The strategy of using one decoder for multiple stems is useful because it simplifies the model and reduces computational complexity. The system integrates a bandsplit source separation model with a query-based setup and a music instrument recognition model (PaSST). The results are competitive wrt. the SOTA HT-Demucs, even though they only exceed its SNR slightly for guitar and piano.\n\nThe authors show that Banquet can perform well on common instrument types like vocals, drums, and bass, and even some less common ones like guitar and piano using the MoisesDB dataset. This suggests that their approach has potential in broader applications. I appreciate that the authors aim to push beyond the standard four-stem setup and address the need for more flexible source separation systems.\n\nThe paper is well-written and well structured, has extensive discussion and a valid scientific approach.\n\nMy main concern is that the authors don't motivate clearly their use of a query-based approach. At train time, they use instrument labels to retrieve queries from other songs. Therefore, a naive approach to simply use the class labels as conditioning (with some embedding layer) could work equally well. Apparently, the authors also didn't understand that conditioning on queries from different tracks defies the idea of a query-based approach, as they note in the discussion \"Interestingly, it appears that querying with excerpts from the same or different track did not affect the model performance for most stems except for electric piano.\" However, it is expected that the model cannot do better with same-song queries, as it was trained on all-song queries using instrument labels.\n\nMore detailed remarks:\n\n- The performance on long-tail instruments (less common instruments) is relatively weak suggesting that the model couldn't generalize by interpolating in the PaSSt space. This puts into question the query-based approach.\n- The model\u2019s training approach involves using queries from the same stem type across different songs. I feel this undermines the self-supervised potential of the method because it relies on knowing the instrument class. It seems like a missed opportunity to fully explore the benefits of self-supervised learning. When using queries in a supervised setting, one would at least need to compare to a non-query based approach (i.e., conditioning on the instrument label) to show that there are advantages.\n- The authors missed some important references for query-based approaches, which makes it harder to place their work in the context of existing research. It is generally difficult to locate the work within other works.\n- Single decoder approaches already existed. I am not sure what the new contribution is in this paper, besides using the not-yet-used MoisesDB dataset. It would be nice if the authors would have provide a better motivation and explanation to show what makes this work different or better compared to others.\n\nMinor remarks:\n\n- The use of the term \"stem\" is confusing. I think, \"stem type\" or \"instrument type/class\" would work better.\n- The training data for long-tail instruments is very limited. The authors could have considered more aggressive data augmentation and transfer learning techniques to help improve the model\u2019s performance on less common instruments. Another option to achieve more training examples would be to randomly mix stems of the dataset, as it is common in many SOTA source-separation works.\n- The paper could benefit from more detailed comparisons with existing query-based and single-decoder systems to clearly show improvements or differences. This would help in understanding the real contributions of this work.\n\nConclusion:\n\nThe paper proposes an interesting system for music source separation using a single decoder and a query-based approach. I appreciate the authors' effort to create a more flexible system that can handle more than just the standard four stems. However, it has some weaknesses, especially it doesn't motivate the query-based approach and doesn't show that it improves over simply using class labels. The contribution of this work is unclear given existing single decoder and query-based approaches.\n\nConsequently, I don't think the paper is ready for publication in ISMIR 2024.",
      "review_2": "The paper presents an approach for music source separation beyond the traditional 4 stem setup of vocals, other, drums, and bass (referred to as VODB). In order to generalise the model capabilities to more stems, they introduced a query-based conditioning network, which based on PassT embeddings, is fed to the network. In experiments performed on MoisesDB, they evaluate the model and the subsequent stem extensions.\n\nThe paper starts very strong and the authors nicely embed the paper in the current literature. However, as mentioned above, some works are missing. Especially the conditioning through the FiLM layer was proposed earlier and should be credited as prior work. Furthermore, the introduction is very focused on the Moises DB and the models Moises is currently offering. This should be reformulated in a more neutral way or can be left out since it does not affect the research in the paper.\n\nSection 3 is challenging because the system itself is complex, but the writing is clear and concise. Section 4.1 describes the query extraction. There seems to be a significant amount of pre-processing involved to obtain the \"correct\" query. How sensitive is the whole process on the query selection? Is it necessary to take the \"top-1\" query or is a random query good enough? Since PaSST takes 10s chunks, there might also be enough information in any source chunk unless it's not silence (or a certain amount of energy). This remains unclear and is not further discussed. \nSection 5 describes the experiments and the results. It shows structured experiments on the different levels of stem details, from coarse to fine. \n\nAs for reproducibility, the authors provide the source code. By checking the code, it misses an environment.yaml which would be nice to be added. Otherwise it is very clean code and will help subsequent research. The results for the figures and tables are provided through CSV files. Since much compute time went into the models, will the weights be published, as well? Furthermore, I have not found the query positions for the PaSST embeddings, these might be crucial for later comparison.\n\nAll in all, I recommend this paper to be published at ISMIR 2024. It steers into a direction many researchers are no tapping. However, it is very much focused on the MoisesDB, although not >4-stems, it would be interesting to see this system's performance on Musdb, since this data is so well known in the community. Furthermore, evaluation on URMP would be a plus and maybe something to take into account in future research (although very far from the training data).\n\nSmall typos:\n* footnote 7: word is missing after \"for\"",
      "review_3": "This paper presents an approach for deep learning-based music source separation, using a query by example architecture, such that arbitrary types of musical sources can be extracted using the same model. The paper contains a thorough literature review, and to the best of my knowledge, the most thorough study of music source separation beyond four sources using a dataset of real music with singing voice. While the study is not perfect as noted below, I believe it is a worthy contribution to ISMIR.\n\nSpecific Comments:\n\n- Sec. 3.1: The notation in this section feels a bit sloppy. N_{FFT}=2F=2048, isn't correct, since the input audio signal is real, unless the dc or nyquist bin is dropped. Also, wouldn't splitting the signal into subbands decrease the number of frequency bands. I believe this is represented properly in Fig. 1\n\n- Regarding using query-by-example for picking the instrument to extract. Why not just use a learned embedding vector for each class (i.e., instrument stem type)? Given the lack of difference when using the stem from the same song vs. a different song, it seems that the network is potentially doing this anyway. It would be nice to comment or compare with this.\n\n-Related to the above comment, it would be nice if the authors considered performance on source types that were not included in the training set. This could be one potential advantage of using query-by-example?\n\n- It would be nice to have some baseline results, e.g., an oracle mask and noisy samples for the fine grained stems to put the presented results in context?\n\n- Using SNR as the evaluation metric, is it surprising that RMS level is correlated with performance? Would be better to repeat this analysis with SNR improvement.",
      "session": [
        "7"
      ],
      "slack_channel": "p7-12-a-stem-agnostic",
      "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
      "video": "https://drive.google.com/file/d/1mujQfs5mHI_SB15pzwPf0CUSTuoPyDXg/view?usp=drive_link"
    },
    "forum": "32",
    "id": "32",
    "pic_id": "https://drive.google.com/file/d/1x1wJNvbQ6PwtUhqMOxJVfLOhPviARX2I/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "We introduce a project that revives a piece of 15th-century Korean court music, Chwipunghyeong, composed upon the poem 'Songs of the Dragon Flying to Heaven'. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chwipunghyeong was evaluated by experts and is scheduled to be performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",
      "abstract": "We introduce a project that revives a piece of 15th-century Korean court music, Chwipunghyeong, composed upon the poem 'Songs of the Dragon Flying to Heaven'. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chwipunghyeong was evaluated by experts and is scheduled to be performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",
      "author_changes": "",
      "authors": [
        "Han, Danbinaerin",
        " Gotham, Mark R H",
        " Kim, DongMin",
        " Park, Hannah",
        " Lee, Sihun",
        " Jeong, Dasaem*"
      ],
      "authors_and_affil": [
        "Danbinaerin Han (KAIST)",
        " Mark R H Gotham (Durham)",
        " DongMin Kim (Sogang University)",
        " Hannah Park (Sogang University)",
        " Sihun Lee (Sogang University)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGDGH1A",
      "day": "1",
      "keywords": [
        "Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation",
        "Applications -> music composition, performance, and production"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1DeBMx3GiwXq6Jss-UNAHqg3knxh3NT38/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ToZXB9UB1VjG19bVSX2NzE1xLd10d4E-/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-01-six-dragons-fly",
      "title": "Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding",
      "video": "https://drive.google.com/file/d/1niUE0tEkH0PMP_GE9EhMe1FyckKUHpSr/view?usp=drive_link"
    },
    "forum": "35",
    "id": "35",
    "pic_id": "https://drive.google.com/file/d/1zQ_ErYB9wyqKxrPflKBkMw0Y-cGPqtZc/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",
      "abstract": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",
      "author_changes": "",
      "authors": [
        "Suda, Hitoshi*",
        " Yoshida, Shunsuke",
        " Nakamura, Tomohiko",
        " Fukayama, Satoru",
        " Ogata, Jun"
      ],
      "authors_and_affil": [
        "Hitoshi Suda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Shunsuke Yoshida (The University of Tokyo)",
        " Tomohiko Nakamura (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Satoru Fukayama (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Jun Ogata (AIST)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5SUSMB",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1tP9dkZCAXSH3EY2iZk5znhBUFDXitWFr/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/14XSOt9VO4srbVQIc08K1PqxXeJsczxbX/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-05-fruitsmusic-a-real",
      "title": "FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs",
      "video": "https://drive.google.com/file/d/1uO7oXRmT8aqi3TqoO7X9bXHMi77WpUiP/view?usp=drive_link"
    },
    "forum": "38",
    "id": "38",
    "pic_id": "https://drive.google.com/file/d/1wEaOrGd12pgHCnwduMjbyzjuChxdbBK1/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.",
      "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.",
      "author_changes": "I have modified the paper according to the reviewers comments. The paper has been corrected by a native english speaker. I have modified details and citations according to their comments as well.",
      "authors": [
        "Rouard, Simon*",
        " Defossez, Alexandre",
        " Adi, Yossi",
        " Copet, Jade",
        " Roebel, Axel"
      ],
      "authors_and_affil": [
        "Simon Rouard (Meta AI Research)*",
        " Alexandre Defossez (Kyutai)",
        " Yossi Adi (Facebook AI Research )",
        " Jade Copet (Meta AI Research)",
        " Axel Roebel (IRCAM)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MM27CZ",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper was generally well received by the reviewers. Congrats, authors!\n\nPlease, address the reviewers comments as closely as possible. Especially the following:\n- Cite the appropriate previous work (i.e., this is not the only audio-conditioned music generative model)\n- Have a proficient English speaker to proofread the manuscript",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1AXwd4qZXBWY7jJ12ASDW13nKpzPeW910/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1I1TzCEWABu-BD52VU9EDr0hjPFr3Cg1k/view?usp=drive_link",
      "review_1": "The paper is well-written, with an extensive related work section.  The accompanying website features many interesting examples and code will be released on acceptance. The authors propose to investigate the use of textual inversion with a pretrained MusicGen model to generate variations of an existing song.\n\nTwo similarity metrics are introduced to spot copies in the generated material.\n\nMy main concern is about the original motivation and statement of the problem. If textual inversion proves to be able to capture audio features from the audio conditioning and regenerate audio with similar characteristics, we can regret that this is very close from standard continuation and that the possibilities offered by textual inversion are not fully exploited. Examples like \"Chill lofi remix ex. 1\" from the demo website is very similar to the prompt for instance, \n\nIn this particular case, the learnt textual embeddings are only learnt from one song: I would have liked to see more general and diverse applications. \nFor instance, is it possible to learn new styles / instruments / chord progressions based on a collection of songs? (in this paper style is often used as a replacement for song, which may be misleading). If such application is closer to what's mentioned in the introduction \"Given a few images (3-5) of a concept or object\", it is never discussed in the paper. It would also remove the necessity to introduce bottlenecks.\n\n\nSome questions:\n\"For some song, we never achieve to obtain hearable\n392 music as the result suffers from glitches, and tempo instabilities.\": is it possible to automatically know when this method fails?\n\nSome things to clarify in \u00a74.4:\n-nearest neighbours written i_1^C but chunks indexed with i,j: this is confusing, is it done on the chunk or song level?\n-\"However, if a model copies the conditioning (i.e. xG \u2248 xC) the metric will tend to 1, we thus need a second metric to avoid xG and xC being too similar.\" it sounds as if the metric had an influence on the generation process\n- G is the Nearest Neighbor of C: should be clearer for a subtitle, what are G and C?\n\n\n\n\nInteresting paper but this may seem like a straightforward application of the textual inversion method. Choosing the audio condition from the same song when performing the textual inversion may not be the best use case as this forces the introduction of bottlenecks in the feature extractor. The comparision with different feature extractors is interesting, especially the fact that  \"Self-supervised encoder like MERT and MusicFM outperforms low level acoustic models like EnCodec.\". Demos are of good quality and well presented.",
      "review_2": "Very well written and complete paper with a lot of impressive details and convincing demo results. I have troubles finding anything majorly wrong with this work and just have to say kudos for the impressive piece of work!",
      "review_3": "This paper introduces a method for generating music conditioned on other music \u2014 using a high-level semantic embedding from another piece of music as conditioning. \n\nThe authors evaluate their method against a textual inversion baseline (adapted from the image domain by the authors) and find that their proposed method is faster and generates higher-quality audio than the given baselines. \n\nThe authors perform ablations of several aspects of their system and discuss challenges, like how text conditioning is ignored when audio conditioning is present, and how including too much information from a style embedding can lead to the model perfectly reconstructing the audio. \n\nOverall, I like the authors' approach and find their evaluation sufficient. My recommendation is Strong Accept.\n\n**strengths**\n\n- The claims of the paper are held together well by objective experiments and a human listening study.\n- The listening examples demonstrate that the author\u2019s proposed method generates high-quality music that \u201criffs\u201d on the musical idea given in an audio \u201cstyle\u201d prompt.\n\n**weaknesses**\n\n- While the KNN-based objective metrics make intuitive sense, I wonder if there\u2019s a more interpretable way to observe the effects of which aspects of the \u201cstyle\u201d get preserved or not.  For example, I could imagine it is possible to check if the generated audio has too similar of a tempo/accents/rhythms/harmony/melodic contour as the style reference audio by leveraging pretrained MIR models/feature extractors to extract these music descriptors from both the reference and generated audio.\n- The paper claims to be the only existing audio-conditioned music generative model, but VampNet (ISMIR 2023) year also claims to be an audio-conditioned model, albeit with a different approach. It would have been great to include it as a baseline in evaluations. VampNet is mentioned in line 136, but is missing a citation.",
      "session": [
        "1"
      ],
      "slack_channel": "p1-13-audio-conditioning-for",
      "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features",
      "video": "https://drive.google.com/file/d/1C_XUyhaIMcDv04fFALbIhQemHpVAyYw9/view?usp=drive_link"
    },
    "forum": "41",
    "id": "41",
    "pic_id": "https://drive.google.com/file/d/1KIzQlQbxxz8C4TrXwWVAgSnKHl5QL_vU/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "It is well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.   In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning.  As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",
      "abstract": "It is well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners.   In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning.  As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",
      "author_changes": "",
      "authors": [
        "Kim, Hyon*",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Hyon Kim (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGDLYLU",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> music signal processing; Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1j-WrXG6w8826Xd_mmCnsdDVdCBcg_AjI/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Roqh2RqoMLQfxwRgH-Wtxocb58su2ODH/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-13-a-method-for",
      "title": "A Method for MIDI Velocity Estimation for Piano Performance by a U-Net with Attention and FiLM",
      "video": "https://drive.google.com/file/d/1T_Ldu-OZ5QK6O68vFquQHNZWz_x0lzAd/view?usp=drive_link"
    },
    "forum": "42",
    "id": "42",
    "pic_id": "https://drive.google.com/file/d/1aOPRZRHwkysFf7uyhlBGLG2HTJhJeEMN/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference, and ambivalence were preferred. In contrast, themes such as liberation, familiarity, and activity are not as favored. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",
      "abstract": "Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference, and ambivalence were preferred. In contrast, themes such as liberation, familiarity, and activity are not as favored. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",
      "author_changes": "",
      "authors": [
        "Chowdary, Pavani B*",
        " Singh, Bhavyajeet",
        " Agarwal, Rajat",
        " Alluri, Vinoo"
      ],
      "authors_and_affil": [
        "Pavani B Chowdary (International Institute of Information Technology, Hyderabad)*",
        " Bhavyajeet Singh (International Institute of Information Technology, Hyderabad )",
        " Rajat Agarwal (International Institute of Information Technology)",
        " Vinoo  Alluri (IIIT - Hyderabad)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MM4X7T",
      "day": "4",
      "keywords": [
        "Human-centered MIR",
        "Applications -> music and health, well-being and therapy; Human-centered MIR -> user behavior analysis and mining, user modeling; Human-centered MIR -> user-centered evaluation; MIR fundamentals and methodology -> lyrics and other textual data; Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1h1h8IbGas1Eq65yRk8IoohDgh6ZVh67e/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1LokDJCDPPXaforbEr7DhcxuOGm3Pcg_U/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-11-lyrically-speaking-exploring",
      "title": "Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk",
      "video": "https://drive.google.com/file/d/1WqcSFrn16ZcX2us_cF3Y4JbxaWEfnJ5c/view?usp=drive_link"
    },
    "forum": "43",
    "id": "43",
    "pic_id": "https://drive.google.com/file/d/1CTC9XD66wpDs9bwasfXRg6nleMZu2zs8/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",
      "abstract": "The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",
      "author_changes": "In response to the reviewers' comments, we made the following changes to our paper:\n\n1. We corrected minor grammatical and typographical errors.\n2. We ensured all references are accurate and properly formatted.\n3. We revised Fig. 1 to improve readability.\n4. We clarified and expanded explanations in several sections as requested by reviewers.\n\nWe appreciate the reviewers' valuable feedback.",
      "authors": [
        "Luna-Barahona, Noelia N",
        " Rosell\u00f3, Adri\u00e1n",
        " Alfaro-Contreras, Mar\u00eda",
        " Rizo, David",
        " Calvo-Zaragoza, Jorge*"
      ],
      "authors_and_affil": [
        "Noelia N Luna-Barahona (Universidad de Alicante)",
        " Adri\u00e1n Rosell\u00f3 (Universidad de Alicante)",
        " Mar\u00eda Alfaro-Contreras (University of Alicante)",
        " David Rizo (University of Alicante. Instituto Superior de Ense\u00f1anzas Art\u00edsrticas de la Comunidad Valenciana)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5T53C5",
      "day": "2",
      "keywords": [
        "Applications -> digital libraries and archives",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "meta_review": "All reviewers agree that the presented unsupervised domain adaptation framework to improve Optical Music Recognition performance would be of interest to the ISMIR community, would create discourse, possibly inspire work in other parallel areas, and in that sense the topic has high relevance to the conference. The readability of the paper is good, novelty and the scientific contributions are sufficient for publication.\n\nWe recommend acceptance of the paper provided the following issues are addressed in the final version.\n- Please see all reviews for detailed suggestions and changes to improve the clarity of the paper.\n- Code be made available as mentioned in the paper (but was not provided for this review).\n- Add comparison with previous works (Reviewer 1; R1) to the extent possible. Include a comparative discussion of the two references mentioned.\n- Consider the work in the papers listed by R3 on leveraging batch normalization statistics for domain adaptation and higher-order moment matching to tie in work in that area for a better grounding of presented approach. Make it clear that idea of domain adaptation is not being newly introduced in this work.\n",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1uAPEcIw9skchg5UQxmgCkLc1BsClOgFF/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1bzCCw1CM6f0GbA-yFkX4XjLxGClfb6P0/view?usp=drive_link",
      "review_1": "This paper aims to create more practical OMR system. Most ML research suffers from insufficient 'good' dataset, and this is especially true for OMR research. To address this, the authors present an unsupervised Domain Adaptation method using a synthetic dataset. They carefully designed the loss function, and the entire technical process is explained in detail in the paper. Compared to previous works that utilize DA, this work targets an end-to-end approach, and the results clarified the effectiveness of their method. \n\nWhile this paper uses different DA architectures and loss functions, I feel that it lacks a comparison with previous works. What are the differences and similarities between this work and \"Domain adaptation for staff-region retrieval of music score images\" by Castellanos, et al., and \"Real world music object recognition\" by Tuggener, et al.?\n\nThe latter also deals with the sheet images written in CWMN, while this paper focuses on Mensural collections. Are you planning to adapt this method to the most common CWMN collections? If successful, this approach could make a more practical contribution to the MIR community, in my opinion.",
      "review_2": "= SUMMARY =\n\nThe authors propose a method to improve domain adaptation for OMR systems by fine-tuning the neural network in an unsupervised way. The approach employs two loss functions in the unsupervised stage, the first one adapting the outputs of the layers to have the same distribution (specified by mean and variance) as during pre-training. The second loss is responsible for regularisation, by maximizing within-frame symbol entropy across the minibatch and minimizing the symbol entropy within each frame. The CRNN model is pre-trained in a supervised way on a large labelled corpus of monophonic works in Mensural notation. The performance is improved consistently for 5 different target corpora of Mensural notation (both typeset and hand-written), though not achieving a performance close to in-collection training.\nThe work is relevant, given that (pre-)training OMR networks on synthetic data is low-threshold, but given the discrepancies of the visual characteristics, adaptation is crucial and annotating data would be costly, so that an unsupervised approach is most welcome.\n\n\n= EVALUATION =\n\nSTRENGTHS:\n\n(+) The proposed method achieves a consistent improvement across 5 independent corpora, i.e., it is always useful, which is important as the outcome cannot be evaluated when deploying the method on real data.\n\n(+) Public datasets are used and the code will be made available to make the research fully reproducible (the code could not be reviewed, however).\n\n(+) The activation map gives some insights on potential issues when training on synthetic (off-domain) data.\n\nWEAKNESSES:\n\n(-) There is still a meaningful margin between the \"after adaptation\" and the \"in-collection\" SER results, i.e., the relative improvement from off-domain training plus adaptation to in-domain training is 80% - 90% and thus, much larger than the relative improvement achieved by the proposed unsupervised adaptation method. It remains unclear whether the obtained adapted system is good enough to provide usable outputs.\n\n\nOverall, the paper is well written and understandable. The topic is very relevant for the community and the proposed approach is technically sound.\nThe evaluation on different corpora shows the robustness of the approach.\nDespite the identified weakness, the methodology provides useful insights and has the potential to contribute to other research works.\n\n\n= MAJOR COMMENTS =\n\n-Section 3, ll.130-132: \"First, a general OMR model is pre-trained with synthetic data and made available. Prior to processing a certain real collection,\" -> The wording could be made more precise, given that the first part of the training is a supervised one, e.g.: \"First, a general OMR model is (pre-)trained in a supervised way on synthetic data. Prior to inferring the predictions on a real collection,\"\n\n-Section 3.2, Equation 1: The loss weights \\alpha and \\beta are introduced but it is not described anywhere how they are set or optimised.\n\n-Formula (3): How is \"SUM_S^'\" on the right side defined? Are the \\pi normalised w.r.t. the batch dimension?\n\n-Section 4.1: It would be interesting to report on the number of musical works and notes present in the training corpus. Moreover, it should be clarified whether one or several images (using different graphical filters) were generated for each musical work?\n\n-Figure 3: The meaning of \"L\" and \"S\" could be mentioned.\n\n-Table 1: The meaning of \"Complete\" is not clear. Is it the number of \"form\"/\"height\" combinations found in the corresponding dataset?\n\n-Section 5.2: The \"batch size\" for (ii), i.e., the unsupervised adaptation phase, should also be given as it is quite relevant for the defined regularisation loss.\n\n-Table 2: \"For completeness, we also include the in-collection performance, where both the training and testing data originate from the same corpus.\" -> This wording is not fully clear. Does it mean, these are the results when performing a supervised training on the training partition of the respective corpus? Is also the synthetic PRIMENS data included or not?\n\n-Section 6: Might the relatively short stems of the notes be the reason for the misplaced activations and resulting low before-adaptation performance on the Guatemala corpus?\n\n= MINOR COMMENTS =\n\n-General: The \"Il Lauro Secco\" dataset is also known as \"SEILS\" (Symbolically Encoded Il Lauro Secco) dataset (which was iteratively extended), so it could be considered to mention this\n-Abstract, l.3: \"reading music from document images\" -> maybe: \"reading music scores from document images\"\n-Section 2, l.88: \"stave\" -> \"staff\" (for consistency)\n-Section 3.2.1., ll.195-196: \"mean and variance vectors, represented as \u00b5_S and \u03c3_S\" -> \"\u03c3_S^2\"\n-Section 5.2, l.394: \"learning\" -> \"learning rate\"\n\nReferences:\n-The capitalisation of paper titles should be consistent.\n-The naming of the proceedings/date/location should be consistent for the same venue, e.g.: [5] vs [13]\n-Some information is missing in [24].",
      "review_3": "The article shows how to use a combination of labeled synthetic music notation data for training and unlabeled real images for domain adaptation in order to build an OMR pipeline that can deal with sheet music collections without having to label any. The relationship between synthetic and real data is one of the vital issues for OMR, because labeled training data is still very expensive. While the domain adaptation still is far from solving this problem (based on the results in Table 2), it certainly does help significantly.  I want to especially commend that the article performs these experiments not on one collection, but across five diffferent mensural corpora. \n\nThe domain adaptation loss consists of two terms that each implement a clever trick. The adaptation loss term takes advantage of the batch normalization mechanism that remembers a summary of the incoming representations via their mean and standard deviations that are used to perform the batch normalization during the supervised training phase, and during fine-tuning (if I understand this correctly), it adapts the weights of the upstream layers so that the batch norm statistics coming from the in-domain data match these statistics from the supervised training phase as closely as possible (with KL divergence), so that the distributions going to the downstream layers look as close to what the network is used to from the training phase. The regularization term then prevents pathological collapses of the adaptation loss by encouraging predictions that look like plausible OMR outputs.\n\nWhile these are clearly workable ideas, the paper suffers from a bit of in-domain OMR tunnel vision. By not considering related methods in more broad computer vision terms, the paper implicitly overclaims that the domain adaptation term is an original contribution. This is important and in my view must be fixed for camera-ready, if accepted. The widely cited methods that leverage batch norm statistics for unuspervised domain adaptation are not exactly the same as the proposed domain adaptation term, so this merits also some light discussion of at least what advantages and disadvantages the choices made in this paper have compared to existing methods (new experiments with these existing methods are perhaps a bit unrealistic for camera-ready, and not necessarily valuable anyway).\n\nIn order for the paper to move from showing that the idea has potential for OMR into actionable intelligence, I would also recommend a best-of-both-worlds experiment that combines domain adaptation with some amounts of in-domain data to show whether the suggested ceiling is in fact really as fragile as the glass metaphor suggests: does adding already a little bit of in-domain data help significantly?\n\nThe error analysis could also benefit from showing if there are errors that domain adaptation introduces. The delta term in Table 2 has in fact two parts \u2014 errors fixed with domain adaptation, and errors introduced by it (with the second number likely being much smaller). However, especially if some of the test domains (=different datasets) exhibits a large number of errors introduced by the method, this could offer valuable qualitative insights into the limitations of the presented method \u2014 and suggest directions for improving it towards applicability.\n\nI recommend the paper for acceptance, especially because it is a good attempt at tackling this important problem of synthetic-to-real domain adaptation in OMR with purely synthetic training data, but with the important caveat that the domain adaptation using batch norm matching should not be presented as a new thing \u2014 it has already been explored repeatedly throughout the past years (see the comments on related work above), hence a weak and not strong accept. This is a serious omission, but it can be easily fixed in camera-ready and in my view does not detract much from the long-term value of the paper. It is a great first step to applying these techniques for OMR. I am looking forward to next steps that moves the results of this method closer to the results with in-domain training data, especially with an eye of adapting other techniques used for this in computer vision.",
      "session": [
        "3"
      ],
      "slack_channel": "p3-13-unsupervised-synthetic-to",
      "title": "Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition",
      "video": "https://drive.google.com/file/d/17uyevRjXv8eJp77erGQKQP9Z9AzlI89B/view?usp=sharing"
    },
    "forum": "45",
    "id": "45",
    "pic_id": "https://drive.google.com/file/d/1pS1if-khfQb_HZLoWqkpYsGqsWCTeT5J/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods are capable of adequately performing this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. For that, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create dynamic class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently outperforms them. This highlights the interest in CL methods tailored specifically for music classification.",
      "abstract": "Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods are capable of adequately performing this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. For that, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create dynamic class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently outperforms them. This highlights the interest in CL methods tailored specifically for music classification.",
      "author_changes": "",
      "authors": [
        "Gonz\u00e1lez-Barrachina, Pedro",
        " Alfaro-Contreras, Mar\u00eda",
        " Calvo-Zaragoza, Jorge*"
      ],
      "authors_and_affil": [
        "Pedro Gonz\u00e1lez-Barrachina (University of Alicante)",
        " Mar\u00eda Alfaro-Contreras (University of Alicante)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1D4V50",
      "day": "2",
      "keywords": [
        "MIR tasks -> automatic classification",
        "MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1C62NxDl4MWhctcXKlDNq2cBbsRMCLFYQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1UmdgpGdCSDy7rDeRGwyHYH_Qj4_ZiD_S/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-13-continual-learning-for",
      "title": "Continual Learning for Music Classification",
      "video": "https://drive.google.com/file/d/1j8PN64wnrXGHAQ8QN5rDLoRpqGPhoDNn/view?usp=sharing"
    },
    "forum": "46",
    "id": "46",
    "pic_id": "https://drive.google.com/file/d/1BUDLfutqRkVEJFy1z2culI2uv_G1lrYL/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",
      "abstract": "Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",
      "author_changes": "",
      "authors": [
        "Deng, Jiajun*",
        " Ju, Yaolong",
        " Yang, Jing",
        " Lui, Simon",
        " Liu, Xunying"
      ],
      "authors_and_affil": [
        "Jiajun Deng (The Chinese University of HongKong)*",
        " Yaolong Ju (Huawei)",
        " Jing Yang (Huawei 2012 Labs)",
        " Simon Lui (Huawei)",
        " Xunying Liu (The Chinese University of Hong Kong)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MM9281",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1CG6ZsCT7eqBT-FHt5XzVXU6JleGoe-FQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1TJ7tKyht_8XhKfcKFU6X-yl5rINUj9Eo/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-18-efficient-adapter-tuning",
      "title": "EFFICIENT ADAPTER TUNING FOR JOINT SINGING VOICE BEAT AND DOWNBEAT TRACKING WITH SELF-SUPERVISED LEARNING FEATURES",
      "video": "https://drive.google.com/file/d/1_QEmBe1S2hLH83IqJlc_z89AlwAbSMjq/view?usp=drive_link"
    },
    "forum": "48",
    "id": "48",
    "pic_id": "https://drive.google.com/file/d/1qLg3J2L7_BQfBfCtWQe9vWm8s7pHWwcB/view?usp=drive_link",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Recent advancements in audio processing provide a new opportunity to study musical trends using quantitative methods. While past work has investigated trends in music over time, there has been no large-scale study on the evolution of vocal lines. In this work, we conduct an exploratory study of 145,912 vocal tracks of popular songs spanning 55 years, from 1955 to 2010. We use source separation to extract the vocal stem and fundamental frequency (f0) estimation to analyze pitch tracks. Additionally, we extract pitch characteristics including  mean pitch, total variation, and pitch class entropy of each song. We conduct statistical analysis of vocal pitch across years and genres, and report significant trends in our metrics over time, as well as significant differences in trends between genres. Our study demonstrates the utility of this method for studying vocals, contributes to the understanding of vocal trends, and showcases the potential of quantitative approaches in musicology.",
      "abstract": "Recent advancements in audio processing provide a new opportunity to study musical trends using quantitative methods. While past work has investigated trends in music over time, there has been no large-scale study on the evolution of vocal lines. In this work, we conduct an exploratory study of 145,912 vocal tracks of popular songs spanning 55 years, from 1955 to 2010. We use source separation to extract the vocal stem and fundamental frequency (f0) estimation to analyze pitch tracks. Additionally, we extract pitch characteristics including  mean pitch, total variation, and pitch class entropy of each song. We conduct statistical analysis of vocal pitch across years and genres, and report significant trends in our metrics over time, as well as significant differences in trends between genres. Our study demonstrates the utility of this method for studying vocals, contributes to the understanding of vocal trends, and showcases the potential of quantitative approaches in musicology.",
      "author_changes": "",
      "authors": [
        "Georgieva, Elena*",
        " Ripoll\u00e9s, Pablo",
        " McFee, Brian"
      ],
      "authors_and_affil": [
        "Elena Georgieva (NYU)*",
        " Pablo Ripoll\u00e9s (New York University)",
        " Brian McFee (New York University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5T97PX",
      "day": "1",
      "keywords": [
        "Computational musicology",
        "Computational musicology -> digital musicology; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1xzg8GGGmGb1t9bAm9cqIDJcrH_tT6vOl/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1L3qtZakVzi-56K2d_mwi1J6wJ2loBEaF/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-03-the-changing-sound",
      "title": "The Changing Sound of Music: An Exploratory Corpus Study of Vocal Trends Over Time",
      "video": "https://drive.google.com/file/d/1SGdVso59OcRTJKrPKPv_X_segXmtNzcn/view?usp=drive_link"
    },
    "forum": "57",
    "id": "57",
    "pic_id": "https://drive.google.com/file/d/1BpZHxeCiXiUja41F8lU9_CH3nz4_yUze/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",
      "abstract": "We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",
      "author_changes": "-De-anonymized title and text.\n\n-Added/discussed the 3 references suggested by Reviewer 2 and Meta-Reviewer 1.\n\n-Two reviewers felt that Figure 5 was difficult to understand. Figure 5 has been replaced with a new figure (still Figure 5) that gives more information, and the text referring to Figure 5 has been rewritten.\n\n-\"How does the model take care of future context?...\", \"How are the control tokens and tokens to be generated ordered/arranged?\"\n\nIt places future context on the encoder side. (Think of an input as being a page of sheet music with some track-measures masked. The model can \"see\" all unmasked notes on the page before writing any notes.) The paper has been updated in Fig 2 and Sec 4 to make this more clear. Sec 4 was also updated to describe where the control tokens are placed.\n\n-\"In Table 1, how much of the improvement of CA2 is attributable to general improvements of the CA model, vs. the greater control signal...?\"\n\nThere were some general improvements to the CA codebase (primarily around training example generation), as well as improvements to the training dataset (some additional CC0 files and some reweighting of the training data) that we did not feel were worth discussing in the paper. To ensure we made a fair comparison, the CA model in the paper is actually a retrained CA model that incorporates these improvements. Hence, the numbers in Table 1 demonstrate exactly the improvement derived from the greater control signal.\n\nThe original CA model scores are as follows:\n\nF1 50.63; 29.98; 53.35\nPrecision 52.21; 33.29; 55.16\nRecall 49.67; 29.38; 52.76\nPCHE difference 33.92; 52.77; 33.29\nGroove sim 97.85; 96.17; 97.91\n\nRoughly, general improvements increased performance by 1-2 points, and the remaining 20-40 points of improvement in the paper come from the increased control signal from the ground truth.\n\n-Rewrote Sec 5.3 for clarity and discussion of subjective results.\n\n-Removed a few sentences from Sections 1 and 2 to make room for the above changes.",
      "authors": [
        "Malandro, Martin E*"
      ],
      "authors_and_affil": [
        "Martin E Malandro (Sam Houston State University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGDV12Q",
      "day": "2",
      "keywords": [
        "Creativity -> human-ai co-creativity; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The reviewers broadly appreciate the contribution of a generative model integrated into a DAW. While I think the empirical evaluation is somewhat weak, this is counterbalanced by the relevance of the artifact. The system will be released and open source, making this a valuable baseline for future work on DAW-integrated models.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1tjF6ljH2xbNTv6enbQfeU4UecpR-C3vH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1yI-Ig00wI16BRqVhS7cqLuLwPHqXoiNy/view?usp=drive_link",
      "review_1": "Strengths\nI highly recognize this work for its wide range of controls that\n- focus on different aspects of music, including pitch, rhythm, density and novelty, which composers would likely find important and useful; and,\n- allow very high flexibility, i.e., users can choose whichever control to apply, on arbitrary track or measure.\n\nI also thank the authors a lot for releasing their model/implementation as part of a DAW, which increases the system's potential real-world impact.\n\nWeaknesses\n(W1) Somewhat insufficient ML technical descriptions --\nThe authors put most of the technical details in the appendix. To enhance readers' understanding of the method, I would recommend including at least the following content in the main text:\n- How does the model take care of future context? Is it by reordering, or placing future context on the encoder side?\n- How are the control tokens and tokens to be generated ordered/arranged?\n- What are the differences/advancements compared to REAPER v1? (The improvement seems sizable from the numbers in Table 1.)\n\n(W2) Misleading plot about control effectiveness (Figure 5) --\nI find this plot a bit difficult to understand, particularly because the control signals/levels were not shown. Perhaps a better presentation is to display generated examples conditioned on different note density levels.",
      "review_2": "**strengths**:\n\n- strong objective evaluation of the improvements and controls added to the infilling model\n- authors think about deep learning models (and the way we should control them) from a co-creative standpoint, developing their models and conditioning techniques guided by the interaction needs of a composer working with MIDI in a DAW,  co-creating with a generative model.\n-  the authors contribute a fully open-source ecosystem with source code, pretrained models, and REAPER scripts for incorporating their system into a DAW. \n\n**weaknesses:** \n\n- would have been good to know about the musical background of the volunteers in the subjective evaluation.",
      "review_3": "This paper describes additional features to REAPER Infiller incorporating user controls (pitch, rhythm, horizontal and vertical note density). The methods and the model are well evaluated along with subjective evaluations.\n\nWhile the paper is well written and most details are properly presented, it builds on previous work (RI). A section to review RI itself would have been helpful for an uninitiated reader. However, lines 280-289 lends some insight into the model architecture, so I am inclined to believe that the text in this paper is sufficient to understand the method.\n\nOverall I feel this work is an important addition to RI and thus should be published. ",
      "session": [
        "3"
      ],
      "slack_channel": "p3-09-composer-s-assistant",
      "title": "Composer's Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control",
      "video": "https://drive.google.com/file/d/1PDuCcmOrjiQU0Gxzm0JIy3C6jCases84/view?usp=drive_link"
    },
    "forum": "60",
    "id": "60",
    "pic_id": "https://drive.google.com/file/d/1GIhTiaj-zJipZdmgeIm2H8uoBPfU77vZ/view?usp=drive_link",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.",
      "abstract": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music \\textit{indirectly} through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). \\textit{We aim to further equip the models with direct and \\textbf{content-based} controls on innate music languages} such as pitch, chords and drum track. To this end, we contribute \\textit{Coco-Mulla}, a \\textbf{co}ntent-based \\textbf{co}ntrol method for \\textbf{mu}sic \\textbf{l}arge \\textbf{la}nguage modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with \\textbf{low-resource} semi-supervised learning. We fine-tune the model with less than 4$\\%$ of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online\\footnote{\\url{https://github.com/Kikyo-16/coco-mulla-repo}.}\\footnote{\\url{https://kikyo-16.github.io/coco-mulla/}.}.",
      "author_changes": "Rephrase the methodology sec to make it clearer; Adding references in the related work sec; Correct typos in the exp sec.",
      "authors": [
        "Lin, Liwei*",
        " Xia, Gus",
        " Jiang, Junyan",
        " Zhang, Yixiao"
      ],
      "authors_and_affil": [
        "Liwei Lin (New York University Shanghai)*",
        " Gus Xia (New York University Shanghai)",
        " Junyan Jiang (New York University Shanghai)",
        " Yixiao Zhang (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCN1N8N",
      "day": "3",
      "keywords": [
        "MIR tasks -> music generation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "Reviewers agree that this paper is well written and clearly presented. Further, the code to be published along with the paper is a huge plus.\n\nIt is a logical approach to training controllable music generation with limited resources, using llama adaptors, and great to see a model that can take the range of modes of control that were presented. However, many reviewers point out that there is room for improvement in the evaluation. There are comments regarding the clarity around the description of the evaluation process, the presence of good baselines, and regarding limited insight into how useful each of the modes of control are to the model. Please take careful note of the each of the reviewer's comments and use them to address outstanding reviewers concerns.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1HutKNTlmekAao1OWMxx7J-4HFwD3EA0K/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1g1WMrdURcqIJpRyjJrpz4kpifCg57BZl/view?usp=drive_link",
      "review_1": "This paper presents an approach for learning content-based controls for the large open-source text to music transformer model, musicGen. The controls come from an example piece of music, from which automatic chord recognition, music transcription, and audio codec features are computed. Results are presented in terms of chord recognition and beat detection accuracy, however, I found it difficult to put the results in context, as only upper-bound ground truth results were provided without any baseline or lower-bound comparisons.\n\nStrengths:\n- The proposed conditioning mechanism is clearly explained \n- Makes strong use of off-the-shelf transcription and source separation tools\n\nWeakness:\n- Some of the design choices in the proposed conditioning scheme seem arbitrary\n- No baseline results (even unconditioned MusicGen) are included for chord and beat recognition accuracy\n- I found it difficult to take away reusable insights about what worked and didn\u2019t work in the proposed approach. For example, training a model without the acoustic representation input would have been insightful, as then someone could condition the model without an audio example. The large drop in performance between full conditioning and chord only conditioning make it seem like the model is cheating quite a bit with the audio and melody inputs. Since MusicGen was already conditioned using audio and melody inputs, this makes it seem like the proposed extension to chord conditioning doesn\u2019t actually work in practice.\n\nSpecific comments:\n- Table 1: It would be nice to have an example here where root and bass aren\u2019t always the same, unless they\u2019re required to always be the same, in which case, why do you need both?\n\n- Section 4.2: It was unclear to me why the condition prefix tokens are the same length as the generated encodec tokens? Doesn\u2019t the number of generated encodec tokens change over time?\n\n- Equations (8)-(13) the notation for the W matrices appears to be overloaded, e.g., the same W symbols are used for (8) and (11), but they must certainly mean different things?\n\n- End of Section 4.2, it would be nice to list the symbols or refer to the equation numbers  to specify exactly what the trainable parameters are. For example, I\u2019m unclear as to what the \u201cjoint embedding\u201d trainable parameters are.\n\n- End of Section 4.2, assigning a random text description during training. Why do this, as opposed to no text conditioning at all? It seems like this could very negatively impact fine-tuning, and makes it difficult to trust this work without exploring this aspect more.\n\n- Table 2: why not include chord and beat accuracy results for unconditioned musicgen as a lower bound?\n\n- Table 3: Why are the CLAP score results so much higher than in Table 2 and never discussed? Is this a typo?\n\n- Section 5.2: How are the 20s samples chosen? Randomly? Can they overlap? Without this information, I\u2019m unclear what an epoch means?\n\n- Section 5.3: I\u2019m confused about the different FAD scores. Why is * higher? Wouldn\u2019t we expect * to be lower (i.e., better) since it includes the audio used for conditioning?\n\n- Section 5.4: The authors state that the approach \u201cmaintains text-control ability.\u201d What is the evidence for this? It appears CLAP score drops quite a bit.\n\n- Section 5.4.1: The text in this section doesn\u2019t seem to match what I see in Table 3.  As the number of trainable layers increases, Table 3 appears to show an increase in CLAP score, not a reduction as stated in the text. ",
      "review_2": "This paper utilizes frame-wise chord, midi, and content representations as conditions for content-based controllable generation. It also proposes a Condition Adaptor to efficiently train large models with small datasets and employs an attention mechanism with condition prefixes to allow condition prefixes to effectively influence token prediction. The condition prefixes are learned through self-attention layers. As a result, the paper enables efficient content-based control using only 300 music tracks. Another new finding is that such content-based control further improves the fidelity of music (Table 2 FAD Score).\nDespite these many new attempts and discoveries, the paper has some drawbacks:\n\n1. Lack of rigorous evaluation. The paper does not describe the inference method of the baseline MusicGen model. How did you infer MusicGen using the RWC-POP-100 data? Did you input chords, beat timing (i.e, [0.27, 0.88, 1.49, 2.06, 2.66, 3.26, 3.88, 4.49, 5.10]), and melody note of midi as textual representations separately? If you used text conditions such as the Main Category  of RWC-POP-100 (single word: pop), it would be meaning-less comparison. Furthermore, if this paper aims to strictly compare text vs. content-based generation, you should compare it to a model trained on natural language expressions of beat sequence text, chord sequences text, or melody note text.\n\n2. Joint embedding. Can the concatenation and linear projection of different data be called joint embedding? According to the latest representation learning reference [1], defines joint embedding as \"which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs.\" (in Chapter 2) The joint embedding mentioned in this paper seems closer to fusion or mixed representation.\n\n3. Regarding the layer-wise functionality of the transformer layer claimed in line 359, it would be better to rigorously compare fine-tuning only the lower layers and fine-tuning only the higher layers separately. In this case, the influence of the fine-tuning parameters is added, lowering the validity of the message.\n\n4. It is unclear whether the time-varying sequential conditions work because of the proposed condition prefix-based adaptor or simply due to the preserved sequence representation of condition. It is regrettable that a comparison with a sequence representation conditioned LoRA (the same conditioning method with MusicGen) was not reported.\n\n[1] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, MahmoudAssran et al. https://arxiv.org/pdf/2301.08243\n\n----------------------------------------\nIn light of this, I recommend a weak aceept, considering that the above limitations have little impact on content-based conditioning and can be addressed in camera-ready or follow-up research.",
      "review_3": "The strengths of this paper is clear: (1) the idea of introducing musical context control for LLM based music generation systems (2) the use of LLaMA-Adapter type PEFT for this application (3) the presentation of methodology, implementation details are clear and solid.\n\nThe weaknesses for this paper are: (1) To make it more insightful, a research on the fine tuning dataset scale versus evaluation metrics can be helpful.\n(2) Since nowadays music generation systems are usually not publicly available or easily reproducible due to data and compute, it's not possible to compare with other context based controllable systems such as diffusion based Music ControlNet or Mustango (which should be cited in the paper), but they should be cited, a comparison with Mustango under the same criteria is also a plus;  \n(3) I'm interested in the masking scheme introduced in section 4.1.3, I'm not clear why only MIDI and acoustic embedding are masked and how would the ratio r will influence the final performance, there is no intuitive explanations or references for this justification;\n(4) I wonder if the proposed system is able to \"outpaint\" on previous given controls so that improvise a bit and can remain consistency;\n(5) In terms of writing, section 3 and be merged into one subsection of section 4.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-13-content-based-controls",
      "title": "Content-based Controls for Music Large-scale Language Modeling",
      "video": "https://drive.google.com/file/d/1s9CKLuAQRybFBY1b20UyGI6-JLmKv9Lw/view?usp=drive_link"
    },
    "forum": "61",
    "id": "61",
    "pic_id": "https://drive.google.com/file/d/1G0Z008KDxw7aYe1eUq7Z1N53egh15jZT/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",
      "abstract": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",
      "author_changes": "We would like to thank all the reviewers for their insightful reviews.\n\nIn this Camera-Ready version we did our best to address most of the highlighted reviewer notes:\n- Added a clear enumeration of our main contributions at the end of the introduction part.\n- Added details regarding the amount of raters that took part in our human evaluation.\n-Added intuition to the information bottleneck design choices (why we use the continuous latent of the first codebook and discard all other).\n- Made clarifications regarding concepts and notations such as 'Outpainting' or 'MLD(clf)'.\n- Fixed invalid citation and grammar.\n",
      "authors": [
        "Tal, Or*",
        " Ziv, Alon",
        " Kreuk, Felix",
        " Gat, Itai",
        " Adi, Yossi"
      ],
      "authors_and_affil": [
        "Or Tal (The Hebrew University of Jerusalem)*",
        " Alon Ziv (The Hebrew University of Jerusalem)",
        " Felix Kreuk (Bar-Ilan University)",
        " Itai Gat (Meta)",
        " Yossi Adi (The Hebrew University of Jerusalem)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1DBBS6",
      "day": "1",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis; MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "We have thoroughly reviewed this paper from various aspects, and we all believe that the paper is ready for publication with minor revisions. For more details, please see the comments of each reviewer. It is also highly recommended that you add more details about the user study",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/view?usp=drive_link",
      "review_1": "This paper introduces a novel approach to a controllable text-to-music generation model. It leverages pretrained models for chord estimation, f0 classification, and source separation. The preprocessed signals from these models are fed into a conditional flow matching model, which consists of a transformer with residual connections, to enable chord conditioning, melody conditioning, and audio conditioning, along with textual inputs. Classifier-free guidance is employed to ensure control adherence during the inference stage. The model was evaluated both quantitatively and qualitatively. Experimental results demonstrate that the model maintains generation quality while offering a high degree of controllability.\n\nThe paper is well-organized and clearly written. Related works are cited, the experimental design is clearly described, and the evaluation metrics are reasonable. Well-designed experiments and thorough evaluation highlight its strengths. Although the model was trained with proprietary data, the paper provides detailed descriptions of their experiments, and the inclusion of code and pretrained models facilitates reproducible research. Notably, this work bridges the gap between music analysis research from the MIR community and data-driven generative models.\n\nI have three minor questions/comments:\n* Line 417: Why is there a significant gap in FAD? Is it due to the model characteristics or the training scheme? Addressing this will provide more insights into evaluation metrics for generative models.\n* Emboldening the best performance in the table would enhance readability.\n* The reference papers are missing conference names. Adding these would improve the completeness of the citations.",
      "review_2": "Promising article on text-to-music generative AI, extending model customizability with audio and symbolic input. The algorithm design and details are the strong points here, and while the blind process does not allow reviewers to access the code, the provided audio examples show that it appears to be quite successful in scope, even if improvements could be made in terms of audio quality. This is the type of research where the novelty dictates that human subjective evaluation is paramount, but the authors decided to make it an afterthought and provide little to no detail on this part of the evaluation study. Furthermore, this part of the results is the one with more generous and misleading interpretations, as it is apparent that on all accounts except the one that could not be compared (including drum input), JASCO fared worse (still it is stated \"however, when considering melody conditioning, reaches significantly better scores\"). The fact that it did not fare better is not the problem, the problem is the description of the process. The article's structure is unconventional and I fail to understand why related work appears between analysis and discussion, and why analysis is more rushed than it should be. It is also not clear why the authors chose to separate results from analysis, as it is not as clear-cut as it usually is. There are some minor typos in the text, particularly singular-plural problems (eg lines 322, 445) and excess spaces (eg lines 153, 249), which should still be revised.",
      "review_3": "The JASCO paper introduces an innovative model that advances the text-to-music generation domain, employing Flow Matching and hybrid conditioning to enable detailed control over music generation. The model's integration of symbolic and audio inputs through advanced conditioning mechanisms presents a pioneering approach that enhances the quality and versatility of generated music. The samples showcased on the demo page demonstrate impressively good performance, adhering well to the specified controls.\n\nWhile the paper is generally well-written and informative, there are areas where further details could enhance understanding and application:\n\nIn Section 3.1, the paper would benefit from a more detailed explanation of why specific pre-trained models such as the multi-F0 classifier, the source separation model, and the melody extraction models were selected. For example, it could provide some discussions on their performance metrics, computational efficiency, or effectiveness in diverse settings. Additionally, elaborating on any adaptations made to integrate these pre-trained models into the JASCO framework would be useful. Similarly in Section 3.3, it would be helpful if the paper clarifies why the ODE solver was chosen for the inference process.\n\nIn Section 3.1 Audio, the process for converting discrete tokens back into continuous vectors needs clarification. Is this achieved through interpolation or direct mapping? Furthermore, I am curious about why the authors choose the continuous latent representation converted from the first codebook, rather than using the continuous latent tensor directly from the EnCodec model.\n\nIn Section 3.2 on Inpainting/Outpainting, the term 'Outpainting' is used without a definition, which might lead to confusion. \n\nIn Section 4.1, the paper could include more details about the human study, such as the number of raters involved and the number of samples each assessed.\n\nIn Section 5, it would be helpful if the paper addressed the limitations or common errors associated with those pre-trained models within the JASCO system. For example, how might the errors in chord extraction or melody extraction affect the final generations?\n\nOne minor correction in Table 2, I am not sure what \"Mld(clf)\" refers to?\n\nYou might also want to fix title \"Mo\\\u02c6 usai\" in reference [4] \"F. Schneider, O. Kamal, Z. Jin, and B. Sch\u00f6lkopf, \u201cMo\\\u02c6 usai: Text-to-music generation with long-context latent diffusion,\u201d arXiv preprint arXiv:2301.11757, 2023.\"\n",
      "session": [
        "2"
      ],
      "slack_channel": "p2-07-joint-audio-and",
      "title": "Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation",
      "video": "https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG"
    },
    "forum": "65",
    "id": "65",
    "pic_id": "https://drive.google.com/file/d/17bUEpA6WnKZn6bcPrHmY82DqvQ8_VXrd/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Recent advances in Deep Learning have propelled the development of fields such as Optical Music Recognition (OMR), which is responsible for extracting the content from music score images. Despite progress in the field, existing literature scarcely addresses core issues like performance in real-world scenarios, user experience, maintainability of multiple pipelines, reusability of architectures and data, among others. These factors result in high costs for both users and developers of such systems. Furthermore, research has often been conducted under certain constraints, such as using a single musical texture or type of notation, which may not align with the end-user requirements of OMR systems. For the first time, our study involves a comprehensive and extensive experimental setup to explore new ideas towards the development of a universal OMR system---capable of transcribing all textures and notation types. Our investigation provides valuable insights into several aspects, such as the ability of a model to leverage knowledge from different domains despite significant differences in music notation types.",
      "abstract": "Recent advances in Deep Learning have propelled the development of fields such as Optical Music Recognition (OMR), which is responsible for extracting the content from music score images. Despite progress in the field, existing literature scarcely addresses core issues like performance in real-world scenarios, user experience, maintainability of multiple pipelines, reusability of architectures and data, among others. These factors result in high costs for both users and developers of such systems. Furthermore, research has often been conducted under certain constraints, such as using a single musical texture or type of notation, which may not align with the end-user requirements of OMR systems. For the first time, our study involves a comprehensive and extensive experimental setup to explore new ideas towards the development of a universal OMR system---capable of transcribing all textures and notation types. Our investigation provides valuable insights into several aspects, such as the ability of a model to leverage knowledge from different domains despite significant differences in music notation types.",
      "author_changes": "",
      "authors": [
        "Martinez-Sevilla, Juan Carlos*",
        " Rizo, David",
        " Calvo-Zaragoza, Jorge"
      ],
      "authors_and_affil": [
        "Juan Carlos Martinez-Sevilla (University of Alicante)*",
        " David Rizo (University of Alicante. Instituto Superior de Ense\u00f1anzas Art\u00edsrticas de la Comunidad Valenciana)",
        " Jorge Calvo-Zaragoza (University of Alicante)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCN4FQE",
      "day": "3",
      "keywords": [
        "Applications -> digital libraries and archives; Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music transcription and annotation",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1msfdNh_Jg391bVoLqiGLcqreH9QfuOoL/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1N_O2bUMsp4wYxxdc16ZrkYTWUqsY4F4K/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-13-towards-universal-optical",
      "title": "Towards Universal Optical Music Recognition: A Case Study on Notation Types",
      "video": "https://drive.google.com/file/d/1-ry2zGPZyw5DdrpHfa-gpeXQEQAfWVh-/view?usp=drive_link"
    },
    "forum": "66",
    "id": "66",
    "pic_id": "https://drive.google.com/file/d/1z2XhRwBCux5aiPatuuh-1UxL5-EAyGT8/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The importance of automatic drum transcription lies in the potential to extract useful information from a musical track; however, the low reliability of the models for this task represents a limiting factor. Indeed, even though in the recent literature the quality of the generated transcription has improved thanks to the curation of large training datasets via crowdsourcing, there is still a large margin of improvement for this task to be considered solved. Aiming to steer the development of future models, we identify the most common errors from training and testing on the aforementioned crowdsourced datasets. We perform this study in three steps: First, we detail the quality of the transcription for each class of interest; second, we employ a new metric and a pseudo confusion matrix to quantify different mistakes in the estimations; last, we compute the agreement between different annotators of the same track to estimate the accuracy of the ground-truth. Our findings are twofold: On the one hand, we observe that the previously reported issue that less represented instruments (e.g., toms) are less reliably transcribed is mostly solved now. On the other hand, cymbal instruments have unprecedented relative low performance. We provide intuitive explanations as to why cymbal instruments are difficult to transcribe and we identify that they represent the main source of disagreement among annotators.",
      "abstract": "The importance of automatic drum transcription lies in the potential to extract useful information from a musical track; however, the low reliability of the models for this task represents a limiting factor. Indeed, even though in the recent literature the quality of the generated transcription has improved thanks to the curation of large training datasets via crowdsourcing, there is still a large margin of improvement for this task to be considered solved. Aiming to steer the development of future models, we identify the most common errors from training and testing on the aforementioned crowdsourced datasets. We perform this study in three steps: First, we detail the quality of the transcription for each class of interest; second, we employ a new metric and a pseudo confusion matrix to quantify different mistakes in the estimations; last, we compute the agreement between different annotators of the same track to estimate the accuracy of the ground-truth. Our findings are twofold: On the one hand, we observe that the previously reported issue that less represented instruments (e.g., toms) are less reliably transcribed is mostly solved now. On the other hand, cymbal instruments have unprecedented relative low performance. We provide intuitive explanations as to why cymbal instruments are difficult to transcribe and we identify that they represent the main source of disagreement among annotators.",
      "author_changes": "",
      "authors": [
        "Zehren, Mickael*"
      ],
      "authors_and_affil": [
        "Mickael Zehren (Ume\u00e5 University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1DE84S",
      "day": "4",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1gEOvHDCXQgJj7FRjub0iJSYmgjmzMLSo/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Npf1inFFhujGu5KnDzRcUL7plPCoP3AT/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-13-in-depth-performance",
      "title": "In-depth performance analysis of the ADTOF-based algorithm for automatic drum transcription",
      "video": "https://drive.google.com/file/d/1MZQ9UD99OaWpbzK6z2m1h28hCphyEh3U/view?usp=sharing"
    },
    "forum": "68",
    "id": "68",
    "pic_id": "https://drive.google.com/file/d/1mRt3ZrQqeDvZexbXDGXITLTB_vTM9ok0/view?usp=drive_link",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for \"variation success\", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",
      "abstract": "Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for \"variation success\", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",
      "author_changes": "",
      "authors": [
        "Gao, Chenyu*",
        " Reuben, Federico",
        " Collins, Tom"
      ],
      "authors_and_affil": [
        "Chenyu Gao (University of York)*",
        " Federico Reuben (University of York)",
        " Tom Collins (University of York",
        " MAIA, Inc.)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCN7K5L",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1N4ey26D4sZQphIzi74uNjr9fQNtuBCSx/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1vPlg5tf9-nwKr5mi20Xp1qt44iIR6ZDO/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-14-variation-transformer-new",
      "title": "Variation Transformer: New datasets, models, and comparative evaluation for symbolic music variation generation",
      "video": "https://drive.google.com/file/d/14-NxP7Q7c5tEO4UaKWGkEhVLwtTjgT0E/view?usp=drive_link"
    },
    "forum": "72",
    "id": "72",
    "pic_id": "https://drive.google.com/file/d/1ZtDDsSuSB4YVng0Vj5L8X30Pa4TYFeIA/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.  Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",
      "abstract": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.  Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",
      "author_changes": "",
      "authors": [
        "Ramoneda, Pedro*",
        " Rocamora, Mart\u00edn",
        " Akama, Taketo"
      ],
      "authors_and_affil": [
        "Pedro Ramoneda (Universitat Pompeu Fabra)*",
        " Mart\u00edn Rocamora (Universidad de la Rep\u00fablica)",
        " Taketo Akama (Sony CSL)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGE89T6",
      "day": "1",
      "keywords": [
        "Creativity; Creativity -> creativity and learning; Generative Tasks; Generative Tasks -> interactions; MIR tasks -> music generation",
        "Creativity -> human-ai co-creativity"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ia2OxA9wgadP1SiWWYN7clsiaz9kJbmA/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1VGJLTy1natiGB5LEP1zOBPUmO6oVdVFm/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-04-music-proofreading-with",
      "title": "Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context",
      "video": "video_77_ramoneda.mp4"
    },
    "forum": "77",
    "id": "77",
    "pic_id": "https://drive.google.com/file/d/1MMgr2dhkmNjdfdcLItJes-4fwbYPNM3a/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educators role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. This work illustrates how building on top of past research can  offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the MIR community and the music education one.",
      "abstract": "Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educators role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. This work illustrates how building on top of past research can  offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the MIR community and the music education one.",
      "author_changes": "",
      "authors": [
        "Ramoneda, Pedro*",
        " Eremenko, Vsevolod E",
        " D'Hooge, Alexandre",
        " Parada-Cabaleiro, Emilia",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Pedro Ramoneda (Universitat Pompeu Fabra)*",
        " Vsevolod E Eremenko (Music Technology Group at Universitat Pompeu Fabra)",
        " Alexandre D'Hooge (Universit\u00e9 de Lille)",
        " Emilia Parada-Cabaleiro (Nuremberg University of Music)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9EZSR4P",
      "day": "2",
      "keywords": [
        "Applications -> music training and education",
        "Computational musicology"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1hlcxltpSxXHhMIYDO3tOYRB8-4MknL8b/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/165PCbniHB28GHqVK2Lu6IxOe6a6wZAIM/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-03-towards-explainable-and",
      "title": "Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-efficient Approach",
      "video": "https://drive.google.com/file/d/1MZQ9UD99OaWpbzK6z2m1h28hCphyEh3U/view?usp=sharing"
    },
    "forum": "78",
    "id": "78",
    "pic_id": "https://drive.google.com/file/d/1Vir2OlbagGVMM8WMiUtdpEGM1RzC5-UD/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human-AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest teams. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP contributes to advancing understanding of human-AI co-creation from the users' perspective. Furthermore, we outline potential use cases for the dataset, ranging from analyzing AI tools utilized in songwriting to gaining insights into users' ethical considerations and expanding creative possibilities. This can help to inform both scholarly inquiry and practical applications in music composition and beyond.",
      "abstract": "The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human-AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest teams. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP contributes to advancing understanding of human-AI co-creation from the users' perspective. Furthermore, we outline potential use cases for the dataset, ranging from analyzing AI tools utilized in songwriting to gaining insights into users' ethical considerations and expanding creative possibilities. This can help to inform both scholarly inquiry and practical applications in music composition and beyond.",
      "author_changes": "",
      "authors": [
        "Morris, Lidia J*",
        " Leger, Rebecca",
        " Newman, Michele",
        " Burgoyne, John Ashley",
        " Groves, Ryan",
        " Mangal, Natasha",
        " Lee, Jin Ha"
      ],
      "authors_and_affil": [
        "Lidia J Morris (University of Washington)*",
        " Rebecca Leger (Fraunhofer IIS)",
        " Michele Newman (University of Washington)",
        " John Ashley Burgoyne (University of Amsterdam)",
        " Ryan Groves (Self-employed)",
        " Natasha Mangal (CISAC)",
        " Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCNBHFG",
      "day": "2",
      "keywords": [
        "Creativity -> computational creativity; Creativity -> creative practice involving MIR or generative technology ; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Creativity -> human-ai co-creativity"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1VjFIWqMwUxnsBKdPsG3jok97RngcMBSQ/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1GHUgD19TlzgmMjZSe60cgR2uMBZqGWqv/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-04-human-ai-music",
      "title": "Human-AI Music Process: A Dataset of AI-Supported Songwriting Processes from the AI Song Contest",
      "video": "https://drive.google.com/file/d/1Ple4x4zVPxCMeHRgnjA-WEE5OQt-epk3/view?usp=drive_link"
    },
    "forum": "79",
    "id": "79",
    "pic_id": "https://drive.google.com/file/d/1G7LXNeVNfho8g_TVWUYUFRe93En4BY-P/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Every artist has a creative process that draws inspiration from previous artists and their works. Today, \"inspiration\" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. We compare the effect of applying CLMR and CLAP embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet, a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples accompanying this paper are available at tinyurl.com/exploring-musical-roots.",
      "abstract": "Every artist has a creative process that draws inspiration from previous artists and their works. Today, \"inspiration\" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. We compare the effect of applying CLMR and CLAP embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet, a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples accompanying this paper are available at tinyurl.com/exploring-musical-roots.",
      "author_changes": "",
      "authors": [
        "Barnett, Julia*",
        " Pardo, Bryan",
        " Flores Garc\u00eda, Hugo"
      ],
      "authors_and_affil": [
        "Julia Barnett (Northwestern University)*",
        " Bryan Pardo (Northwestern University)",
        " Hugo Flores Garc\u00eda (Northwestern University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCQXSCNL",
      "day": "1",
      "keywords": [
        "Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        "MIR tasks -> similarity metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1CrDjx5hn-KAF72RcynMYQUtVrP--ZzxX/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/10FYn52ikEsrMZPj4oNGGvr7_wp95L-vM/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-20-exploring-musical-roots",
      "title": "Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model",
      "video": "https://drive.google.com/file/d/1enH3tfkgI9otV5uK-0IuExK_xaIaybXM/view?usp=drive_link"
    },
    "forum": "81",
    "id": "81",
    "pic_id": "https://drive.google.com/file/d/1kMAvAAFtGGHfOLJq58b734Yk463Ykcnh/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as \"purposeful play\". Furthermore, we discuss its implications for creative MIR.",
      "abstract": "The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as \"purposeful play\". Furthermore, we discuss its implications for creative MIR.",
      "author_changes": "",
      "authors": [
        "Newman, Michele*",
        " Morris, Lidia J",
        " Kato, Jun",
        " Goto, Masataka",
        " Yip, Jason",
        " Lee, Jin Ha"
      ],
      "authors_and_affil": [
        "Michele Newman (University of Washington)*",
        " Lidia Morris (University of Washington)",
        " Jun Kato (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Jason Yip (University of Washington)",
        " Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU8LG4B",
      "day": "2",
      "keywords": [
        "Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; Human-centered MIR -> user-centered evaluation",
        "Creativity -> creativity and learning"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Iq4_tkuSSKZdUphnPm-8DtyKE4jFWk2N/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/16URMaG7w64HLOG6njAm9UEc1GseMVYX1/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-04-purposeful-play-evaluation",
      "title": "Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications with Children",
      "video": "https://drive.google.com/file/d/1mP85ID1OC1_QFZreMCh-HxUQOwbNZLQp/view?usp=share_link"
    },
    "forum": "83",
    "id": "83",
    "pic_id": "https://drive.google.com/file/d/11W7-Grx5vqLGGNfNTkRSlmack3-92kQj/view?usp=share_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Despite the success of contrastive learning in Music Information Retrieval, the inherent ambiguity of contrastive self-supervision presents a challenge. Relying solely on augmentation chains and self-supervised positive sampling strategies may lead to a pretraining objective that does not capture key musical information for downstream tasks. We introduce semi-supervised contrastive learning (SemiSupCon), an architecturally simple method for leveraging musically informed supervision signals in the contrastive learning of musical representations. Our approach introduces musically-relevant supervision signals into self-supervised contrastive learning by combining supervised and self-supervised contrastive objectives in a simple framework compared to previous work. This framework improves downstream performance and robustness to audio corruptions on a range of downstream MIR tasks with moderate amounts of labeled data. Our approach enables shaping the learned similarity metric through the choice of labeled data which (1) infuses the representations with musical domain knowledge and (2) improves out-of-domain performance with minimal general downstream performance loss. We show strong transfer learning performance on musically related yet not trivially similar tasks - such as pitch and key estimation. Additionally, our approach shows performance improvement on automatic tagging over self-supervised approaches with only 5% of available labels included in pretraining.",
      "abstract": "Despite the success of contrastive learning in Music Information Retrieval, the inherent ambiguity of contrastive self-supervision presents a challenge. Relying solely on augmentation chains and self-supervised positive sampling strategies may lead to a pretraining objective that does not capture key musical information for downstream tasks. We introduce semi-supervised contrastive learning (SemiSupCon), an architecturally simple method for leveraging musically informed supervision signals in the contrastive learning of musical representations. Our approach introduces musically-relevant supervision signals into self-supervised contrastive learning by combining supervised and self-supervised contrastive objectives in a simple framework compared to previous work. This framework improves downstream performance and robustness to audio corruptions on a range of downstream MIR tasks with moderate amounts of labeled data. Our approach enables shaping the learned similarity metric through the choice of labeled data which (1) infuses the representations with musical domain knowledge and (2) improves out-of-domain performance with minimal general downstream performance loss. We show strong transfer learning performance on musically related yet not trivially similar tasks - such as pitch and key estimation. Additionally, our approach shows performance improvement on automatic tagging over self-supervised approaches with only 5% of available labels included in pretraining.",
      "author_changes": "",
      "authors": [
        "Guinot, Julien PM*",
        " Quinton, Elio",
        " Fazekas, George"
      ],
      "authors_and_affil": [
        "Julien Guinot (Queen Mary University of London)*",
        " Elio Quinton (Universal Music Group)",
        " Gy\u00f6rgy Fazekas (QMUL)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU8MTB5",
      "day": "2",
      "keywords": [
        "Knowledge-driven approaches to MIR",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; MIR tasks -> similarity metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ECakxR9sVf_ZHnob-6l6JbKdrWVgi38g/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1X1wQwqw57B8YJmkW-9TNJlcAkbnwneti/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-09-semi-supervised-contrastive",
      "title": "Semi-Supervised Contrastive Learning of Musical Representations",
      "video": "https://drive.google.com/file/d/19iL-BCJj3nzFS7B-iUXkv0H0J5Fp6N-M/view?usp=sharing"
    },
    "forum": "84",
    "id": "84",
    "pic_id": "https://drive.google.com/file/d/1SEF_iws0abIu6oXPeRGG0DcJNLTtro00/view?usp=share_link",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song\u2019s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their spectral characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch\u2019s method prioritizes the spectral properties of the bass drum over traditional pitch values, enhancing bass response and overall sound quality. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary music production.",
      "abstract": "The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song\u2019s key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their spectral characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch\u2019s method prioritizes the spectral properties of the bass drum over traditional pitch values, enhancing bass response and overall sound quality. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary music production.",
      "author_changes": "",
      "authors": [
        "Deruty, Emmanuel*"
      ],
      "authors_and_affil": [
        "Emmanuel Deruty (Sony Computer Science Laboratories)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCQY1D96",
      "day": "1",
      "keywords": [
        "Applications -> music composition, performance, and production; Computational musicology -> digital musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> musical style and genre",
        "Computational musicology"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/13eZzr4F6FtWq7Kb7YCHicXXkl4N3zA8R/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/1O2570jxaug06VoO0xsDhykKHpqJiMlV4/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-04-harmonic-and-transposition",
      "title": "Harmonic and Transposition Constraints Arising from the Use of the Roland TR-808 Bass Drum",
      "video": "https://drive.google.com/file/d/1-7SRb1F67xRtNMhbnUuc06tTkUX2YJ8N/view?usp=share_link"
    },
    "forum": "86",
    "id": "86",
    "pic_id": "https://drive.google.com/file/d/1qualw556L5tVvL-2lA3CeS-tWGvHDswT/view?usp=share_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further significant improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8% and 26.2% compared to the baseline model score of 0.562, respectively.",
      "abstract": "Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further significant improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8% and 26.2% compared to the baseline model score of 0.562, respectively.",
      "author_changes": "",
      "authors": [
        "Ju, Yaolong*",
        " Wu, Chun Yat",
        " Lorenzo , Betty Cortinas",
        " Yang, Jing",
        " Deng, Jiajun",
        " FAN, FAN",
        " Lui, Simon"
      ],
      "authors_and_affil": [
        "Yaolong Ju (Huawei)*",
        " Chun Yat Wu (Huawei)",
        " Betty Corti\u00f1as Lorenzo  (Huawei)",
        " Jing Yang (Huawei 2012 Labs)",
        " Jiajun Deng (Huawei)",
        " Fan Fan (Huawei)",
        " Simon Lui (Huawei)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F05UDD",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> expression and performative aspects of music",
        "Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1iTqz4z5OY5AaRH_0QhYaPILXT5iSZNle/view?usp=share_link",
      "poster_pdf": "",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-17-end-to-end",
      "title": "End-to-end automatic singing skill evaluation using cross-attention and data augmentation for solo singing and singing with accompaniment",
      "video": ""
    },
    "forum": "89",
    "id": "89",
    "pic_id": "",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on MelodyHub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",
      "abstract": "In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on MelodyHub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",
      "author_changes": "",
      "authors": [
        "Wu, Shangda",
        " Wang, Yashan",
        " Li, Xiaobing",
        " Yu, Feng",
        " Sun, Maosong*"
      ],
      "authors_and_affil": [
        "Shangda Wu (Central Conservatory of Music)",
        " Yashan Wang (Central Conservatory of Music)",
        " Xiaobing Li (Central Conservatory of Music)",
        " Feng Yu (Central Conservatory of Music)",
        " Maosong Sun (Tsinghua University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5U0SCV",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music generation; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form",
        "MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1I72iajjKOPsQMmee8CDFiNCSIkQdArvP/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/1jZrxMhcXPZU7iMAQID3nf_BI246t2GNs/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-19-melodyt5-a-unified",
      "title": "MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing",
      "video": "https://drive.google.com/file/d/1GaJwHFJhCRNmFLeZGYLMhi9EKvnr7JV6/view?usp=share_link"
    },
    "forum": "90",
    "id": "90",
    "pic_id": "https://drive.google.com/file/d/1KZHABImBHV5T-_TK4OEVb1glgv-Xpbs5/view?usp=share_link",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music  model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted chords and rhythm features as the control signal. During inference, the control can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets---one derived from extracted features and the other from user-created inputs---demonstrates that MusiConGen can generate realistic music that aligns well with the specified temporal control. Sound examples can be found at the supplementary material and the anonymous demo page, \\url{https://musicongen.github.io/musicongen_demo/}.",
      "abstract": "Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music  model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted chords and rhythm features as the control signal. During inference, the control can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets---one derived from extracted features and the other from user-created inputs---demonstrates that MusiConGen can generate realistic music that aligns well with the specified temporal control. Sound examples can be found at the supplementary material and the anonymous demo page, \\url{https://musicongen.github.io/musicongen_demo/}.",
      "author_changes": "",
      "authors": [
        "Lan, Yun-Han*",
        " Hsiao, Wen-Yi",
        " Cheng, Hao-Chung",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Yun-Han Lan (Taiwan AI Labs)*",
        " Wen-Yi Hsiao (Taiwan AI Labs)",
        " Hao-Chung Cheng (National Taiwan University)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5U2FGV",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Generative Tasks; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/11kDKpVYCpSs9YI2UGG9QecHPLjwvv2pv/view?usp=share_link",
      "poster_pdf": "https://drive.google.com/file/d/1uZgfvu9DHuCKXcIOYjnMWWVEwHWVV4Rb/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-14-musicongen-rhythm-and",
      "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
      "video": "https://drive.google.com/file/d/1vXa5hVkLjiP-AsDcsQmkokGJekFP-6gd/view?usp=share_link"
    },
    "forum": "91",
    "id": "91",
    "pic_id": "https://drive.google.com/file/d/1RzIDsBLRZwLAWZo6aQsL_5mEw6IdL-B2/view?usp=share_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of it\u0002erations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realis\u0002tic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30\u00d7 faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of appli\u0002cations (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we in\u0002terpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",
      "abstract": "Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of it\u0002erations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realis\u0002tic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30\u00d7 faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of appli\u0002cations (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we in\u0002terpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",
      "author_changes": "",
      "authors": [
        "Comunita, Marco",
        " Zhong, Zhi*",
        " Takahashi, Akira",
        " Yang, Shiqi",
        " Zhao, Mengjie",
        " Saito, Koichi",
        " Ikemiya, Yukara",
        " Shibuya, Takashi",
        " Takahashi, Shusuke",
        " Mitsufuji, Yuki"
      ],
      "authors_and_affil": [
        "Marco Comunit\u00e0 (Queen Mary University of London)",
        " Zhi Zhong (Sony Group Corporation)*",
        " Akira Takahashi (Sony Group Corporation)",
        " Shiqi Yang (Sony)",
        " Mengjie Zhao (Sony Group Corporation)",
        " Koichi Saito (Sony Gruop Corporation)",
        " Yukara Ikemiya (Sony Research)",
        " Takashi Shibuya (Sony AI)",
        " Shusuke Takahashi (Sony Group Corporation)",
        " Yuki Mitsufuji (Sony AI)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F0AJ6T",
      "day": "2",
      "keywords": [
        "Generative Tasks -> interactions; Generative Tasks -> real-time considerations; Generative Tasks -> transformations; MIR tasks -> automatic classification; Musical features and properties -> representations of music",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Vinmy5AVoQSPJOvA-qYFu32kPhx8_r8x/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1YZkyRX_07zw72sOvTA9eoaEjcDtPfVIm/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-07-specmaskgit-masked-generative",
      "title": "SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond",
      "video": "https://drive.google.com/file/d/1GvfnJaoT73dkCHy0CsV5WcrTSjfomUFe/view?usp=sharing"
    },
    "forum": "96",
    "id": "96",
    "pic_id": "https://drive.google.com/file/d/1h34o3dFNby363faQnn4IYC8mFwPWu4fc/view?usp=sharing",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of raw audio data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data's lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",
      "abstract": "We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of raw audio data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data's lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",
      "author_changes": "",
      "authors": [
        "ZHU, Jinlong*",
        " Sakurai, Keigo",
        " Togo, Ren",
        " Ogawa, Takahiro",
        " Haseyama, Miki"
      ],
      "authors_and_affil": [
        "Jinlong ZHU (Hokkaido University)*",
        " Keigo Sakurai (Hokkaido University)",
        " Ren Togo (Hokkaido University)",
        " Takahiro Ogawa (Hokkaido University)",
        " Miki Haseyama (Hokkaido University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F0C4RM",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> representations of music",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1h2RYen_J2xlE_zcIMKar6f6grNUAsP2C/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1V-r2tzbzCUOKtJWrLyoxeOmBnNSKeAB-/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-14-mmt-bert-chord",
      "title": "MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
      "video": "https://drive.google.com/file/d/1ShFl4bLk6UHjMko6B562YsCrbwWrtQKF/view?usp=sharing"
    },
    "forum": "101",
    "id": "101",
    "pic_id": "https://drive.google.com/file/d/13TVApqN6sCQay-u3RZCWiJK6LzyXrfBW/view?usp=sharing",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",
      "abstract": "Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network --- TheGlueNote --- which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",
      "author_changes": "We thank the reviewers for their thoughtful and constructive feedback! We made the following changes for the final submission:\n- removed uninformative Figure 1 (note alignment visualization) mainly to make room for:\n- extended section 4.1 with a thorough description of the data augmentations\n- improved readability of Figure 2 (now Figure 1): consistent wording, high-contrast matrices, loss description in main text\n- Table 4 includes mean results for all model/dataset variations\n- added links and acknowledgements\n- changed title of 5.1\n- fixed several typos\n- reworded \"non-local information\"\n- removed confusing use of cross- and self-attention\n- fixed table numbering\n- fixed several references\n",
      "authors": [
        "Peter, Silvan*",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Silvan David Peter (JKU)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU92QGK",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This is a meta-review, summarising the main points of the individual reviews.\n\nTraditional methods like Dynamic Time Warping (DTW) or Hidden Markov Models (HMM) struggle with aligning symbolic note sequences that have large mismatches. This paper proposes a novel approach using various complex manipulations (e.g., repeats, skips, block insertions, and long trills) to train a model that predicts pairwise note similarities for two 512-note subsequences. As a post-processing step, DTW is used and compared to two alternatives. The approach is shown to lead to very high accuracy, even in the presence of large mismatches, and it is directly applicable to any kind of MIDI file.\n\nStrengths:\n- Innovative Approach: The paper proposes a novel method using a transformer-based model for aligning symbolic note sequences, which addresses the limitations of traditional methods like DTW and HMM, especially in the presence of large mismatches.  \n- Detailed Explanation: The model and experimental setup are explained in detail, providing a clear understanding of the approach and its implementation.  \n- High Accuracy: The proposed method demonstrates very high accuracy, outperforming baseline methods and showing robustness against a large number of mismatches.  \n- Reproducibility: The authors promise to provide code and datasets online, which facilitates reproducibility and further research.\n\nWeaknesses:\n- Clarity in Description: Some parts of the paper, particularly the data augmentations and certain concepts (see e.g. the comment on attention by reviewer 4) could be explained more clearly.  \n- Figures 1 and 2 have been noted to either lack informativeness or be too dense. Additionally, inconsistencies in terminology and numbering in tables need addressing.  \n\nPlease also consider the very detailed and helpful comments by the individual reviewers!\n\nOverall, the paper is a valuable contribution to ISMIR and recommended for acceptance.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1GHRxv4pvcpny1pO3Lgx1W1u7kbGoUXnn/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PfmicdgUqlBQIlc7QFRllz3Km0qc3w_D/view?usp=sharing",
      "review_1": "This paper presents a method to perform note alignment via learned representation.\nIt essentially learns pairwise similarity of two 512-note sequences.\nThe core of the method is a transformer encoder that encodes the note sequences to determine a similarity measure, which is then ultimately matched by a match extractor, using either (1) taking the maximally similar pairs, (2) using a transformer decoder to return the matches, and (3) using DTW.  DTW-based matching is done in two stages, by first roughly aligning by typical DTW path constraints, and match the notes pitch-wise so that is close to the initial DTW result.\n\nThe method is evaluated with various augmentations including repeats, skips, insertions, deletions and trills.  The results show the method outperforms baseline of using pitch-onset similairty matrix, and shows that the DTW-based processing performs the best.  The paper also shows larger models with more residual dimension and blocks perform better.\nFinally, the method is compared against existing methods, which shows the method is more robust under great number of mismatches.\n\nThe paper is interesting and provides a robust method for note-level alignment, which is valuable for the community.  There are a few questions, however.\n\nFirst, please state the terminal conditions for the DTW, i.e. if there are constraints at the beginning and the end of the sequences.  Second, in theory this model can be applied to sequences of arbitrary lengths, except potentially when using a transformer decoder.  It would have been nice to understand the performance when the note duration is varied (and hence the extent of nonlocal information is provided to the model is varied).",
      "review_2": "This paper presents \u201cTheGlueNote\u201d, a transformer encoder that predicts note similarities for the alignment of two MIDI sequences with potentially complex mismatch cases such as note repetitions, insertions, or deletions. The authors give a good introduction into the topic, and, to the best of my knowledge, reference all important related work. The core model and its extensions are described in detail, as well as the experimental setup. Together with the promise of providing the code and datasets online, this offers researches an excellent starting point for reproducing results and testing the architecture in their own scenarios. The proposed model is evaluated in three different levels of complexity (# parameters) and three different decoding variants. A comparison with suitable baseline methods is conducted and the results of the proposed model seem to be impressive, while requiring relatively row runtime. \n\nI therefore recommend a \u201cstrong accept\u201d for this paper.\n\nFor the camera-ready version of this paper, I have a few minor suggestions:\n- Make Fig. 1 more illustrative, e.g., by zooming in to a smaller segment and highlighting the problems for interesting sections like trills, insertions,\u2026 \n- Enhance the readability of the similarity matrices in Fig. 2 by e.g., choosing black and white colormaps and improving the contrast\n- Explain the data augmentation in Table 2 better. What does gT_t2^{n_t} mean? What\u2019s does U(-50,50) (given in MIDI ticks) mean in seconds? Does P_{repeat/skip/trill}=1 mean that the probability of repeats/skips/trills is one? Does it mean that every note is repeated or becomes a trill?\n- Facilitate the quick readability of Table 4; It is very interesting to see the results individually for all pieces, but space would permit to add two more columns representing the mean score over all pieces for Default data and 20% mismatch data.",
      "review_3": "This paper proposed TheGlueNote, a transformer-based model to learn note-wise representations for note alignment in Midi files. The training data is synthesized by data augmentation with complex mismatch cases. Authors also compare three ways to extract note matches from the similarity matrix computed on the learned representations. The experiment and results are convincing, but the paper writing can be improved (see details below).\n\n\nmajor comments:\n1. Since the proposed model and data augmentation are the key parts of the paper, they should be described more clearly.\nLine 183 -186\n\u2018processed using the fixed-length structured tokenization [22, 23], which encodes relative onset, pitch, duration, and velocity.\u2019\nAuthors should describe how onsets, pitch, duration and velocity are encoded, rather than assuming that the readers have read [22, 23]. Only with this information, the numbers in Table 2 will make sense.\n\nSection 4.1\nData augmentations should be describe more clearly. The notations in the Table2 should be explained clearly in the paper.\n\nLine 306\n\u201can experiment including extended (100+ note) mismatches\u201d\nThe author should describe more about how the extended mismatches are augmented. This will help readers to understand in which aspects the proposed model is robust to.\n\n2. the confusion of the concepts\nLine 12-13, Line 192-195\nThe authors concatenate s1 and s2 as one sequence as the network input.\nThe concepts of \u2018within-sequence self-attention\u2019, and \u2018between-sequence cross-attention\u2019 are confused with the self-attention and cross-attention commonly-used in the Transformer. Please describe this concepts in a better way.\n\n\nminor comments:\n\n\u2018CEL Row, CEL Col, CEL\u2019\nThe loss functions are only mentioned in the Figure 2 caption. \nPlease clearly describe the losses in the paper as well and clearly write \u2018cross-entropy loss (CEL)\u2019 in the paper. \n\nFigure 2\n\u2018Data processing\u2019 in Figure 2, but \u2018pre-processing module\u2019 in the caption. Please make them consistent.\nPlease clearly mark the \u2018Attention Block\u2019 in TheGlueNote part, and it should also be \u2019n \\times Attention Block\u2019 as in the Decoder Head part.\n\nTable 3\n\u2018#ph\u2019 in the table, but \u2018#p dh\u2019 in the caption. Please make them consistent.\n\nLine 60\nwhat do you mean by \u2018non-local information\u2019? Please rewrite the sentence to make it clear. \n\nLine 74-78\nPrior approaches \u2026 \nadd references at the end of the sentence \nor mention that, for example, \u2018Please see details in related work (Section 2).\u2019\n\nLine 163\nto to => to\n\nLine 175-177\n\u2018A pairwise similarity matrix of the note representations of either sequence\u2019 =>A pairwise similarity matrix computed between the note representations of two sequences\n\nLine 195 \ncross-attention (s1-s2, s2-s1)\ndoes s1-s2 is the same as s2-s1 after transposing s2-s1? Please describe the relationship between s1-s2 and s2-s1.\n\nSection 5.1 title\nmodel configuration => Ablation study of model configuration\n\nSome references are not in correct format.\nPlease check carefully.",
      "session": [
        "4"
      ],
      "slack_channel": "p4-14-thegluenote-learned-representations",
      "title": "TheGlueNote: Learned Representations for Robust and Flexible Note Alignment",
      "video": "https://drive.google.com/file/d/12FVcGsJpZaS3qKKGy82vNAFXsaaoTm_9/view?usp=sharing"
    },
    "forum": "103",
    "id": "103",
    "pic_id": "https://drive.google.com/file/d/1CcnPsI0Fq-S-7zkqwYY6WIRd5K3wrNAn/view?usp=share_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "This paper discusses the transcription of a collection of musical works using Optical Music Recognition (OMR) technologies during the implementation of the Spanish PolifonIA project. The project employs a research-oriented OMR application that leverages modern Artificial Intelligence (AI) technology to encode musical works from images into structured formats. The paper outlines the transcription workflow in several phases: selection, preparation, action, and resolution, emphasizing the efficiency of using AI to reduce manual transcription efforts. The tool facilitated various tasks such as document analysis, management of parts, and automatic content recognition, although manual corrections were still indispensable for ensuring accuracy, especially for complex musical notations and layouts. Our study also highlights the iterative process of model training and corrections that gradually improved transcription speed and accuracy. Furthermore, the paper delves into challenges like managing non-musical elements and the limitations of current OMR technologies with early musical notations. Our findings suggest that while automated tools significantly accelerate the transcription process, they require continuous refinement and human oversight to handle diverse and complex musical documents effectively.",
      "abstract": "This paper discusses the transcription of a collection of musical works using Optical Music Recognition (OMR) technologies during the implementation of the Spanish PolifonIA project. The project employs a research-oriented OMR application that leverages modern Artificial Intelligence (AI) technology to encode musical works from images into structured formats. The paper outlines the transcription workflow in several phases: selection, preparation, action, and resolution, emphasizing the efficiency of using AI to reduce manual transcription efforts. The tool facilitated various tasks such as document analysis, management of parts, and automatic content recognition, although manual corrections were still indispensable for ensuring accuracy, especially for complex musical notations and layouts. Our study also highlights the iterative process of model training and corrections that gradually improved transcription speed and accuracy. Furthermore, the paper delves into challenges like managing non-musical elements and the limitations of current OMR technologies with early musical notations. Our findings suggest that while automated tools significantly accelerate the transcription process, they require continuous refinement and human oversight to handle diverse and complex musical documents effectively.",
      "author_changes": "",
      "authors": [
        "Rizo, David*",
        " Calvo-Zaragoza, Jorge",
        " Delgado-S\u00e1nchez, Teresa",
        " Garc\u00eda-Iasci, Patricia"
      ],
      "authors_and_affil": [
        "David Rizo (University of Alicante. Instituto Superior de Ense\u00f1anzas Art\u00edsrticas de la Comunidad Valenciana)*",
        " Jorge Calvo-Zaragoza (University of Alicante)",
        " Patricia Garc\u00eda-Iasci (University of Alicante)",
        " Teresa Delgado-S\u00e1nchez (Biblioteca Nacional de Espa\u00f1a)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCQYBRG8",
      "day": "1",
      "keywords": [
        "Applications -> digital libraries and archives",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1NLEOmBHoUA5CLQvB9Bzolv9PWAIPySnl/view?usp=share_link",
      "poster_pdf": "",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-02-lessons-learned-from",
      "title": "Lessons learned from a project to encode Mensural music on a large scale with Optical Music Recognition",
      "video": ""
    },
    "forum": "104",
    "id": "104",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017\u20132023 that introduce an AMG model, and 2) ISMIR papers from the years 2022\u20132023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",
      "abstract": "The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017\u20132023 that introduce an AMG model, and 2) ISMIR papers from the years 2022\u20132023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",
      "author_changes": "",
      "authors": [
        "Holzapfel, Andre*",
        " Kaila, Anna-Kaisa",
        " J\u00e4\u00e4skel\u00e4inen, Petra"
      ],
      "authors_and_affil": [
        "Andre Holzapfel (KTH Royal Institute of Technology in Stockholm)*",
        " Anna-Kaisa Kaila (KTH Royal Institute of Technology, Stockholm)",
        " Petra J\u00e4\u00e4skel\u00e4inen (KTH)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPU957MZ",
      "day": "2",
      "keywords": [
        "Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        "Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Philosophical and ethical discussions -> legal and societal aspects of MIR"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1rAepoJk1U2R4g3S_AcxdqWDGzRGd7xPg/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1cYKFcKBYn5ndyPg95kzhNTzvaUouhemE/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-01-green-mir-investigating",
      "title": "Green MIR? Investigating computational cost of recent music-Ai research in ISMIR",
      "video": "https://drive.google.com/file/d/1FIrhz2X1ejLKxNdDNhi3msM28OhC1MA3/view?usp=share_link"
    },
    "forum": "113",
    "id": "113",
    "pic_id": "https://drive.google.com/file/d/1tPtxxZyBA0nvUGkMXkTlkGIyxkIwzxft/view?usp=share_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",
      "abstract": "Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",
      "author_changes": "We appreciate the reviewers' feedback and have made the following changes in the final version:\n\n1. Added a comparison with Wang et al.'s work to provide more context and highlight our contributions.\n2. Removed the figure illustrating the alignment algorithm.\n3. Redrawn the system overview figure, incorporating more details about the training process.\n\nRegarding the limitations of MCA in reflecting output quality and the inconsistencies in weak alignment, we have included them in our discussion of future work to emphasize their importance for further research.\n\n",
      "authors": [
        "Tan, Chih-Pin*",
        " Ai, Hsin",
        " Chang, Yi-Hsin",
        " Guan, Shuen-Huei",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Chih-Pin Tan (National Taiwan University)*",
        " Hsin Ai (National Taiwan University)",
        " Yi-Hsin Chang (National Taiwan University)",
        " Shuen-Huei Guan (KKCompany Techonologies)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MNBZT3",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Applications -> music composition, performance, and production; Generative Tasks -> transformations; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music generation; Musical features and properties -> musical style and genre"
      ],
      "long_presentation": "FALSE",
      "meta_review": "Based on the reveiwers and internal discussion, we would like to  voted for a weak acceptance. Please see R2 on the concern on evaluation, and please also do incorporate the important missing reference to deliver a complete study.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZtVeAwZ4H40Ed2JSPt2uKGBJNjhMvXcP/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1YM75UhJ5geuLqXYH111Zk8vm9diDLPtd/view?usp=drive_link",
      "review_1": "The paper introduces a method for generating piano covers from pop songs using a transfer learning model that employs weakly-aligned data instead of strongly-aligned, note-remapped data. This method retains the musical integrity of the original piano compositions and addresses data inaccuracy issues due to forced synchronization in previous methods. The paper is overall well-written and informative, yet several aspects could be improved:\n\nFirst, the proposed pairing function for weakly aligning the piano and song addresses the rhythmic distortion caused by note-remapping but relies heavily on the performance of the beat detection algorithm. The paper lacks an investigation into the accuracy and efficiency of this algorithm. Since 50% of the bars were removed due to mapping errors (mentioned in Line 303), this method results in a significant loss of data for training or testing the model. A better justification for choosing this method over others, such as midi-to-audio alignment algorithms, is needed.\n\nSecond, the use of SheetSage to extract \"high-level\" musical ideas from song audio raises some concerns. While SheetSage was primarily trained for melody extraction, which aligns with generating accurate melody contours for piano cover generation, it may not effectively handle tempo and accompaniment reconstruction. The SheetSage encoder could lead to unexpected information loss compared to using the MT3 encoder for processing song audio. This issue could be observed in the provided listening samples, where chords and tempo were reconstructed less effectively compared to melody.\n\nFinally, considering that melody matching is a crucial criterion for evaluating the success of a piano cover system, I recommend re-evaluating the melody chroma accuracy after some post-processing to align the generation with the target. This would provide a more meaningful comparison with the Pop2Piano model. Given that the target (human composition) has a low MCA score, it is difficult to justify the performance of the proposed model without this adjustment.",
      "review_2": "Good task direction! without relying on perfect alignment is crucial for the task. The methodology is clear and well-explained. The final fine-tuning to avoid errors from beat detection is intelligent.\n\nMy first comment is that i would remove from the title \"transfer learning approach and\". Although the contributions of the paper are both, the latter for me is a strongest contribution.\n\n\nHowever, my criticisms are mainly about the task evaluation. The subjective metrics are significantly better than the objective ones, which is noteworthy. Considering there is no bias in the subjective tests, I believe an analysis of why the objective metrics perform poorly would strengthen the paper. It would also be beneficial to provide reasons for the musical dimensions that are not being considered, and perhaps even propose a new metric for the task.\n\n\nAnother point of concern is the significant cherry-picking in the examples provided. I would prefer to see the YouTube video,  \"pop2song\" and your main approach. Given that the dataset you compiled has fewer than 100 pairs, it would have been fairer to show everything. It is evident that the model does not perform like a human, but examining the cases where the algorithm fails is also interesting.\n",
      "review_3": "#### Strengths\n\nThe paper \"PIANO COVER GENERATION WITH TRANSFER LEARNING APPROACH AND WEAKLY ALIGNED DATA\" presents a significant advancement in the field of automatic piano cover generation. One of the notable strengths of this work is its innovative use of transfer learning, which leverages pre-training on piano-only data followed by fine-tuning on weakly-aligned song-to-piano paired data. This approach effectively addresses the common issue of losing piano information during the synchronization process, which has plagued previous models that relied on strongly aligned data.\n\nThe methodology is robust, involving the use of a lead sheet transcription model as the encoder during the pre-training phase. This step ensures that the model learns high-level musical concepts rather than merely transcribing audio. By fine-tuning on weakly-aligned data, the model is able to retain the musical quality of the original piano performances, thereby producing more natural and musically coherent outputs. The experiments conducted across five music genres demonstrate the model's versatility and effectiveness, as it outperforms baseline models in both objective and subjective evaluations.\n\nMoreover, the use of a decoder-only Transformer model that interleaves song audio and piano performance sequences enhances the temporal correspondence between the condition and target outputs. This architectural choice, coupled with the innovative training strategy, contributes to the high quality of the generated piano covers.\n\n#### Limitations\n\nDespite the strengths, the paper does have some limitations that should be addressed. One of the primary concerns is the lack of a direct comparison with the model presented in the arXiv paper \"AUDIO-TO-SYMBOLIC ARRANGEMENT VIA CROSS-MODAL MUSIC REPRESENTATION LEARNING\" (arXiv 2112.15110). This omission is significant, as the arXiv paper addresses a similar problem space using a cross-modal representation learning approach, which also aims to capture major audio information for symbolic music generation. A comparative analysis would provide a clearer benchmark for the performance of the proposed transfer learning model.\n\nFurthermore, while the paper emphasizes the advantages of weakly-aligned data, it does not fully explore the potential limitations and challenges associated with this approach. For instance, the alignment errors between the piano segments and their corresponding song segments, even at a beat level, could introduce inconsistencies that affect the overall quality of the generated covers. ",
      "session": [
        "4"
      ],
      "slack_channel": "p4-07-picogen2-piano-cover",
      "title": "PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data",
      "video": "https://drive.google.com/file/d/1GN7GywKGpF4j-DxhbAJey0_uLccSOJre/view?usp=drive_link"
    },
    "forum": "114",
    "id": "114",
    "pic_id": "https://drive.google.com/file/d/1c8Y5WtQiVYHB4GSufL3261GJ2Y00-rWZ/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Generative models are starting to become very good at generating realistic text, images, and even music. Identifying how exactly these models conceptualize data has become crucial. To date, however, interpretability research has mainly focused on the text and image domain, leaving a gap in the music domain. In this paper, we investigate the transferability of straightforward text-oriented interpretability techniques to the music domain. Specifically, we examine the usability of these techniques for analyzing how the generative music model MusicGen constructs representations of human-interpretable musicological concepts. Using the DecoderLens, we gain insight into how the model gradually composes these concepts, and using interchange interventions, we observe the contributions of individual model components in generating the sound of specific instruments and genres. We also encounter several shortcomings of the interpretability techniques for the music domain, which underscore the complexity of music and need for proper audio-oriented adaptation. Our research marks an initial step toward understanding generative music models, fundamentally, paving the way for future advancements in controlling music generation.",
      "abstract": "Generative models are starting to become very good at generating realistic text, images, and even music. Identifying how exactly these models conceptualize data has become crucial. To date, however, interpretability research has mainly focused on the text and image domain, leaving a gap in the music domain. In this paper, we investigate the transferability of straightforward text-oriented interpretability techniques to the music domain. Specifically, we examine the usability of these techniques for analyzing how the generative music model MusicGen constructs representations of human-interpretable musicological concepts. Using the DecoderLens, we gain insight into how the model gradually composes these concepts, and using interchange interventions, we observe the contributions of individual model components in generating the sound of specific instruments and genres. We also encounter several shortcomings of the interpretability techniques for the music domain, which underscore the complexity of music and need for proper audio-oriented adaptation. Our research marks an initial step toward understanding generative music models, fundamentally, paving the way for future advancements in controlling music generation.",
      "author_changes": "",
      "authors": [
        "V\u00e9lez V\u00e1squez, Marcel A*",
        " Pouw, Charlotte",
        " Burgoyne, John Ashley",
        " Zuidema, Willem"
      ],
      "authors_and_affil": [
        "Marcel A V\u00e9lez V\u00e1squez (University of Amsterdam)*",
        " Charlotte Pouw (University of Amsterdam)",
        " John Ashley Burgoyne (University of Amsterdam)",
        " Willem Zuidema (ILLC, UvA)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGF4WSY",
      "day": "3",
      "keywords": [
        "MIR tasks -> music generation",
        "Knowledge-driven approaches to MIR -> representations of music; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR tasks -> music synthesis and transformation; Musical features and properties -> musical style and genre; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1DWHlGw08xGoHtjrDJ8TAcrODnM2Oj23F/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/17ku5P9dyPTxyVAiHvOOkdDv9Gx1rt4zx/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-14-exploring-the-inner",
      "title": "Exploring the inner mechanisms of large generative music models",
      "video": "https://drive.google.com/file/d/18eFvercPT7l7NlT6AmTBgswkiGaN3YWk/view?usp=drive_link"
    },
    "forum": "119",
    "id": "119",
    "pic_id": "https://drive.google.com/file/d/1TyD-z9NevZqChQ5fWC9mxy634k888c2n/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "In piano education for preschool to early elementary school-aged children, both weekly lessons with a teacher and daily home practice supported by parents are fundamental for skill advancement. However, the nature of students' home practice, often unseen by teachers, has been less examined. This paper aims to (1) identify challenges inherent in home practice and (2) develop a system to mitigate these issues, enhancing teachers' understanding and support of students' home practice to improve piano performance skills. Through observing actual lessons and analyzing 177 days of practice videos from 30 students, this study identifies effective strategies, such as assigning appropriately difficult pieces and rewarding with stickers or stamps. It also highlights obstacles, including student tension under parental guidance and repetitive, unproductive practice. The insights from field study suggest the potential of third-party(system) feedback, practice segmentation, reporting practice records to teachers, and rewards for practice sessions. We developed a system incorporating these solutions and examined with 80 children over four months. Results showed increased teacher engagement with students' home practice, improved student motivation and practice duration, and enhanced sight-reading skills, demonstrating the system's effectiveness in supporting piano education.",
      "abstract": "In piano education for preschool to early elementary school-aged children, both weekly lessons with a teacher and daily home practice supported by parents are fundamental for skill advancement. However, the nature of students' home practice, often unseen by teachers, has been less examined. This paper aims to (1) identify challenges inherent in home practice and (2) develop a system to mitigate these issues, enhancing teachers' understanding and support of students' home practice to improve piano performance skills. Through observing actual lessons and analyzing 177 days of practice videos from 30 students, this study identifies effective strategies, such as assigning appropriately difficult pieces and rewarding with stickers or stamps. It also highlights obstacles, including student tension under parental guidance and repetitive, unproductive practice. The insights from field study suggest the potential of third-party(system) feedback, practice segmentation, reporting practice records to teachers, and rewards for practice sessions. We developed a system incorporating these solutions and examined with 80 children over four months. Results showed increased teacher engagement with students' home practice, improved student motivation and practice duration, and enhanced sight-reading skills, demonstrating the system's effectiveness in supporting piano education.",
      "author_changes": "",
      "authors": [
        "Fukuda, Seikoh*",
        " Fukuda, Yuko",
        " Motomura, Ami",
        " Sasao, Eri",
        " Hosoda, Masamichi",
        " Matsubara, Masaki",
        " Niitsuma, Masahiro"
      ],
      "authors_and_affil": [
        "Seikoh Fukuda (PTNA Research Institute of Music)*",
        " Yuko Fukuda (Kyoritsu Women\u2019s University, To-on Kikaku Company)",
        " Ami Motomura (To-on Kikaku Company)",
        " Eri Sasao (To-on Kikaku Company)",
        " Masamichi Hosoda (NTT East Corporation)",
        " Masaki Matsubara (University of Tsukuba)",
        " Masahiro Niitsuma (Keio University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5V2669",
      "day": "2",
      "keywords": [
        "Applications -> music training and education",
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/104iLn5nzx0ocx9skVb8iCcF9iaJzQ_Ol/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1YmK5bZ6NGhemaO6_HooMS3V_lyfvXjqi/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-02-field-study-on",
      "title": "Field Study on Children's Home Piano Practice: Developing a Comprehensive System for Enhanced Student-Teacher Engagement",
      "video": "https://drive.google.com/file/d/1Xt8hHGue5Hq-y___PIdsHzVR7qZoJrBa/view?usp=drive_link"
    },
    "forum": "128",
    "id": "128",
    "pic_id": "https://drive.google.com/file/d/1aQ0OKoof7HABrUyBoPslf8grP5xgd4Av/view?usp=drive_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.cusing on an under-explored genre.",
      "abstract": "Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI- vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.cusing on an under-explored genre.",
      "author_changes": "",
      "authors": [
        "Sarmento, Pedro Pereira",
        " Loth, Jackson J*",
        " Barthet, Mathieu"
      ],
      "authors_and_affil": [
        "Pedro Pereira Sarmento (Centre for Digital Music)",
        " Jackson J Loth (Queen Mary University of London)*",
        " Mathieu Barthet (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUACTUK",
      "day": "3",
      "keywords": [
        "Creativity -> human-ai co-creativity; Creativity -> humanistic discussions; Musical features and properties -> musical style and genre; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies; Philosophical and ethical discussions -> philosophical and methodological foundations",
        "Generative Tasks -> qualitative evaluations"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1YtkoSFwjRZRWBoBSyzxItdPMH0HS2-gj/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1xI02FsNtZTNZsYLy4xH-1-lK30ok51vy/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-07-between-the-ai",
      "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
      "video": "https://drive.google.com/file/d/1Gz6u9QdoBF6iREW6k4742hjVStT5sNMH/view?usp=drive_link"
    },
    "forum": "131",
    "id": "131",
    "pic_id": "https://drive.google.com/file/d/1p4iM4QJ49Hr4TaPrP9qkgUISHLKvhf39/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose.  We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, which demonstrate varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances.  Our best approach achieved 93.6% accuracy in expertise ranking, 33.7% in difficulty estimation, and 46.7% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.",
      "abstract": "Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose.  We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, which demonstrate varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances.  Our best approach achieved 93.6% accuracy in expertise ranking, 33.7% in difficulty estimation, and 46.7% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.",
      "author_changes": "I mainly added to the background and literature reviews to make them more coherent with the work itself, as well as some explanation on the results. I'd love to add more discussions as suggested, but was limited by space.  \n\nI also highlighted some details that the reviewer are confused about.",
      "authors": [
        "Zhang, Huan*",
        " Liang, Jinhua",
        " Dixon, Simon"
      ],
      "authors_and_affil": [
        "Huan Zhang (Queen Mary University of London)*",
        " Jinhua Liang (Queen Mary University of London)",
        " Simon Dixon (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCQD31U",
      "day": "2",
      "keywords": [
        "Applications -> music training and education",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "There has been a consensus among the reviewers regarding this paper for accepting it. I strongly advise the authors to look at the detailed comments from the reviewers and address the raised points in order to further improve the publication.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1rFIbPl_S3DB7RuKDouwM1G93VJs2xNyG/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1vqusYksr6Y8gJrfP-wWV-3TjpQQQAMrZ/view?usp=drive_link",
      "review_1": "As the title states, the paper demonstrated the three piano-performance-related tasks with pre-trained (and general-purpose) audio encoding embeddings. \nThe authors not only utilized existing task setups but also contributed to composing new datasets (expertise ranking and technique identification). \nThe paper is written clearly and is easy to read.\n\nThe paper refers to many related papers and introduces related issues well. However, I expect more literature reviews on the downstream tasks. For example, what is each downstream task's state of the art, and what is the expectation? \n\nThe paper covers quite a wide range of topics, as each downstream task does not have a firm benchmark. Thus, the authors also had to propose the dataset and the evaluation procedure. The paper is compact enough, but the experiments could be elaborated on more. That makes me feel the 6-page limit is short for this paper.\n\nI have two concerns about the message of the paper.\nFirst, I'm not convinced that the proposed projection model is the proper design for these downstream tasks. The CNN-based model only allows for the analysis of very local features, while difficulty and expertise ranking are expected to require some temporal features. I worried that the conclusion of the paper could not be general.\n\nSecond, the correlation between piece selection and expertise ranking is not discussed enough. The network might only consider the piece, not the pianist's touch or sound. The ICPC-2015 dataset slightly resolves it, but I think the discussion on this issue is important but handled enough.\n\nI think the paper has many points of contribution. but also, I believe the paper's main message could be elaborated in both the discussion and the experiment.",
      "review_2": "- This paper discusses an approach for 3 tasks; expertise ranking, difficulty estimation, and \npiano technique detection leveraging pre-trained audio encoders. \n- Paper also introduces a dataset Pianism-Labelling Dataset (PLD).\n- The authors have enumerated the contributions in the introduction. This is helpful for the reader to quickly get a clear idea about the paper. \n- This approach is the first of its kind in piano technique detection and expertise ranking. It is good to explicitly state this. The research gap in difficulty estimation is not clear in the Related Work. \n- Section 3.1.1: It is not very clear how R4 works, if a pair of performances have the same Q value. Looks like ranking is not possible with this metric when both the performances are from the persons with the same expertise level.\n- It is not clear how the neural network is designed to do the n-way ranking. \n\n-  Fine tuned Audio-MAE is performing better in case of Expertise Ranking whereas fine tuned DAC is performing better in the case of competition dataset. Looking a bit more into the data on which these embeddings are trained with may give an explanation for this. \n\n- It will be interesting to see the how the embeddings of the performances of expert players differ from that of beginners. This may help to understand the nature of the information,  embeddings are able to communicate to the further layers in the neural network. \n\nThe authors present a relatively simple approach for the discussed tasks among which some of the tasks are novel. Experiments are quite rigourous considering different embeddings and, the design of the medthodology is driven by the nature of the tasks. Various relevant  metrics are employed for evaluation. ",
      "review_3": "The paper proposes new performance-based music evaluation tasks, introduces a new dataset, and evaluates several deep pretrained audio music models as feature generators for solving the tasks.\nThe paper presents initial experiments with the tasks, but they are well executed and highlight the gaps that need to be addressed in the future.\n\nSome comments:\n- since fine-tuning does not seem to improve results or even seems to worsen them - explain how you chose the fine-tuning parameters (e.g. learning rate, number of epochs, etc.) and whether they might play a role in this.\n- I would not attribute the poorer performance of the expertise ranking in the Chopin competition to the \"sight over sound phenomenon\". I would imagine that it has much more to do with the fact that most of the participants in the competition are experts, whereas the model is trained to distinguish between beginner, advanced and virtuoso levels.",
      "session": [
        "4"
      ],
      "slack_channel": "p4-02-from-audio-encoders",
      "title": "From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano",
      "video": "https://drive.google.com/file/d/1wi4OTC5XBQhQ7qpas0Rn9vleCH3E40W_/view?usp=drive_link"
    },
    "forum": "132",
    "id": "132",
    "pic_id": "https://drive.google.com/file/d/1GtHVvjc3ynd1kpH5Yi-nMyLSmsxd49FV/view?usp=drive_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks -- pitch spelling and cadence detection -- demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https://github.com/manoskary/graphmuse",
      "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks -- pitch spelling and cadence detection -- demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https://github.com/manoskary/graphmuse",
      "author_changes": "",
      "authors": [
        "Karystinaios, Emmanouil*",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Emmanouil Karystinaios (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5W2E0M",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1v3xnltIXVjl686mPvV34AsC3eN_vNlXr/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1OBlsXp-7Cbh0dH5M1H7HEpVbFp0bsDAt/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-20-graphmuse-a-library",
      "title": "GraphMuse: A Library for Symbolic Music Graph Processing",
      "video": "https://drive.google.com/file/d/1hsWV5uz5Y3sBhXcOUIYbhFU5CzFdMBdl/view?usp=drive_link"
    },
    "forum": "142",
    "id": "142",
    "pic_id": "https://drive.google.com/file/d/1yKYiFaUKBy94b4hVCUKY5VxPMavNYv5w/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems.  We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",
      "abstract": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems.  We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",
      "author_changes": "",
      "authors": [
        "Bjare, Mathias Rose*",
        " Lattner, Stefan",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Mathias Rose Bjare (Johannes Kepler University Linz)*",
        " Stefan Lattner (Sony Computer Science Laboratories, Paris)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCR0SXSL",
      "day": "3",
      "keywords": [
        "MIR tasks -> music generation",
        "Applications -> music composition, performance, and production; Creativity -> computational creativity; Creativity -> human-ai co-creativity; Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1OfyMa9IiQxA-tXZ3Z8mIu3EQosT7b0ke/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1IuShjd5Ve4Rlu8GXnRA-6E5RrjWD3Qxm/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-14-controlling-surprisal-in",
      "title": "Controlling Surprisal in Music Generation via Information Content Curve Matching",
      "video": "https://drive.google.com/file/d/1C5iStI9QSKT53hU1edU3-s5IPpl-WLto/view?usp=drive_link"
    },
    "forum": "143",
    "id": "143",
    "pic_id": "https://drive.google.com/file/d/1peWDFb7ZP5WXiPBECA59VmMawalX4-hC/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.",
      "abstract": "Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.",
      "author_changes": "",
      "authors": [
        "Hu, Patricia*",
        " Mart\u00e1k, Luk\u00e1\u0161 Samuel",
        " Cancino-Chac\u00f3n, Carlos Eduardo",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Patricia Hu (Johannes Kepler University)*",
        " Luk\u00e1\u0161 Samuel Mart\u00e1k (Johannes Kepler University Linz)",
        " Carlos Eduardo Cancino-Chac\u00f3n (Johannes Kepler University Linz)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCRP206",
      "day": "4",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1aMATsrlzx0nQEK39SiEANw0oS1TMtV8P/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/12ef3ghWqBJuRoYMf0kdy_SISdzbXNUsJ/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-14-towards-musically-informed",
      "title": "Towards Musically Informed Evaluation of Piano Transcription Models",
      "video": "https://drive.google.com/file/d/1ePJF7Y-vNl2ByEydncvcK0FzFsVaXONq/view?usp=drive_link"
    },
    "forum": "144",
    "id": "144",
    "pic_id": "https://drive.google.com/file/d/1vItA8zUFd-yhDIhJktFSwkB0OcGmCCtq/view?usp=drive_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion inference-time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T-Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.",
      "abstract": "Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion inference-time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T-Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.",
      "author_changes": "",
      "authors": [
        "Novack, Zachary*",
        " McAuley, Julian",
        " Berg-Kirkpatrick, Taylor",
        " Bryan, Nicholas J."
      ],
      "authors_and_affil": [
        "Zachary Novack (UC San Diego)*",
        " Julian McAuley (UCSD)",
        " Taylor Berg-Kirkpatrick (UCSD)",
        " Nicholas J. Bryan (Adobe Research)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F3LCNT",
      "day": "3",
      "keywords": [
        "Generative Tasks -> real-time considerations; MIR tasks -> music generation",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/17SwbJ3L5MUNOuP8LEDQim0X_LXGKKAbm/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/15LEuiK9PlbbtGaccNVc-b0FZW57CbOW1/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-07-ditto-2-distilled",
      "title": "DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation",
      "video": "https://drive.google.com/file/d/1-hSAWIENA4xRie320xWofu0KbbUOfFxr/view?usp=drive_link"
    },
    "forum": "146",
    "id": "146",
    "pic_id": "https://drive.google.com/file/d/1kPhHdEsYRq0lmiaOiBBKnS5QjGwD8EWP/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This paper surveys 560 publications about music genre recognition (MGR) published between 2013\u20132022, complementing the comprehensive survey of [474], which covered the time frame 1995\u20132012 (467 publications). For each publication we determine its main functions: a review of research, a contribution to evaluation methodology, or an experimental work. For each experimental work we note the data, experimental approach, and figure of merit it applies. We also note the extents to which any publication engages with work critical of MGR as a research problem, as well as genre theory. Our bibliographic analysis shows for MGR research: 1) it typically does not meaningfully engage with any critique of itself; and 2) it typically does not meaningfully engage with work in genre theory.",
      "abstract": "This paper surveys 560 publications about music genre recognition (MGR) published between 2013\u20132022, complementing the comprehensive survey of [474], which covered the time frame 1995\u20132012 (467 publications). For each publication we determine its main functions: a review of research, a contribution to evaluation methodology, or an experimental work. For each experimental work we note the data, experimental approach, and figure of merit it applies. We also note the extents to which any publication engages with work critical of MGR as a research problem, as well as genre theory. Our bibliographic analysis shows for MGR research: 1) it typically does not meaningfully engage with any critique of itself; and 2) it typically does not meaningfully engage with work in genre theory.",
      "author_changes": "# Summary of Changes\n\n* Abstract opening includes citation to Sturm\u2019s original survey [meta / R1]\n* Abstract and text both highlight the date range of this survey [meta / R1]\n* Missing paper noted has been incorporated, analysed and the data in the paper updated [R2]\n* Added to discussion a note acknowledging the possibility of a follow up examining auto-tagging as a related endeavour (discussed below in our response) [R2]\n* The 560 survey references were not sorted in a consistent way in the original submission. This is now fixed; note that this changes the citation numbers.\n* Reinstated to the supplement a discussion of different Figures of Merit, to alleviate concerns of unfamiliar terminology [meta/R3]\n\n# Terse Responses to Other Comments (2k char limit!)\n\nWith thanks to the reviewers, there are some changes we did not make, either from respectful disagreement or spatial constraints.\n\n* Reliance on Sturm's methodology: We see this as a legitimate and robust approach, as we are seeing what has changed methodologically since 2013\n* Annotation errors: with thanks for diligence, we cannot fix without details. We welcome corrections, and our repo will have a message saying so\n* Organization / self-containment: We appreciate the sympathy, and emphasize that we feel it important to address this to ISMIR, practical and cultural challenges to critical work notwithstanding\n* Coverage of musicology: The supplement covers this, we feel to an adequate degree\n* Highlight influential work: We already highlight work we feel is meritorious; influence could be an interesting follow-up\n* Coverage of new datasets: We feel is adequate to the scope, esp. as 4/5 most used datasets are still pre-2013\n* Coverage of representation learning: We feel is out of scope, like other algorithms\n* Various interesting follow-ups (changes in genre itself, extended bibliometrics): Thanks! Some rich material for an extended follow-up",
      "authors": [
        "Green, Owen*",
        " Sturm, Bob L. T. ",
        " Born, Georgina",
        " Wald-Fuhrmann, Melanie"
      ],
      "authors_and_affil": [
        "Owen Green (Max Planck Institute for Empirical Aesthetics)*",
        " Bob L. T.  Sturm (KTH Royal Institute of Technology)",
        " Georgina Born (University College London)",
        " Melanie Wald-Fuhrmann (Max Planck Institute for Empirical Aesthetics)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCSADAA",
      "day": "3",
      "keywords": [
        "MIR tasks -> automatic classification",
        "Philosophical and ethical discussions -> philosophical and methodological foundations"
      ],
      "long_presentation": "FALSE",
      "meta_review": "Overall, the reviewers felt that this paper has a number of important strengths, including:\n\n- An impressively extensive and detailed survey of publications on automatic music genre recognition research from 2013 to 2022.\n\n- Detailed data and source code are provided.\n\n- Well-informed and thought-out critical commentary that emphasizes the need to carefully (re)consider the aims, methodology and impact of music genre recognition at a fundamental level.\n\nHowever, the reviewers also highlighted the following problems with the paper:\n\n- The first sentence of the abstract is confusing and uncomfortably close to reference [129] by  Sturm, which is not cited in the abstract (but is cited in the rest of the paper). Reference [129] must be explicitly indicated in this first sentence\n\n- Perhaps too much reliance on the (cited) methodology used and critical perspectives expressed in Sturm\u2019s earlier survey of genre recognition work before 2013, as well as on other work previously published work by Sturm and his collaborators (also cited in this paper). Could have expanded here a little more (although there certainly are novel contributions too).\n\n- Some objective annotation errors were found by one reviewer when a (small) sampling was performed of the raw data in in the provided \u201cnew_survey.csv\u201d file.\n\n- One reviewer suggested that more explicit discussion of specifically musicological issues connected to genre recognition (as opposed to just the social sciences and humanities in general). It was also suggested that a number of relevant musicological publications on genre recognition may have been missed because the survey methodology used was better suited to finding more technical sources than musicological sources, which still often tend to be less likely to be published (and indexed) electronically.\n\n- One reviewer suggested that the paper should include discussion of related work in music auto-tagging and music representation learning, as they play key connected roles in the evolution of the field of music genre recognition during the time period in question. Related to this, it was pointed out that this paper does not adequately discuss how the kinds of datasets being used are shifting to larger ones more suitable to deep learning. This, combined with the concerns expressed by another reviewer in connection to musicological sources, suggests that this survey, while certainly extensive, may have systematically excluded certain important kinds of publications and developments relating to automatic genre recognition.\n\n- With respect to clarity, some of the terminology used might be confusing to readers who have not read Sturm\u2019s earlier work specifically, as this paper incorporates terms from it that are not necessarily widespread or understood by the MIR community at large.\n\n- One reviewer suggested that it would be helpful to highlight particular publications mentioned in the survey that have been especially influential or that are of particular merit, and to provide a sense of how different publications may be connected to one another.\n\nPlease see the individual reviews for more details.\n\nThanks to the authors for submitting this work. If it is not ultimately accepted at ISMIR this year, we encourage them to revise it based on the suggestions in the individual reviews and resubmit next year, as we believe this work can be an important contribution to the ISMIR community. The authors may also wish to submit this to TISMIR, as it seems that this paper may be overly constrained by the length limitations of ISMIR.\n",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1BSGRQpoOINbaS9DOBRyVzaB4EL2sZe9w/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/12jc4wEqTSpiPIoltg1YFjTufK5eOhjYY/view?usp=drive_link",
      "review_1": "I'll say flat out at the start of this review: color me impressed with this paper. This work is deep, detailed, and acutely observed, and it sheds light on the need for some serious introspection within the MGR field. I am reminded of the ISMIR 2023 paper \u201cThe Games We Play\u201d, which I was asked to review last year and enthusiastically recommended for publication \u2013 both papers issue clarion calls to stop operating on autopilot and rethink the aims and impact of scholarship in the fields studied.\n\n\tI have no problem calling the literature review exhaustive in the domain of MGR, certainly as exhaustive as I could have imagined. In particular, I like the thoughtful collection of critical studies of genre that are aggregated and presented as meaningful commentaries with which the analyzed authors are not grappling. The works of Simon Frith, Philip Tagg, Leonard Meyer, Keith Negus, and other socio-musicological theorists of genre feel like they talk about music, and musical style, in a completely different world than the vast majority of toolbuilders for MGR (or for that matter, ISMIR) research. The paper puts, beautifully, a crucial observation about the purpose and function of the 500+ papers reviewed: they are \u201cseldom motivate[d...to learn] something about music rather than classifier performance.\u201d In a nutshell, this is the problem with so much MIR research; it is often dramatically more concerned with Information Retrieval than with Music.\n\n\tThe other meta-analytical observations of the paper are similarly trenchant. The paper reiterates and extends Sturm's concerns about the use of the GTZAN dataset, and is justified in noting the lack of caveats appended to online mirrors of the set. Section 3.8, on discussions of genre theory, suggests that the whole field may need to go back to the drawing board conceptually. What does MGR research even mean by genre, if it is playing so loosely with such a notoriously slippery eel?\n\n\tThe abstract could use a little more detail on the theoretical contributions and could do with the elimination of some boilerplate. The sentence \u201cWe present several recommendations\u201d is vague and not very useful to the prospective reader, and there's no need for the abstract to point to supplementary material.\n\n\tThere are some organizational problems with this paper that feel as if they are generated from the desire to publish at ISMIR despite having a paper and topic that simply is too expansive to address within the 6-plus-n page format. Obviously, it's unusual for 'n' to be 31 pages (this doesn't bother me, but it's probably an ISMIR record), but there's also an additional supplement file describing some of the finer points of method choices, and much of what's in there, in my view, ought to be in the full paper proper. I certainly don't want to drive good work to other venues at this conference's expense, but the parsimony that is required at ISMIR seems like a real Procrustean bed for a paper of this breadth and depth. ISMIR is, decidedly, a place where it needs to be heard.",
      "review_2": "This work offers a systematic literature review for the task of Music Genre Recognition (MGR), leveraging and extending the work by Sturm et al., conducted in 2013 and considering publications up to 2022. The paper provides statistics on published works of MGR regarding these 8 axes: publication type, MGR datasets used, experimental design, figure of merit, justification of the MGR task, publication venue, presence of critic to MGR, and engagement with musical genre theory.\u00a0\n\nNotably, the main strength of this work is its comprehensiveness compared to the work by Sturm in 2013, both in terms of the number of publications covered (1026 vs 467) and dimensions analyzed (8 vs. 3).\u00a0\n\nIn general, I find that this survey follows a continuistic approach compared to the previous work by Slurm. My main point of criticism is the lack of discussion over certain aspects that I consider crucial to understanding the evolution and current open directions of MGR. Namely:\n\nInfluence of Music Auto-Tagging (MAT): From the appearance of datasets such as the Million Song Dataset or the MagnaTagATune, a part of the community shifted their attention from MGR to MAT. While MAT differs from MGR in several aspects, including being generally modeled as a multi-label task, and working with label sets typically derived from online folksonomies instead of created by musicologists, the connection to MGR research is obvious. While I understand that this connection was not discussed by Sturm since the first relevant auto-tagging papers are contemporaneous with his survey, I find it surprising that it is not addressed in this work.\n\nMusic representation learning: Developing broad-purpose representation models that can be finetuned in multiple music understanding tasks, including MGR, has become a popular approach (including several examples already published before 2022). It is a bit disappointing that this paradigm shift is not reflected or acknowledged in this study and that these methods are simply reflected as traditional classify experiments.\n\nDatasets. The dataset section summarizes existing publications in terms of the most used datasets and features, but it doesn\u2019t discuss or help to understand why newer datasets are developed and used. For example, in my opinion, the community is shifting towards larger MGR datasets consisting of music distributed under permissive licenses (such as the FMA or the genre subset of the MTG-Jamendo dataset) due to a tendency to rely on deep learning approaches requiring more data and to favor reproducibility. However, these aspects are not discussed in detail.\n\nAdditionally, these are some minor comments to the authors:\n- Line 80-82: In my opinion such details about file formats can be omitted if not relevant later on.\n- Line 491-493: I would say most researchers interested in Genre would nowadays do some form of representation model, and probe their systems in several downstream datasets, possibly (but not necessarily) including GTZAN. The motivation is that being able to show consistent improvements across multiple music classification datasets (including MGR) suggests more robust and generalizable performance than using a single small dataset such as GTZAN. In my opinion, this sentence neglects the effort present in many of the referred publications [372, 373, 421, 521]* for developing more robust evaluation setups. \n* references from the paper.\n- Section 3.7: In my opinion, many MIR researchers interested in MGR, are already familiar with the pitfalls of GTZAN. Although it is always worth mentioning it, this section could be less focused on this particular problem, and instead, develop a bit more other less-known critics to common practices in MGR.\n- Section 3.8: Consider that an overall intuition of how genre theory is typically addressed from the perspective of social science and humanities would be valuable by many MIR researchers.\n- Consider including [1], a work that performs MGR using GTZAN and performs a cross-collection validation to assess the generalization capabilities of the learned model (classify, feature, and generalize). \n\nFinally, while I appreciate the comprehensive and systematic approach of this research, my recommendation to the authors is to consider taking into account the aforementioned aspects to bring an updated perspective on MGR.\u00a0\n\n\n[1] Alonso-Jim\u00e9nez, P., Bogdanov, D., Pons, J., & Serra, X. (2020, May). Tensorflow audio models in essentia. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "review_3": "This paper presents a comprehensive survey of Music Genre Recognition (MGR) research published between 2013 and 2022, extending the previous survey done by Sturm [219]. It describes the process for the paper corpus creation, as well as the different dimensions considered when classifying the papers. Such dimensions are analysed individually and discussed, evidencing the main issues (still) present in MGR research.\n\nThe paper is dense but properly written, easy to understand and well structured. The methodology is detailed in a way that is easy to understand every step, and the analysis is synthetic but clear, providing examples for every dimension analysed. I also appreciate the new aspects not considered in the previous Sturm\u2019s survey, i.e., analysing the engagement of MGR research with critical work and genre theory. Overall, I believe the paper could be a great addition to the conference, and therefore I am prone to recommend accepting it, even if with one reservation.\n\nIn fact, several concepts inherited from the previous Sturm\u2019s survey, used also in this work, are detailed in the supplementary material. Having read [219] in the past, for me it was not difficult to understand the meaning of the proposed terminology, especially with regards to \u201cExperimental Design\u201d and \u201cFigure of Merit\u201d, however a reader engaging for the first time with these concepts might feel disoriented. Therefore, I am not sure if the principle of presenting a \u201cself-contained\u201d  paper is respected in this submission. However, I remit to the Scientific Program Chairs the decision on whether this aspect may preclude or not the acceptance. \n\nIn terms of the methodology, one aspect of the approach that I do not particularly like, in this work as in [129], even if I do accept it, is the egalitarian vision of the corpus of MGR research, i.e. in the presented analysis every paper counts one. Even if I do understand the value of a comprehensive survey for depicting a picture of the current status of the research in the field, I also see a major limit in this approach: I am quite sure that most of the research done in the area is not properly done and neither robust from a scientific perspective, but after reading this paper I still am not sure what work is worth reading and what is not. It would be nice to have a list of which among the 599 papers may be considered a good reading.\n\nEventually, this was not the goal of this work, and therefore this is not a major issue, but I would like to suggest reflecting on the value of conducting a quality assessment for selecting publications that respect certain standards, instead of taking whatever is out there. It is a common practice in medical research (see [1]), where the volume of work published is huge, and maybe also a survey on MGR research could benefit from it. \n\nOne missed opportunity, which somehow could be approached by using bibliometric analysis as also written in lines 525-529, is the lack of a more fine-grained temporal analysis of the publications in MRG research. In fact, if we consider the construction of knowledge as a process that evolved through time, it would be interesting to see how publications are temporally connected and see when things started to go in a specific direction e.g. maybe a paper using the \u201cClassify\u201d design with a confusion matrix, cite a previous paper using \u201cClassify\u201d and a confusion matrix, which cite a previous paper using \u201cClassify\u201d and a confusion matrix, and so on. \n\nI would also recommend adding a comparative table between the corpus of [129] and the one presented in this manuscript, because it might be helpful to catch easily the differences between the two corpora, instead of having only such aspects discussed in the main text e.g. lines 234, 238, 251, 269.\n\nLastly, I would also add in the conclusion that another main problem which threatens the validity of MGR research is the reviewers who accept for publication works that lack any sort of validity and scientific rigour, so it is not only the author's fault.  \n\nIn conclusion, I would like to thank the authors for this submission, and I hope to see in future more critical research on MGR like this one presented.\n\n[1] Delavari S, Pourahmadi M, Barzkar F. What Quality Assessment Tool Should I Use? A Practical Guide for Systematic Reviews Authors. Iran J Med Sci. 2023 May;48(3):229-231. doi: 10.30476/IJMS.2023.98401.3038. PMID: 37791333; PMCID: PMC10542923.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-12-a-critical-survey",
      "title": "A Critical Survey of Research in Music Genre Recognition",
      "video": "https://drive.google.com/file/d/1zno_hZyNoFxobuXbAucl2wttAv_UFUdq/view?usp=drive_link"
    },
    "forum": "149",
    "id": "149",
    "pic_id": "https://drive.google.com/file/d/1ob_iIyuP03H5EDRaqelgeqdo1sQXrJRe/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",
      "abstract": "Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",
      "author_changes": "",
      "authors": [
        "Nakatsuka, Takayuki*",
        " Hamasaki, Masahiro",
        " Goto, Masataka"
      ],
      "authors_and_affil": [
        "Takayuki Nakatsuka (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGJJ8G4",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "MIR and machine learning for musical acoustics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/179hgCXSyqxXxz1erf68XrB8Qy09KbX_R/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1xRye0jC3dcCoaocwUr0RzfrK0pcEw4dY/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-11-harnessing-the-power",
      "title": "Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval",
      "video": "https://drive.google.com/file/d/1C_MVMWoqeur5iulba_NfP21N647EL0W8/view?usp=drive_link"
    },
    "forum": "155",
    "id": "155",
    "pic_id": "https://drive.google.com/file/d/1Qv2AWSlvU5TxzQTTBP6tFCISYjdfYNpA/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.",
      "abstract": "The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files.  We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values.  This technique preserves more score information while reducing sequence lengths by 3.5x compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.",
      "author_changes": "We would like to thank all reviewers for their helpful comments and constructive feedback. \n\nWe have incorporated suggestions about the scope of the paper and additional detail about the tokenization method into the title and abstract (#1).\nAdditionally, we clarified the explanation of the conditioning and space tokens, moving them into Section 3.1.2. and Section 2.2, respectively (#4). \nThe tokenization section now includes more detail about handling polyphonic music, note ordering, and the encoding of musical time (#4 and meta #1).\nDataset statistics in Table 2 were recomputed in order to reconcile inconsistent preprocessing between different splits (#4).\nFurthermore, section 3.3 now contains additional information on how MusicXML scores were obtained for baseline methods (#4).\n\nDue to space restrictions, we were unable to insert a figure illustrating the tokenization in the paper (meta #1). However, clear code for all tokenization/detokenization steps will be available on GitHub, along with an illustration of a tokenization of a melody.\n\nWe also want to address our focus on classical music and the ASAP dataset (#3). To our knowledge, there are no other publicly available datasets which include paired performance-MIDI and MusicXML scores. CrestMusePEDB is the only other dataset in the literature with MusicXML scores, but it is smaller, not openly available, and also limited to classical music. To illustrate our method's generalization, the supplementary material includes examples of generations for out-of-genre music from in-the-wild performances. \n\nFinally, we fixed various typos and formatting issues (#4).\nTo adhere to the page limit, some changes required minor reworking of other text passages in the paper; however, no content was significantly altered.",
      "authors": [
        "Beyer, Tim*",
        " Dai, Angela"
      ],
      "authors_and_affil": [
        "Tim Beyer (Technical University of Munich)*",
        " Angela Dai (Technical University of Munich)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCSSRSA",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> rhythm, beat, tempo; Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper describes end-to-end MIDI-to-score conversion, which has not been focused on in the MIR field.\n\nThe paper is written very well. The proposed method is technically solid and its effectiveness was clearly shown by the well-designed experiments.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/10tX3QWkVV5ALUq3aD0QwXSJ4JP9dmD1B/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1X8aKfd0wZiXnLgeiCqNYOBWXzxHWIDLY/view?usp=drive_link",
      "review_1": "Overall, the paper is very clear and well-written. The methodological decisions are motivated, and the evaluation is rigorous, using standard metrics, comparing with the state-of-the-art works, and with an ablation study for key design decisions. In addition, the code and pre-trained models will be released upon publication.\n\nSince the dataset used for training and evaluation only includes piano performances (ASAP), I suggest including this aspect in the title (e.g., \"End-to-end piano performance-MIDI...\") and in the abstract.\n\n",
      "review_2": "This work presents an approach for obtaining a musical score from a performance MIDI file based on Transformers. The authors consider a reference dataset for the experiments and compare against existing approaches (both commercial and research-oriented) with several evaluation metrics and conclude the superiority of their proposal.\n\nThe manuscript is well written and sounds technical. Also, while the authors are commited to share the code and data if the work is accepted for publication, they also share as supplementary material to the conference some examples of the results that the approach may obtain.\n\nMy only concern in the work is the exclusive use of the ASAP dataset (as labeled data). In this sense, have the authors consider any other alternative dataset, at least for a particular experiment, to assess the generalization capabilities of the approach?",
      "review_3": "This paper proposed a new end-to-end approach to performance-MIDI to score conversion using the Transformers architecture. The proposed method is well-motivated (there is no one-to-one mapping between a performance MIDI sequence and a music score note sequence), achieves good results (beating the state-of-the-art), well-explained, and supported with rigorous experiments.\n\nApart from the key contributions stated in the paper's introduction, the biggest advantage of this paper, from my point of view, is the detailed technical considerations presented in the paper. The use of data (labelled and unpaired), the tokenization of the music score data (11 attributes derived from the MusicXML score, and the use of space token to fulfill beat-level alignment), the data augmentation (especially the duration jitter and onset jitter to simulate human performance), and the detailed ablation study makes the paper technically strong and provides plenty of useful insights to people working on related field. I especially like the way the paper describes data-related design choices by supporting them with comparisons to previous/other settings (e.g. Section 3.1.2 Unpaired data and Table 5). Furthermore, it's great to see the ethics statement which mentions future research for other music genres.\n\nOverall, I think it's a good paper that should be presented at ISMIR, which will have good audiences and can be very helpful to others working on related tasks.\n\nBelow are only some minor comments.\n\n* L27: \"significant downstream applications\" => significant number of downstream applications?\n* L147: \"Eq. (4)\" => Eq. (5)\n* L178: \"ml_j stores the preceding measure's length for downbeat notes or is set to false otherwise.\" => I'm still not very sure what ml_j is precisely. Is a downbeat note a note those onset is on a downbeat? How is it related to the measure's length?\n* L205: \"an inner dimension of 3072 for the position-wise feed-forward network\" => It is said at the beginning of this paragraph that \"the backbone model follows the original architecture described by Vaswani et al.\" But the original feed-forward dimension is 2048.\n* L211: After reading this paragraph, it's still not clear to me what are the \"conditioning token\" and the \"space token\". These are made clear after I finished reading Section 3.1.2. It may be helpful to polish this paragraph a bit, and add a reference to Section 3.1.2.\n* L234: \"space token steam\" => stream\n* **Table 2**: The total number of distinct pieces, P-MIDI notes, and score notes are not equal to the corresponding sum over the train/valid/test splits.\n* Table 4: Which SOTA is it in the table? Add a reference?\n* Table 5: MIDI scores do not actually have stem directions. How are the stems obtained for the MIDI scores (e.g. by importing to MuseScore or Finale)? It will be helpful to mention that to improve reproducibility. Or to annotate it as a \"-\" indicating there is no stem prediction.\n* L387: How are the barlines predicted after removing the time signature module? And similarly, how are the predictions converted into MusicXML format for evaluation?\n* References: Some minor formatting issues for items [6, 19, 20, 25, 26, 27, 29, 40, 41], mostly related to capitalisation.\n",
      "session": [
        "2"
      ],
      "slack_channel": "p2-15-end-to-end",
      "title": "End-to-end Piano Performance-MIDI to Score Conversion with Transformers",
      "video": "https://drive.google.com/file/d/11SgGetPXGSIFCwFpgwiUIefA0Hy3jLjk/view?usp=drive_link"
    },
    "forum": "158",
    "id": "158",
    "pic_id": "https://drive.google.com/file/d/1WEIyNwDfEfHyazJa093LiyjPY5310QN-/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",
      "abstract": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",
      "author_changes": "Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: \n\n- We have made a modification on our title so that it best fits the content. \n- We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.\n- We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).\n- In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. \n- We have addressed spelling errors and inconsistencies. \n- We have added a link to access our proposed dataset. ",
      "authors": [
        "Muluneh, Mequanent Argaw*",
        " Peng, Yan-Tsung",
        " Su, Li"
      ],
      "authors_and_affil": [
        "Mequanent Argaw Muluneh (Academia Sinica",
        " National Chengchi University",
        " Debre Markos University)*",
        " Yan-Tsung Peng (National Chengchi University)",
        " Li Su (Academia Sinica)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5YDGLV",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR",
        "Applications -> music heritage and sustainability; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "meta_review": "After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.\n\nAll reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.\n\nHowever, the authors should improve the paper before submitting the camera-ready version on the following aspects.\n\nFirst, the readability of the paper should be improved, since the language at some points is not clear enough.\n\nSecondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)\n\nThirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.\n\nFinally, details about reproducibility, regarding not only access to the data, but to the code, should be given.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link",
      "review_1": "ISMIR Paper Review # 160\n\n\nThe paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. \n\n\u2014 The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. \n\n\u2014 Overall the paper is well written. \n\n\u2014 The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. \n\n\u2014 The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on \u201c\t\tClassifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. \n\n\u2014  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. \n\n\u2014 It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. \n\n\u2014 Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. \n\n\u2014 I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. \n\n\nResult: Somewhere between borderline/weak accept and accept. Could have been much much better.  ",
      "review_2": "This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.\n\nThe authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the \"vital insights\" aren't described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors _must_ reconsider the statements in the evaluation part of Section 4.\n\n### Detailed comments ###\nIn the supplied material, it is observable that the singer is louder in the beginning than towards the end. In \"Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3\" there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?\n\nFigure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It's unclear what exactly the red boxes mean and where the \"recurring consistent melodic patterns\" appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.\n\nFigure 3: Needs units on the axes. Y = number of files, X = seconds.\n\nFigure 4: Suggestion: could superimpose the blue GMM estimations onto the red & green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.\n\nTable 1: Misspelling: Array -> Araray\n\nTable 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.\n\n161: \"except for color coding\": you mean to say that with color coding it's easier to identify a mode?\nFootnote 7: what do you mean by \"the reverse is not [...] true\", that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text\n180, 190 & 444: the spelling of Kidase/Qidase-bet varies.\n222: \"way larger\" is a colloquialism\n240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.\n248: what does \"sliding\" refer to?\n253-254: what do you mean by \"during the performance\" and \"along the whole recording\"? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?\n267: \"less number of parameters\" than what?\n270: \"as regard it as\" -> \"regarding it as...\"?\n277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?\n297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?\n304: you mention \"15 seconds\", but in the results (Table 2) you show \"20 seconds\". Apart from that, how much silence do these durations contain?\n312: 23 percentage points is NOT a \"relative small performance drop\"!\n313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.\n318: from the numbers, it is not evident how you make the claim that stabilization helps \"for most of the cases\". The _opposite_ is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!\n320: with -> within\n323: \"...has better classifcation accuracy _than the masking_\", because this is not true for no stabilization.\n336-340: It's not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to \"no calibration\" and \"no stabilization\". The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.\n377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?\n380: \"Fig. 6\" -> do you mean 4?\n400: remove second \"g_2\"\n\nMinor comment:\nIt would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge'ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.",
      "review_3": "= SUMMARY =\n\nThe authors investigate the Chant of the Tewahido (Ethiopian Orthodox) Church, which has its roots in the 6th century, with MIR.\nA primary goal is to classify three main types of the original Yaredawi YeZema Silt standard, which is shown to be feasible best with pitch contours. The pitch distributions that are characteristic for each of the three types are further analyzed with a GMM.\nA corresponding, public and manually curated audio dataset will be published upon publication of the paper. The experiments include a cross-corpus comparison with an existing smaller dataset.\n\n\n= EVALUATION =\n\nSTRENGTHS:\n\n(+) Interesting ideas for modelling the chant types by pitch distribution are presented.\n\n(+) Cross-corpus experiments and analysis.\n\n(+) The paper is generally well written and understandable.\n\nWEAKNESSES:\n\n(-) Some parts of the machine learning experiment are not clearly described or well designed (see major comments below). Especially, it is not fully clear which representation is fed into the neural network classifier.\n\n(-) All samples in the dataset originate from a single singer. (However, the cross-database analysis compensates this, partially.)\n\n\n= MAJOR COMMENTS =\n\n-General comment: It might be mentioned if all EOTC chants are monophonic or if there are also polyphonic ones.\n\n-Introduction, ll.56-57: \"based on the analysis of a series of EOTC Chants collected in Addis Ababa, 1975.\" -> It might be pointed out that the recordings originate from 1975 and if also written documents/oral statements were analyzed in the study. (It is slightly confusing when reading, given that the papers are from the 1990s).\n\n-Section 2.1, ll.95-96: \"which need to be manipulated for the language\" -> It is not clear why and in which way (especially) audio and video need to be manipulated.\n\n-Section 3: It would be interesting to give some information on the approximate number of EOTC schools and students in Ethiopia.\n\n-Section 4 reads as if only pitch-related features are taken into account. However, they are compared to MFCCs etc. This is slightly confusing when reading through the paper so it should be considered to rephrase.\n\n-Section 4.1: It is not fully clear, how the slope \"s\" is computed? Is it simply the regression line across all (stable) pitch values over time (i.e., minimizing the squared error)?\n\n-Section 4.1: It is not understandable, which representation is input to the neural network. Is it the frequency contour of the pitch (in Hz or normalized) or a histogram (distribution)? The same applies to Section 4.2. Moreover, it should be considered to have a development split for hyper-parameter optimization and early stopping.\n\n-Section 4.2: Taking other statistics besides the mean, e.g., standard deviation and slope, into account might give a meaningful performance boost.\n\n-Section 4.2: A 5-fold CV might not be the best choice for the within-dataset experiment if different chants were manually segmented beforehand, so that the same chant can appear in both training and test splits.\n\n-Section 5: \"The recording which has the highest average correlation with all the other recordings is considered as an anchor:\" -> It should be discussed/reasoned whether this approach for pitch shifting is the optimal one. As an example, it might lead to a different solution taking into account the minimum summed-up absolute (or squared) pitch shift across all samples of the dataset.\n\n-Section 8: It might be meaningful to consider the usage restricted to educational purposes in the license when publishing the dataset.\n\n\n= MINOR COMMENTS =\n\n-General comment: \"chant\" is sometimes capitalized and sometimes not, which should be unified.\n-Section 2.1, l.94: \"[11]).\" -> drop \")\"\n-Footnote 5: \"data in reported\" -> \"data reported in\"\n-Section 2.1, l.111: \"like some other\" -> \"like for some other\"\n-Section 2.2, l.129: \"sorrow. [1].\" -> \"sorrow [1].\"\n-Section 2.2, l.138: \"[3] stated\" -> better: \"Shelemay et al. [3] stated\"\n-Section 3, l.176: \"departments.There\" -> \"departments. There\"\n-Section 3, l.188: \"departemnts,\" -> \"departments,\"\n-Section 3, l.219: \"On the other hand,\" -> drop this\n-Section 5, l.345: \"musc\" -> \"music\"\n-Section 5, l.353: \"contain\" -> \"contains\"\n-References: The provided information (location, page numbers) should be consistent for papers in ISMIR proceedings.\n-References: [22] vs [23] -> Capitalization of the conference name should be consistent.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-09-computational-analysis-of",
      "title": "Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants",
      "video": "https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link"
    },
    "forum": "160",
    "id": "160",
    "pic_id": "https://drive.google.com/file/d/1VdrhCgcONuBQ_PGqf-4qf3l7isvYMLVN/view?usp=drive_link",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers' vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model's ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.",
      "abstract": "Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers' vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model's ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.",
      "author_changes": "",
      "authors": [
        "Shikarpur, Nithya Nadig*",
        " Dendukuri, Krishna Maneesha",
        " Wu, Yusong",
        " CAILLON, Antoine",
        " Huang, Cheng-Zhi Anna"
      ],
      "authors_and_affil": [
        "Nithya Nadig Shikarpur (Mila",
        " University of Montreal)*",
        " Krishna Maneesha Dendukuri (Mila)",
        " Yusong Wu (Mila, University of Montreal)",
        " Antoine CAILLON (IRCAM)",
        " Cheng-Zhi Anna Huang (Google Brain)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUDDVDZ",
      "day": "4",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "Generative Tasks -> artistically-inspired generative tasks ; MIR tasks -> music generation; Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1H-vRUco98pLP7fCkW_S2ZuMGROgIzeUn/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1NOT5skCVZv7xf9Seqp0B2fPNfRuDuCQ1/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-07-hierarchical-generative-modeling",
      "title": "HIERARCHICAL GENERATIVE MODELING OF THE MELODIC VOICE IN HINDUSTANI CLASSICAL MUSIC",
      "video": "https://drive.google.com/file/d/1U_ukptuOX3X_RU7MTrWgKKH98jkHs3zv/view?usp=drive_link"
    },
    "forum": "163",
    "id": "163",
    "pic_id": "https://drive.google.com/file/d/147BlrgUuCe8O0bEWiG_J2xLWQn7RA11G/view?usp=drive_link",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.",
      "abstract": "Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.",
      "author_changes": "We re-ran the data mining pipeline with the july dump, where the small amount of release youtube annotations are discarded. Using this new version of the dataset we re-ran the model training experiments a updated the corresponging table. We added a new table comparing the number of artists for selected datasets. All figures and tables are improved for visibility.",
      "authors": [
        "Araz, Recep Oguz*",
        " Serra, Xavier",
        " Bogdanov, Dmitry"
      ],
      "authors_and_affil": [
        "Recep Oguz Araz (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MSQ4M7",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> fingerprinting; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper proposes a new dataset for the task of version identification. The dataset is based on the Discogs music database and is substantially larger than related datasets.\n\nThe reviewers all agree that the paper and the dataset are a valuable contribution to the community. The paper is well-written and the methodology is clearly described. The reviewers also mention a few minor issues that could further strengthen the paper. \n",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZLoWu-jji5KbPxEsQtWLJIxxYGASZ919/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/11b84MPrhbqy3NfjtwJRNsJnNKlPLExfe/view?usp=drive_link",
      "review_1": "The paper presents a new dataset for the task of version identification that is significantly larger than existing datasets. The demand for datasets for this task is accurate due to the reasons presented (data volume), but mainly because of the difficulty in ensuring data quality (such as intra-click correctness) and the challenge in maintaining and disseminating the data (especially the audio associated with the tracks). Furthermore, obtaining the dataset from a source rich in metadata brought many advantages to the proposed set. Therefore, this paper is relevant.\n\nHowever, I list some points that the text could be better addressed:\n\n- While using \"official videos\" might help with the longevity of link availability, it limits the dataset to less varied versions. For example, the dataset is restricted to good recordings by professional musicians. The data will never include fan versions, such as those with specific instruments.\n- DISCOGS has information on song duration. Couldn't this information be used to filter songs contained in a playlist or videos with other elements besides the target music better, rather than cutting at 20 minutes duration?\n- The paper presenting Da-TACOS has a section devoted to analyzing the characteristics of a version (\"What is a cover song\" or something similar to this). This is a good idea that the paper introduced, as a way to explore the dataset without being limited to a straightforward exploratory data analysis. This article could include something along these lines to enrich its analysis, especially since it would be much richer than that conducted by the group that proposed Da-TACOS (since there are more tracks and more metadata). For instance, they could examine how much versions vary in genre.\n- The baseline model part seemed very disconnected from what the reader would expect to see. Instead of creating and evaluating a new architecture on different datasets, different architectures could be experimented on and applied to the proposed dataset. However, the section presented in the article resembles something a reader would expect in an article proposing a new architecture for the task as the main contribution.\n- Providing different feature sets is very relevant. However, the text does not clearly specify what these features will be (it only says \"including\" some), nor how they will be formatted and made available.",
      "review_2": "The paper introduces a new dataset for version identification that features an unprecedented scale. Part of a dataset is provided with YouTube links, and audio features are available on request. The authors then briefly introduce a NN model to demonstrate a practical use of the dataset, and compare it to state-of-the-art as much as they can.\n\nThe paper is very well written and easy to follow.\n\nOverall, the contribution of this paper is of major importance to the task of version identification. It has been showed numerous times in the literature that the high variability in the nature of music versions, the plurality of music genres and the range of clique sizes are key factors that influence the performance of VI models. Therefore, bringing a new, 1.8M versions with 330k cliques dataset is of major interest to the research community.\n\nMoreover, the authors don't just release a massive metadata database, but explain in details the steps used to produce it. This is even more valuable for future research and datasets construction.\n\nThe baseline model is presented quite thoroughly. Authors do not elaborate much on the choice of modelling and on the set parameters, but this is understandable given the context of a dataset-focused paper. The results are presented in an extensive way, and authors honestly acknowledge the limitations of their interpretation, mainly due to the difficulty to cross compare the baseline databases.\n\nI have very little complaints about the paper, that is quite clear as it is.\nHere are a few minor remarks that could make it even better in my opinion:\n- the end of section 2.1 lists a few version types that are present in the dataset: live versions, remixes, radio edits etc. I think authors should describe more these types, by providing an additional table or graph, as new types may be a valuable contribution, and will describe better the composition of the dataset.\n- section 3 describes the process applied regarding YouTube content. It might be advisable for authors to remind here that audios are not disclosed as part of the database, and content derived from this copyrighted material, namely audio features, are only granted for non commercial, research purposes. I know it is stated elsewhere, but it might be worth mentioning it here.\n- figure 3 is not easy to read, I suggest authors reorganize it. Maybe data can be represented for all 3 splits on a single, larger plot?\n- many references are missing the conference name, references should then be carefully proofread and revamped",
      "review_3": "This work introduces Discogs-VI-YT, a dataset for version identification (VI). The dataset is created by leveraging metadata from the Discogs music database and a search algorithm to programmatically identify a large corpus of music versions. The proposed dataset surpasses existing VI datasets in size and offers more comprehensive metadata. To demonstrate the efficacy of the proposed dataset, a baseline VI model is also trained.\n\nThe manuscript is well-written and easy to follow. The dataset creation process is explained in detail with reasonable design choices and well-recognized limitations. \n\nThe main weakness I found is that the discussion is limited on why the baseline trained with this large new dataset did not improve over other existing models when evaluated on the SHS100K-Test set (even for the CQTNet, which the proposed baseline model is based on). More discussion about potential causes can help readers gain more insights about the dataset, the task, or the existing models.\n\nOther minor comments:\n- Ln 187: duplicated Miles Davis\n- Ln 454: missing \u201care\u201d\n- Ln 480: missing \u201cuse\u201d\n- Author names for reference [6] are missing\n- It could be nice to place Tables and Figures at the top of the page. ",
      "session": [
        "3"
      ],
      "slack_channel": "p3-15-discogs-vi-a",
      "title": "Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata",
      "video": "https://drive.google.com/file/d/1S6tj2j1ocTlGctPUCW7SIr6CuhdifpwX/view?usp=drive_link"
    },
    "forum": "166",
    "id": "166",
    "pic_id": "https://drive.google.com/file/d/1LBJWnWvXdGPgEicEIqG9NF6PkNRNM1cl/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training. In addition, we propose a novel postprocessing step to estimate note velocities in the absence of ground truth training data.  For each track in the dataset, we provide metadata of the composer and performer, giving dates, nationality, gender and links to IMSLP or Wikipedia. We also analyse guitar-specific features of the dataset, such as the distribution of fret-string combinations and alternate tunings. This dataset has applications to various MIR tasks, including automatic music transcription, score following, performance analysis, generative music modelling and the study of expressive performance timing.",
      "abstract": "We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training. In addition, we propose a novel postprocessing step to estimate note velocities in the absence of ground truth training data.  For each track in the dataset, we provide metadata of the composer and performer, giving dates, nationality, gender and links to IMSLP or Wikipedia. We also analyse guitar-specific features of the dataset, such as the distribution of fret-string combinations and alternate tunings. This dataset has applications to various MIR tasks, including automatic music transcription, score following, performance analysis, generative music modelling and the study of expressive performance timing.",
      "author_changes": "",
      "authors": [
        "Riley, Xavier*",
        " Guo, Zixun",
        " Edwards, Andrew C",
        " Dixon, Simon"
      ],
      "authors_and_affil": [
        "Xavier Riley (C4DM)*",
        " Zixun Guo (Singapore University of Technology and Design)",
        " Andrew C Edwards (QMUL)",
        " Simon Dixon (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1JTRDG",
      "day": "2",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1_nkEU4y92n9KsoImukq-hdkQGIF5ayG1/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1jTWK_yvFwyTZ9kZq-aN5mQksmVLpCgFI/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-15-gaps-a-large",
      "title": "GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model",
      "video": "https://drive.google.com/file/d/1pXHGwN_LBlDuo_8h_IuXmJWhH_ARsDqc/view?usp=drive_link"
    },
    "forum": "171",
    "id": "171",
    "pic_id": "https://drive.google.com/file/d/1gmGyqx4ugWoGoOsRJz1AM4gAhkmHhoGR/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Efficient audio representations in a compressed continuous latent space are critical for generative audio modeling and Music Information Retrieval (MIR) tasks. However, some existing audio autoencoders have limitations, such as multi-stage training procedures, slow iterative sampling, or low reconstruction quality. We introduce Music2Latent, an audio autoencoder that overcomes these limitations by leveraging consistency models. Music2Latent encodes samples into a compressed continuous latent space in a single end-to-end training process while enabling high-fidelity single-step reconstruction. Key innovations include conditioning the consistency model on upsampled encoder outputs at all levels through cross connections, using frequency-wise self-attention to capture long-range frequency dependencies, and employing frequency-wise learned scaling to handle varying value distributions across frequencies at different noise levels.    We demonstrate that Music2Latent outperforms existing continuous audio autoencoders in sound quality and reconstruction accuracy while achieving competitive performance on downstream MIR tasks using its latent representations. To our knowledge, this represents the first successful attempt at training an end-to-end consistency autoencoder model.",
      "abstract": "Efficient audio representations in a compressed continuous latent space are critical for generative audio modeling and Music Information Retrieval (MIR) tasks. However, some existing audio autoencoders have limitations, such as multi-stage training procedures, slow iterative sampling, or low reconstruction quality. We introduce Music2Latent, an audio autoencoder that overcomes these limitations by leveraging consistency models. Music2Latent encodes samples into a compressed continuous latent space in a single end-to-end training process while enabling high-fidelity single-step reconstruction. Key innovations include conditioning the consistency model on upsampled encoder outputs at all levels through cross connections, using frequency-wise self-attention to capture long-range frequency dependencies, and employing frequency-wise learned scaling to handle varying value distributions across frequencies at different noise levels.    We demonstrate that Music2Latent outperforms existing continuous audio autoencoders in sound quality and reconstruction accuracy while achieving competitive performance on downstream MIR tasks using its latent representations. To our knowledge, this represents the first successful attempt at training an end-to-end consistency autoencoder model.",
      "author_changes": "",
      "authors": [
        "Pasini, Marco*",
        " Lattner, Stefan",
        " Fazekas, George"
      ],
      "authors_and_affil": [
        "Marco Pasini (Queen Mary University of London)*",
        " Stefan Lattner (Sony Computer Science Laboratories, Paris)",
        " George Fazekas (QMUL)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5Z46PP",
      "day": "1",
      "keywords": [
        "MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1r1mR7WET9fAjCfcgA1eg8-5ksrxxYLTO/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1kvbxJ4jaHuoGgg6G-PogYyFgXFr_H7nf/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-08-music2latent-consistency-autoencoders",
      "title": "Music2Latent: Consistency Autoencoders for Latent Audio Compression",
      "video": "https://drive.google.com/file/d/1RLeOA3JBDyvLD0YlTzjGZkkc-8bmBZ_y/view?usp=drive_link"
    },
    "forum": "172",
    "id": "172",
    "pic_id": "https://drive.google.com/file/d/14kuFBPa1kuMBRP_W8qAhKlrNS-Fyeo1y/view?usp=drive_link",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "This work introduces a data-driven approach for assigning emotions to music tracks. Consisting of two distinct phases, our framework enables the creation of synthetic emotion-labeled datasets that can serve both Music Emotion Recognition and Auto-Tagging tasks. The first phase presents a versatile method for collecting listener-generated verbal data, such as tags and playlist names, from multiple online sources on a large scale. We compiled a dataset of 5,892 tracks, each associated with textual data from four distinct sources. The second phase leverages Natural Language Processing for representing music-evoked emotions, relying solely on the data acquired during the first phase. By semantically matching user-generated text to a well-known corpus of emotion-labelled English words, we are ultimately able to represent each music track as an 8-dimensional vector that captures the emotions perceived by listeners. Our method departs from conventional labeling techniques: instead of defining emotions as generic ''mood tags'' found on social platforms, we leverage a refined psychological model drawn from Plutchik's theory, which appears more intuitive than the extensively used Valence-Arousal model.",
      "abstract": "This work introduces a data-driven approach for assigning emotions to music tracks. Consisting of two distinct phases, our framework enables the creation of synthetic emotion-labeled datasets that can serve both Music Emotion Recognition and Auto-Tagging tasks. The first phase presents a versatile method for collecting listener-generated verbal data, such as tags and playlist names, from multiple online sources on a large scale. We compiled a dataset of 5,892 tracks, each associated with textual data from four distinct sources. The second phase leverages Natural Language Processing for representing music-evoked emotions, relying solely on the data acquired during the first phase. By semantically matching user-generated text to a well-known corpus of emotion-labelled English words, we are ultimately able to represent each music track as an 8-dimensional vector that captures the emotions perceived by listeners. Our method departs from conventional labeling techniques: instead of defining emotions as generic ''mood tags'' found on social platforms, we leverage a refined psychological model drawn from Plutchik's theory, which appears more intuitive than the extensively used Valence-Arousal model.",
      "author_changes": "",
      "authors": [
        "Affolter, Joanne*",
        " Rammos, Yannis",
        " Rohrmeier, Martin A"
      ],
      "authors_and_affil": [
        "Joanne Affolter (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL))*",
        " Yannis Rammos (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGKV0TE",
      "day": "2",
      "keywords": [
        "Musical features and properties -> musical affect, emotion and mood",
        "Applications -> music recommendation and playlist generation; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR fundamentals and methodology -> web mining, and natural language processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/10-WI067fK768G9WPUCqOtAdDZRE6-Lha/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1UdtX6Cv8KESQAb8MhxvjwMR1pHaP8B0X/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-06-utilizing-listener-provided",
      "title": "Utilizing Listener-Provided Tags for Music Emotion Recognition: A Data-Driven Approach",
      "video": "https://drive.google.com/file/d/1guOhHompWVJjf1rj8xCmQ4U5mm0sY-SH/view?usp=sharing"
    },
    "forum": "173",
    "id": "173",
    "pic_id": "https://drive.google.com/file/d/1kV4-VGsTYzPqBglFwEpIF1A-uYjut1kp/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Guitar tablatures enrich the structure of music notation by assigning each note to a string and fret of a guitar in a particular tuning, defining precisely where to play the note on the instrument. The problem of generating tablature from a symbolic music representation involves inferring this string and fret assignment per note across an entire composition or performance. On the guitar, multiple string-fret assignments are possible for each pitch, which leads to a large combinatorial space that prevents exhaustive search approaches. Most modern methods use constraint-based dynamic programming approaches to minimize some cost function (e.g. hand position movement). In this work, we introduce a novel deep learning solution to symbolic guitar tablature estimation. We train an encoder-decoder Transformer model in a masked language modeling paradigm to assign notes to strings. The model is first pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on a curated set of professionally transcribed guitar performances. Given the subjective nature of assessing tablature quality, we conduct a user study amongst guitarists, wherein we ask participants to rate the playability of multiple versions of tablature for the same four-bar excerpt. The results indicate our system significantly outperforms competing algorithms.",
      "abstract": "Guitar tablatures enrich the structure of music notation by assigning each note to a string and fret of a guitar in a particular tuning, defining precisely where to play the note on the instrument. The problem of generating tablature from a symbolic music representation involves inferring this string and fret assignment per note across an entire composition or performance. On the guitar, multiple string-fret assignments are possible for each pitch, which leads to a large combinatorial space that prevents exhaustive search approaches. Most modern methods use constraint-based dynamic programming approaches to minimize some cost function (e.g. hand position movement). In this work, we introduce a novel deep learning solution to symbolic guitar tablature estimation. We train an encoder-decoder Transformer model in a masked language modeling paradigm to assign notes to strings. The model is first pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on a curated set of professionally transcribed guitar performances. Given the subjective nature of assessing tablature quality, we conduct a user study amongst guitarists, wherein we ask participants to rate the playability of multiple versions of tablature for the same four-bar excerpt. The results indicate our system significantly outperforms competing algorithms.",
      "author_changes": "1. We clarify the inference mechanism and illustrate why it reduces the asymmetry between inference and training by having higher confidence / probability values for past values input to the decoder.\n2. We clarify the notion of center note and the choice of a moving window of 11 notes in the post-processing heuristics.\n3. We add more information about the finetuning (train, test, and validation), instead of relying on the reference to Riley et al. [12]\n4. We clarified that subjects were instructed to ignore the difficulty of excerpts in the user study.\n5. We lessen the claim that the system as-is could be used as a generic arranger (regardless of the provenance of the MIDI data) and instead that it can be viewed as a guitar tablature arranging system.\n6. We describe that the BART architecture hyperparameters are not finetuned at all and are simply half the size of the BART base model.\n7. The second sentence of Section 4.3 is rewritten to be more clear.",
      "authors": [
        "Edwards, Andrew C*",
        " Riley, Xavier",
        " Sarmento, Pedro Pereira",
        " Dixon, Simon"
      ],
      "authors_and_affil": [
        "Andrew C Edwards (QMUL)*",
        " Xavier Riley (C4DM)",
        " Pedro Pereira Sarmento (Centre for Digital Music)",
        " Simon Dixon (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGL5AN8",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music",
        "MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This paper presents a deep learning method (Transformer trained with masked language modeling) for MIDI-to-tablature conversion, which achieves better performance than existing methods.\n\nWhile all three reviews and the independent meta-review liked the main idea and contributions of this work, they also pointed out a number of issues. Please read all the reviews to see the detailed comments. Specifically, the paper needs to clarify many important details including the network architecture, test data, the post-processing procedure, the metric on agreement between system output and human annotation, subjective evaluation instructions, and some analyses of experimental results. \n\nBased on the reviews and discussions, the final recommendation is \"Accept\".",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1dA3AfAQem0BFPcvtKOIvP1gPe1LLphtb/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1wNvHcqpvSlqibYDq1Ww4DIltYIjRvpXW/view?usp=drive_link",
      "review_1": "This article adresses the task of assigning realistic guitar string/fret positions to the notes of a MIDI content, which is here judiciously simplified as a string assignment problem. This task is well known in the community, sometimes under the term of \"fingering\" problem. The probem is here addressed with a Transformer, trained with a masking process on the string information.\n\nThe paper is well written and clearly structured. The approach is promising, but some points are still to be clarified, in particular in the evaluation.\n\nRegarding the title: because \"Tablature\" commonly refers to a particular type of musical score, the reader might wrongly think that the term \"tablature inference\" refers to \"tablature generation\" (analogous to \"score generation\"). It might be good therefore to clarify the title.\n\nThe quintile approach judiciously allows to provide some context regarding the data that are being labelled by the model. It although raises the question of the inference of the first ten tokens, which can not benefit from any past context. How does the model handle this ? It seems important as it will arguably impact the predictions for the following quantiles and so on.\n\nIn section 4.4, it would be more clear to state, within the third sentence, the proportion of unplayable predictions (I believe that it is 0,53%, as said in the end of the section, but I think the reader will wonder that number as soon as the problem is stated). Although the introduction of 4.4 is clear, the description of the algorithm lacks details. Are we talking about simultaneous notes ? or notes in a 10-notes window ? In 3.b, how could the center note have a fret value higher than MAX_DEVIATION (which is a fret interval, not a fret number) ?\n\nThe number of tab in the fine-tuning set should be indicated.\n\nI don't understand why the evaluation is on the 9 tablatures only. The (large) size of the datasets used in this work arguably enables a much larger evaluation.\n\nIt is great to compare the results of the proposed approach with functionnalities of existing (and well-used) software, but I regret the lack of comparison with SOTA approaches that have been published, although not necessary implemented in any software. I suspect that GuitarPro, MuseScore and TuxGuitar might not have considered the performance of this functionality as a priority (given that these software are primarily thought for score/tab visualisation/playback/writing rather than for tab midi-to-tab transcription) and might be far from sota, and rather be an \"easy\" baseline.\n\nI don't understand why a mistake like Fig.5 (right) could occur given that the post-processing algo (4.4) is supposed to avoid unplayable content. To disambiguate, it could be interesting to illustrate unplayability (of Section 4.4) with a concrete example.\n\nThe network architecture is poorly described (2 lines). Why is the hidden size 384-dimensional ? What is the input dimension ? I guess it must be the size of the token alphabet, so what is this size ?\n\nThe second sentence of Section 4.3 seems of strong importance, but it is hardly understandable. I think that the pedagogy of the paper could be improved by illustrating the BART architecture for the reader unfamiliar with the original publication.",
      "review_2": "This paper is a nice contribution to the field of automatic transcription to tablature. It summarises prior art, uses a sensible corpus and evaluates in good ways. \nMore explicit reflection on the problems of representation might be nice here - without temporal information ro representations of simultaneity in the input (am I understanding that correctly?), it's not that surprising that physically awkward or impossible results would come out. The example in figure 5 (along with the graph in figure 4) shows this well -- each decision taken individually seems sensible, except for the chords. This is especially true if the sequence of input notes had the B2 as the second note. Where the order of notes in a chord affects the result, that's a suggestion that the model is sub optimal...",
      "review_3": "The paper presents a simple solution for guitar tablature generation from MIDI in which an encoder-decoder Transformer is trained using a masked language modeling supervision scheme (masking string tokens) to assign notes to strings. The inference is done in an autoregressive fashion and some post-processing heuristics are applied to the output of the model. The model is trained on a large dataset of crowdsourced tablatures and then fine-tuned on a small set of professionally transcribed performances. \n\nIt is not clear why the tokenization is deemed as novel in the introduction section of the paper. I understand that the model uses an existing tokenizer (MidiTok) and that the string is encoded with the track token. I encourage the authors to provide more information on why the tokenization is novel. \n\nExperiments compare the proposed model with a commercial system and two open-source software implementations capable of producing tablatures from score or MIDI. Given the challenges involved in assessing the quality of tablatures, the experiments include a user study. The paper provides an interesting discussion of the results and limitations of the proposed approach and suggests various ideas for future research. \n\nThe manuscript is well-written and organized, and overall, the paper makes a good contribution to ISMIR. \n\nThe supplementary material includes a subset of the codebase, but the manuscript does not indicate whether the code or models will be available. It would be very important to clarify whether the code and model will be available for reproducibility. \n\nMinor corrections\n\nLine 43 - References to existing publications should be added here. \n\nLine 56 - Please reconsider the novelty of the tokenization.\n\nSection 5.1 - There is no reference in the text to Figure 4. Please link the figure to the text describing the evaluatiuon on the strech across the chords.\n",
      "session": [
        "2"
      ],
      "slack_channel": "p2-11-midi-to-tab",
      "title": "MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling",
      "video": "https://drive.google.com/file/d/1eKOb0fTCyui8Hwi6WBoAsZZqbSC-2g5G/view?usp=drive_link"
    },
    "forum": "175",
    "id": "175",
    "pic_id": "https://drive.google.com/file/d/1_0gL0ENfndsCMpHoxvaVD8r6lmOOz4xA/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This study aims to measure the similarity of melodies objectively using natural language processing (NLP) techniques. We utilize Mel2word which is a melody tokenization method based on byte-pair encoding to facilitate the semantic analysis of melodies. In addition, we apply two word weighting methods: the modified Tversky measure for word salience and the TF-IDF method for word importance and uniqueness, to better understand the characteristics of each melodic element. We validate our approach by comparing song vectors calculated from an average of Mel2Word vectors to the ground truth in 108 cases of music copyright infringement, sourced from an extensive review of legal documents from law archives. The results demonstrate that the proposed approach is more in accordance with court rulings and perceptual similarity.",
      "abstract": "This study aims to measure the similarity of melodies objectively using natural language processing (NLP) techniques. We utilize Mel2word which is a melody tokenization method based on byte-pair encoding to facilitate the semantic analysis of melodies. In addition, we apply two word weighting methods: the modified Tversky measure for word salience and the TF-IDF method for word importance and uniqueness, to better understand the characteristics of each melodic element. We validate our approach by comparing song vectors calculated from an average of Mel2Word vectors to the ground truth in 108 cases of music copyright infringement, sourced from an extensive review of legal documents from law archives. The results demonstrate that the proposed approach is more in accordance with court rulings and perceptual similarity.",
      "author_changes": "",
      "authors": [
        "Park, Saebyul*",
        " Kim, Halla",
        " Jung, Jiye",
        " Park, Juyong",
        " Kim, Jeounghoon",
        " Nam, Juhan"
      ],
      "authors_and_affil": [
        "Saebyul Park (KAIST)*",
        " Halla Kim (KAIST)",
        " Jiye Jung (Heinrich Heine University D\u00fcsseldorf)",
        " Juyong Park (KAIST)",
        " Jeounghoon Kim (KAIST)",
        " Juhan Nam (KAIST)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCU8PJS",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> melody and motives",
        "MIR tasks -> similarity metrics"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1h2RYen_J2xlE_zcIMKar6f6grNUAsP2C/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1KUZ1jwX_Upxe_npkfZz_2_URZ-AnXUtP/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-15-quantitative-analysis-of",
      "title": "Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases",
      "video": "https://drive.google.com/file/d/1BEklT7FMPCCtK9rbU55EzcjJoXVm3Zd-/view?usp=drive_link"
    },
    "forum": "181",
    "id": "181",
    "pic_id": "https://drive.google.com/file/d/1EQoIOAOjkXA20GzKP0ScroOHIFp75ZmN/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.",
      "abstract": "The pursuit of replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. However, the potential for a one-to-many emulation strategy remains an avenue yet to be explored. This paper presents such an attempt that utilizes conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder designed to distill and encode the distinctive style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrieval-based embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling,  contributing a foundational step towards zero-shot audio modeling applications.",
      "author_changes": "We would like to thank our reviewers for their valuable feedback. Their comments have helped us further improve and strengthen our work.\nIn the camera-ready version of the paper, we made several updates to enhance clarity and improve the presentation of our results. We directly indicated the name of the collaboration company Positive Grid in sections 3.1, 3.2, and 3.4, and we added an acknowledgements section to clearly mention the collaboration. We included a citation regarding other zero-shot FX style transfer tasks that do not model effects on amplifiers.\nWe noted that the performance of tone embedding is expected to be influenced by the number of amplifiers used in training, and we plan to address this in future work by adding more amplifiers to the training set to facilitate a better understanding of tone transformation in the generator.\nRegarding the computational efficiency of our method, we mentioned the number of parameters in our generator and identified the development of a plugin version as future work. In real-world guitar effect plugins, users may not have access to GPU resources, so writing a plugin version and testing its computational efficiency or cost is considered more feasible.",
      "authors": [
        "Chen, Yu-Hua*",
        " Yeh, Yen-Tung",
        " Cheng , Yuan-Chiao",
        " Wu, Jui-Te",
        " Ho,  Yu-Hsiang",
        " Jang, Jyh-Shing Roger",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Yu-Hua Chen (NTU)*",
        " Yen-Tung Yeh (National Taiwan University)",
        " Yuan-Chiao Cheng  (Positive Grid)",
        " Jui-Te Wu (Positive Grid)",
        "  Yu-Hsiang Ho (Positive Grid )",
        " Jyh-Shing Roger Jang (National Taiwan University)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MTPMMF",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        "Applications -> music composition, performance, and production; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "meta_review": "Summary\n\nIn this work, the authors present a new method to perform zero-shot amplifier/tone transfer from one recording to another. Compared to past work, the method focuses on the amplifier/tone application and the idea of having a single network simultaneously model multiple amplified devices instead of a single device with multiple parameters. In this case, all devices are various forms of tone/amplifier fx. To train a style transfer model, a tone embedding module is learned via contrastive learning. This embedding module is then used to condition a second module to process audio. Evaluation is done via comparing a few within method configurations as well as comparing against a search-based method. The search-based method, however, is only used for testing out-of-domain tones.\n\n\nInitial Reviews\n\n\nOverall, the initial reviews for this work are mostly positive with 2x weak accept, 1 strong accept, and weak reject (meta). Positive points for the work include\n\u2022\tR2 \u201c well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.\u201d\n\u2022\tR2 \u201cwell organized and the proposed method is described in sufficient detail.\u201d\n\u2022\tR3 \u201cPersonally, I appreciated the approach of this work. \u201c\n\u2022\tR4 \u201ca strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. \u201c\n\u2022\t\n\nAreas for improvement include\n\u2022\tR2 \u201cdiscussion on similarities and differences with related work would help to place the proposed method within the context of existing work.\u201d\n\u2022\tR2 \u201cevaluation may be lacking to fully understand the efficacy of the proposed method\u201d\n\u2022\tR2 \u201cno discussion on the computational efficiency of this method. Does this model enable real-time processing\u201d\n\u2022\tR3 \u201c the architecture used for the encoder \"3.5 Implementation Details\" is unclear\u2026 t of guesswork to anyone trying to reproduce or compare to this approach.. \u201c\n\u2022\tR4 \u201cThe authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder\u201d\n\u2022\tSee also the initial meta review.\n\n\nDiscussion\n\nDuring the discussion, there were some comments on issues that could cause a reject, but multiple reviewers commented seeking to hold their positive results to champion the current scores. \n\n\nRecommendation\n\nDue to the initial reviews and discussion of reviewers seeking to champion the work, we recommend to accept. Please see several issues in the initial meta review to address as well.\n",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1LB9waLGDAqxl6v7OeYOtljs-yG4VwW0B/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1nL1IHXeDaJOGw2_R-N-6JIyCDSoW0pjs/view?usp=drive_link",
      "review_1": "Summary\n\nIn this work, the authors propose a method for zero-shot guitar amplifier modeling. They achieve this by first training an encoder in a contrastive pre-task to extract features related to audio effect style or \u201ctone\u201d in the context of guitar amplifiers. They then produce a tone embedding from a reference signal, which is used as conditioning to a neural audio effect modeling network, called the generator. This generator is trained with a dataset of paired examples and a reconstruction loss. The convolutional model will process a clean signal to produce an output that has the same tone as the reference signal. \n\nStrengths\n\nThe proposed method is well motivated and effectively builds on previous work to enable zero-shot audio effect style transfer in the context of guitar amplifier modeling.\n\nThe manuscript is well organized and the proposed method is described in sufficient detail. \n\nWeaknesses\n\nFurther discussion on similarities and differences with related work would help to place the proposed method within the context of existing work.\n\nThe presented evaluation may be lacking to fully understand the efficacy of the proposed method. The first experiment demonstrates that ToneEmb conditioning can enable. While a listening test is not strictly required, even a simple perceptual study would significantly strengthen the conclusions of the work. \n\nWhile the authors do provide a discussion on some potential limitations of their work, they fail to address some important aspects. For example, there is no discussion on the computational efficiency of this method. Does this model enable real-time processing? This is critical for guitar amplifier modeling applications. \n\nThe authors do not mention if they will provide open source code or datasets. While this is not strictly required per ISMIR guidelines, it would further strengthen the work.\n\nQuestions\n\nThe claim of \u201cunpaired references\u201d used to train the generator may be problematic. While the underlying content of the reference and the input (clean) may be differing, these training examples are \u201cpaired\u201d in the sense that the data must be synthetically generated such that the guitar amplifier configuration is identical between the two recordings. In other contexts, \u201cunpaired\u201d data generally means \n\nRecommendation \n\nThe proposed method is novel in that it combines two existing methods to enable a new task of zero-shot guitar amplifier modeling. While the presented evaluation demonstrates the efficacy of some aspects of the proposed method, the evaluation could be stronger, and hence limits the potential strength of the conclusions and overall generalization. As a result, this work is recommended for a weak acceptance. The authors are encouraged to strengthen the work through more rigorous evaluation which could include a perceptual study, the inclusion of more zero-shot baselines and a more detailed case study. ",
      "review_2": "In this work, a tone embedding is developed using contrastive training. This embedding is used to train a decoder architecture that conditions on the tone embedding. The use of the embedding allows simultaneous modeling of different amp types within a single model.\n\nTwo leading conditioning strategies, concatenation and FiLM, are compared. The work is evaluated using both seen and unseen amplifiers. The authors identify current weaknesses in the system output, that the system fails to generate high-frequency components.\n\nAs pointed out by another reviewer, the architecture used for the encoder \"3.5 Implementation Details\" is unclear. This would give a lot of guesswork to anyone trying to reproduce or compare to this approach.\n\nOne point on Figure 4: the diagram suggested to me that the embedding failed to distinguish many different types of tones-as I see only two big clusters and quite a bit of overlap. That t-SNE fails to separate the amps makes me wonder how separable they are in the embedding space. Therefore, I wonder if the diversity of the modeled tones is low, if the embedding fails to distinguish between some tones, or both.\n\nPersonally, I appreciated the approach of this work. For future work, I would love to hear this approach applied to a larger range of production styles/effects, not just amplifier simulations.",
      "review_3": "This work addresses neural modelling of guitar amplifiers in the zero-shot setting. That is, it seeks to find a model that can generalise to amplifiers unseen at training time using a conditioning mechanism. This is achieved using a combination of a relatively standard GCN model for audio effect modeling, and an encoder trained with a contrastive (SimCLR) objective, where different pieces of input audio with the same processing constitute positive pairs.\n\nIn many ways this piece of work is overdue \u2014 neural modeling of amplifiers and distortion circuits has been predominantly focused to date on fitting a single device, with more complex models employing conditioning mechanisms to account for different effect parameters. Generalising to multiple different devices is more challenging, however, owing to the variety of different designs and hence behaviours, but is well suited to the zero-shot task formulation proposed here. This is, all round, a strong piece of research with a clearly motivated model design, sufficient evaluation, and a generally high quality of presentation. Whilst not a complete solution to the stated problem, this is a clear step towards it and the paper both acknowledges the current limitations and proposes viable directions for future work.\n\nThe authors appear to have chosen not to reveal which architecture they have used for the tone embedding encoder, describing it instead simply as a \u201ccontemporary audio encoder\u201d. Given the industrial collaboration they report, I assume this is simply an IP issue. If this is the case, however, it should be directly and clearly acknowledged in the text, rather than ambiguously omitting certain details. I hope the authors will make this change in the camera-ready version.\n\nOtherwise, given the clear merits of this paper I am very happy to recommend this work for acceptance.",
      "session": [
        "3"
      ],
      "slack_channel": "p3-11-towards-zero-shot",
      "title": "Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control",
      "video": "https://drive.google.com/file/d/1ylPmxWdUuuKhEQn34Y79rLP3ET2SUQcr/view?usp=drive_link"
    },
    "forum": "184",
    "id": "184",
    "pic_id": "https://drive.google.com/file/d/1NP9o0FXcdpRSEbAPJP8hIPj-kIrqvLl4/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the \"inner workings\" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.",
      "abstract": "Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the \"inner workings\" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.",
      "author_changes": "",
      "authors": [
        "Wei, Megan*",
        " Freeman, Michael",
        " Donahue, Chris",
        " Sun, Chen"
      ],
      "authors_and_affil": [
        "Megan Wei (Brown University)*",
        " Michael Freeman (Brown University)",
        " Chris Donahue (Carnegie Mellon University)",
        " Chen Sun (Brown University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F62P9V",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> representations of music; Musical features and properties -> rhythm, beat, tempo",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1R2hwTNW-EQT5xNODQKtDdxot3yFrTpAE/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1MATeD7zabPqC5O0vgoKabKJZjuKhOpHH/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-03-do-music-generation",
      "title": "Do Music Generation Models Encode Music Theory?",
      "video": "https://drive.google.com/file/d/1kB-DGyxmkJIC_aZ1oqnbhqQBlyjlZPGm/view?usp=drive_link"
    },
    "forum": "189",
    "id": "189",
    "pic_id": "https://drive.google.com/file/d/16sNrWnQaBXKcd2X3bjhM0XRUHzA0FA8w/view?usp=sharing",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the model first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). In this paper, we focus on the MUSCIMA++ v2.0 dataset. It represents musical notation as a graph, where the pairwise relationships among detected music objects are predicted. Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stage in a more holistic way. These findings are an important step toward a more complete OMR solution.",
      "abstract": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the model first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). In this paper, we focus on the MUSCIMA++ v2.0 dataset. It represents musical notation as a graph, where the pairwise relationships among detected music objects are predicted. Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stage in a more holistic way. These findings are an important step toward a more complete OMR solution.",
      "author_changes": "",
      "authors": [
        "Yang, Guang*",
        " Zhang, Muru",
        " Qiu, Lin",
        " Wan, Yanming",
        " Smith, Noah A"
      ],
      "authors_and_affil": [
        "Guang Yang (University of Washington)*",
        " Muru Zhang (University of Washington)",
        " Lin Qiu (University of Washington)",
        " Yanming Wan (University of Washington)",
        " Noah A Smith (University of Washington and Allen Institute for AI)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM5ZULS1",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ymF8sx8o5Krxh8EGMA2UwmhRX0ov0YkR/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1S0-dUNVs10dALNj86ORBmU5EaH9IV-79/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-15-toward-a-more",
      "title": "Toward a More Complete OMR Solution",
      "video": "https://drive.google.com/file/d/161aBb4xSrTI7akF0_bTI2SPwTzI4zunK/view?usp=drive_link"
    },
    "forum": "193",
    "id": "193",
    "pic_id": "https://drive.google.com/file/d/1py7A8U7NlmRV53-cKnyykr626eHooiwL/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",
      "abstract": "Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",
      "author_changes": "We updated the title to use colon \":\" instead of a dash \"-\".\nAbstract was updated to help save space.\nAcknowledgement was added.\n\nWe addressed reviewers\u2019 comments, in specific:\nWe updated our evaluation section by running an extended listening study that included both human and AI-annotated captions to show the similarity between these two. Both general audience and music experts were invited to participate. The section is followed by a discussion on the results and shortcoming of our work, illustrating possibilities for future work.\nWe also corrected a couple of typos and added an explanation on what is meant by \u201cin-context learning\u201d.",
      "authors": [
        "Melechovsky, Jan",
        " Roy, Abhinaba*",
        " Herremans, Dorien"
      ],
      "authors_and_affil": [
        "Jan Melechovsky (Singapore University of Technology and Design)",
        " Abhinaba Roy (SUTD)*",
        " Dorien Herremans (Singapore University of Technology and Design)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUEUDV1",
      "day": "3",
      "keywords": [
        "Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> symbolic music processing; MIR fundamentals and methodology -> web mining, and natural language processing; Musical features and properties -> representations of music",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The paper was commended for the new task to be introduced and on the good presentation and writing. There were specific comments on the motivation, the need for additional evaluations, and a lack of clarity on the features used. Although there are some minor comments on the manuscript itself, these can be addressed in the camera-ready version, and the manuscript can be recommended for acceptance.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/18ltPsN6Gp-anmdtvXVkfsuFWG-tTAvmF/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/16b061WIusDZCvVrtgezh5AiyCE9ElkdT/view?usp=drive_link",
      "review_1": "Overall, the paper is well-written, and the topic itself is quite new, so I think this dataset can be used in future researches well. However, an illustration of the details about the used features is limited and especially the evaluation of the pseudo-generated MIDI captioning dataset is also very limited. \n\nListening study: looking at the 5 audio and caption examples in the web demo page, the quality seems decent. However, the listening study reported in the paper doesn't give much information about the quality of the captioning dataset.\n\nIt would be helpful for the readers if the authors can provide detailed explanation about the used feature extraction algorithms. In this paper, the characteristics about the each of feature extraction algorithms plays critical role in the pseudo generated MIDI captions, however, in the paper, what the authors reported regarding the used feature extraction algorithms were only about which library they have used. If the authors can report the 1) performance of the each feature extraction algorithm, 2) the category of the output fields (e.g. if it's tempo extraction algorithm, then what would be the output of this algorithm, it can be numbers or some text. I can see it from result table, but it is necessary to illustrate it as detail as possible), and some more information, these would be really beneficial for readers who would want to work on pseudo MIDI captioning or other related tasks.\n\nI also think the author can add small paragraph explaining what is the in-context learning for the readers. \n\nAnd, if the author can provide some possible list of applications using this dataset (like text-to-MIDI generation, MIDI-to-text captioning task, etc), it would be also nice for the readers. \n\nAlso, if the authors can pick one of those tasks and show baseline result, it would compensate the lack of the thorough listening study. However, I understand that it cannot be easily added during reviewing phase. \n\nIn conclusion, I think enhancing 1) details about the feature extraction algorithm, 2) listening study, and 3) providing possible tasks using this dataset are necessary to be published as a good paper. Currently, the dataset is novel, but the insights the readers can get from this paper is very limited.",
      "review_2": "I must confess that after my first reading, I was very sceptical about this approach of using traditional MIR algorithms to generate some annotation that then a LLM would convert into text captioning. Given the fact that the long-term motivation of the dataset introduced in this paper is to enable a different field of prompts-to-music generative models, especially symbolic music generation, I was expecting an approach that would be more human-centred for captioning the data. One idea could have been to use one of many sources of data available across various websites where people comment and describe how they feel about a particular music. This allowed among other approaches the text prompt-to-music audio field to achieve good results in the past years, especially when targeting a broad human audience. However, choosing the field of symbolic music generation somehow already implies that the target is not a random human, but more especially a musician, and therefore it is indeed legitimate to approach the captioning of music with semantic descriptors that someone skilled in music theory would use.\nIt is then indeed logical then to rely on the state-of-the-art MIR algorithms to extract such descriptors and then make use of LLMs to generate such caption. However, it would have been useful to discuss a little bit more the limitation of this approach, especially in opposition to a real-world musician captioning; and a small experiment could also have been useful to justify further the usefulness of generated captioning vs human captioning.\n\nThe pipeline for the generation of the dataset is clearly explained, with details on every different annotation extracted from the MIDI files of the Lakh Dataset. It would have been interesting to deliver the scripts implementing this pipeline on the repository but is not mandatory.\n\nThe two listening study that were conducted to validate the hypothesis that this pipeline would generate caption accurate to musicians are interesting. It would be probably pertinent to detail this section further, especially explaining why the PsyToolkit allows an accurate evaluation of the quality of the generated captioning. Also, again on the topic of comparing generated captioning with human hand-made captioning, it would have been useful to introduce a few human caption examples and compare if the listening results shows significant differences between those two groups.\n\nFinally, it would have been interesting to making use of the dataset applied on a simple experiment. It is understood that this is probably the next step of the research, but this would have helped to justify further the usefulness of this dataset.\n\nIn bref;\n\nStrength: First Dataset of MIDI Files Text Captions, Detailed and Reusable Pipeline, Fairly Musically Accurate Captions, Validated with Listening Study\n\nWeakness: No Experiment to demonstrate the Applications of the Dataset, Not enough discussion of Generated Captions vs Human Caption, No Discussion on Future Directions for Improvements of the Captioning, Not enough Discussion on the weaknesses of the Approach",
      "review_3": "The paper is well-written and the motivation is clear, but the way they create the dataset isn't that novel since they mostly use existing models and tools. But we shouldn't expect a dataset paper to be super innovative.\nWhat's great is that the MidiCaps dataset is really big and the automatically generated captions are pretty good quality. Putting together such a large dataset of MIDI files with matching text is valuable in itself, because it allows for new kinds of research combining symbolic music and language processing.\nThus, I think this paper should be accepted.\nTypos: Section 3.2: tweek should be tweak",
      "session": [
        "6"
      ],
      "slack_channel": "p6-05-midicaps-a-large",
      "title": "MidiCaps: A Large-scale MIDI Dataset with Text Captions",
      "video": "https://drive.google.com/file/d/1qIrcmvqJRIO-Ov3kpzGRrxay8Mh_jzom/view?usp=drive_link"
    },
    "forum": "198",
    "id": "198",
    "pic_id": "https://drive.google.com/file/d/1oM1Zo-J2dI_yHQeh3Op6J7Bc3eC4oS6W/view?usp=drive_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Human music annotation is one of the most important tasks in music information retrieval (MIR) research. Results of labeling, tagging, assessment, and evaluation can be used as training data for machine learning models that estimate them automatically. For such machine learning purposes, a single target (e.g., song) is usually annotated by multiple human annotators, and the results are aggregated by majority voting or averaging. Majority voting, however, requires the number of annotators to be an odd number, which is not always possible. And averaging is sensitive to differences in the judgmental characteristics of each annotator and cannot be used for ordinal scales. This paper therefore proposes that the item response theory (IRT) be used to aggregate the music annotation results of multiple annotators. IRT-based models can jointly estimate annotators' characteristics and latent scores (i.e., aggregations of annotation results) of the targets, and they are also applicable to ordinal scales. We evaluated the IRT-based models in two actual cases of music annotation --- semantic tagging of music and Likert scale-based evaluation of singing skill --- and compared those models with their simplified models that do not consider the characteristics of each annotator.",
      "abstract": "Human music annotation is one of the most important tasks in music information retrieval (MIR) research. Results of labeling, tagging, assessment, and evaluation can be used as training data for machine learning models that estimate them automatically. For such machine learning purposes, a single target (e.g., song) is usually annotated by multiple human annotators, and the results are aggregated by majority voting or averaging. Majority voting, however, requires the number of annotators to be an odd number, which is not always possible. And averaging is sensitive to differences in the judgmental characteristics of each annotator and cannot be used for ordinal scales. This paper therefore proposes that the item response theory (IRT) be used to aggregate the music annotation results of multiple annotators. IRT-based models can jointly estimate annotators' characteristics and latent scores (i.e., aggregations of annotation results) of the targets, and they are also applicable to ordinal scales. We evaluated the IRT-based models in two actual cases of music annotation --- semantic tagging of music and Likert scale-based evaluation of singing skill --- and compared those models with their simplified models that do not consider the characteristics of each annotator.",
      "author_changes": "",
      "authors": [
        "Nakano, Tomoyasu*",
        " Goto, Masataka"
      ],
      "authors_and_affil": [
        "Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCURWMC",
      "day": "4",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1BVN3Qgw_tfZC3iUYrjK2mBdM4exBcZhl/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1s7Hc5Jmx_LyU5wfSknSFiNyXYGg6xxgT/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-15-using-item-response",
      "title": "Using Item Response Theory to Aggregate Music Annotation Results of Multiple Annotators",
      "video": "https://drive.google.com/file/d/17c0w9DoxLbJZUjA6tUriZUgnE54eOgqb/view?usp=drive_link"
    },
    "forum": "205",
    "id": "205",
    "pic_id": "https://drive.google.com/file/d/14BG6zTeWW-X8EG95y9GnlrTbIKM-esJo/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting templates and the music-theoretical relevance of the identified repetitions.",
      "abstract": "Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting templates and the music-theoretical relevance of the identified repetitions.",
      "author_changes": "",
      "authors": [
        "Ren, Zeng*",
        " Rammos, Yannis",
        " Rohrmeier, Martin A"
      ],
      "authors_and_affil": [
        "Zeng Ren (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)*",
        " Yannis Rammos (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUF5AKD",
      "day": "1",
      "keywords": [
        "Computational musicology -> mathematical music theory; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> representations of music; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> structure, segmentation, and form",
        "Computational musicology"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1mL_I29Y9OXfstFXm0C2EVB8xozviBLhL/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1g-xHPS7iaP2x5Gbq3DlGXcaiNs5NAXRC/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-01-formal-modeling-of",
      "title": "Formal Modeling of Structural Repetition using Tree Compression",
      "video": "https://drive.google.com/file/d/1bXs3DMiL--96DfbHYoJfdmp4cQe4bIEr/view?usp=drive_link"
    },
    "forum": "207",
    "id": "207",
    "pic_id": "https://drive.google.com/file/d/14P2-03D3SKJsWef0kto2uDsHpqCh4-3h/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The meteoric surge of AI-generated music has prompted significant concerns among artists and publishers alike. Some fear that the adoption of AI is poised to result in massive job destruction; others sense it will jeopardize and eventually upend all legal frameworks of intellectual property. AI, however, is not the first instance where humanity has confronted the prospect of machines emulating musical creativity. Already in the Baroque, various modes of musical artificiality were explored, ranging from automata and organ stops mimicking human performance and natural sounds, up to devices for mechanized composition (e.g., Athanasius Kircher, Johann Philip Kirnberger, C.P.E. Bach, Antonio Calegari and Diederich Nickolaus Winkel).   Valuable insights emerge from the reconsideration\u2014and digital implementation\u2014of these curiosities through the lens of present-day generative models. It can be argued that the very notion of \u2018artificiality\u2019 has presented humanity with long-standing philosophical dilemmas, in addressing the debate on the role of art as a substitute of (divine) nature. By digitally implementing and formalizing some pioneering instances of  algorithmically-generated music we wish to illustrate how mechanical devices have played a role in human art and entertainment prior to our digital era.",
      "abstract": "The meteoric surge of AI-generated music has prompted significant concerns among artists and publishers alike. Some fear that the adoption of AI is poised to result in massive job destruction; others sense it will jeopardize and eventually upend all legal frameworks of intellectual property. AI, however, is not the first instance where humanity has confronted the prospect of machines emulating musical creativity. Already in the Baroque, various modes of musical artificiality were explored, ranging from automata and organ stops mimicking human performance and natural sounds, up to devices for mechanized composition (e.g., Athanasius Kircher, Johann Philip Kirnberger, C.P.E. Bach, Antonio Calegari and Diederich Nickolaus Winkel).   Valuable insights emerge from the reconsideration\u2014and digital implementation\u2014of these curiosities through the lens of present-day generative models. It can be argued that the very notion of \u2018artificiality\u2019 has presented humanity with long-standing philosophical dilemmas, in addressing the debate on the role of art as a substitute of (divine) nature. By digitally implementing and formalizing some pioneering instances of  algorithmically-generated music we wish to illustrate how mechanical devices have played a role in human art and entertainment prior to our digital era.",
      "author_changes": "",
      "authors": [
        "Cornia, Nicholas*",
        " Forment, Bruno"
      ],
      "authors_and_affil": [
        "Nicholas Cornia (Orpheus Instituut)*",
        " Bruno Forment (Orpheus Instituut)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHELJVTQ",
      "day": "2",
      "keywords": [
        "Creativity -> computational creativity; Creativity -> humanistic discussions; MIR tasks -> optical music recognition; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies; Philosophical and ethical discussions -> legal and societal aspects of MIR",
        "Philosophical and ethical discussions -> philosophical and methodological foundations"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1Wd35AEbNSSAsSknVhi-6OXNFaTHJdnKa/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1tShkawH3_Fc0_5EAGZp7ZfCG9nfmnRfO/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-16-who-s-afraid",
      "title": "WHO'S AFRAID OF THE `ARTYFYSHALL BYRD'? HISTORICAL NOTIONS AND CURRENT CHALLENGES OF MUSICAL ARTIFICIALITY",
      "video": "https://drive.google.com/file/d/1oN4xp6Z0jJeApFTE6fruD1CBM_2IjaBQ/view?usp=drive_link"
    },
    "forum": "212",
    "id": "212",
    "pic_id": "https://drive.google.com/file/d/1OSpNzx0I9AaqIsnNCK-qLTBMmtpAhHGQ/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",
      "abstract": "Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",
      "author_changes": "",
      "authors": [
        "Strahl, Sebastian*",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Sebastian Strahl (International Audio Laboratories Erlangen)*",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1LBAUA",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1CHhf2YqFLE4yhviOEnoiWPkquE-MqLBy/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1qYZX0b77rH_vb-cNHkkztkV7ggpFtA3e/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-16-semi-supervised-piano",
      "title": "Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques",
      "video": "https://drive.google.com/file/d/1wLdS2VgE1tyDmEQWvyI0OaVOt-ZAJvL3/view?usp=drive_link"
    },
    "forum": "214",
    "id": "214",
    "pic_id": "https://drive.google.com/file/d/1hvjjP5pOg0rHA1HL7ua4uD2K767PJ1_m/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",
      "abstract": "Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",
      "author_changes": "",
      "authors": [
        "Batlle-Roca, Roser*",
        " Liao, Wei-Hsiang",
        " Serra, Xavier",
        " Mitsufuji, Yuki",
        " Gomez, Emilia"
      ],
      "authors_and_affil": [
        "Roser Batlle-Roca (Universitat Pompeu Fabra)*",
        " Wei-Hsiang Liao (Sony Group Corporation)",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Yuki Mitsufuji (Sony AI)",
        " Emilia Gomez (Joint Research Centre, European Commission & Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCV5PLN",
      "day": "4",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics; Generative Tasks -> evaluation metrics; MIR tasks -> music generation; MIR tasks -> similarity metrics",
        "Evaluation, datasets, and reproducibility -> evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1VxJq1lT9F1gJ64Cvb10en0WeVNUeV0hk/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1WbKAV-UY56QfYuDJhsB4Vzm6A7oUvrp9/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-05-towards-assessing-data",
      "title": "Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio",
      "video": "https://drive.google.com/file/d/1TWr7oLMJb2Q7ENmhfPF6rpZeDhwSDqMI/view?usp=sharing"
    },
    "forum": "216",
    "id": "216",
    "pic_id": "https://drive.google.com/file/d/1yVUzCS6SITQJSFhIqk1PjpCsML0cAtko/view?usp=sharing",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The speech domain prevails in the spotlight for several natural language processing (NLP) tasks while the singing domain remains less explored. The culmination of NLP is the speech-to-speech translation (S2ST) task, referring to translation and synthesis of human speech. A disparity between S2ST and the possible adaptation to the singing domain, which we describe as singing-voice to singing-voice translation (SV2SVT), is becoming prominent as the former is progressing ever faster, while the latter is at a standstill. Singing-voice synthesis systems are overcoming the barrier of multi-lingual synthesis, despite limited attention has been paid to multi-lingual songwriting and song translation. This paper endeavors to determine what is required for successful SV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT, performing lyrics translation from English to Japanese. A cascaded approach is proposed to establish a framework with a high degree of control which can potentially diminish the disparity between SV2SVT and S2ST. The performance of PolySinger is evaluated by a mean opinion score test with native Japanese speakers. Results and in-depth discussions with test subjects suggest a solid foundation for SV2SVT, but several shortcomings must be overcome, which are discussed for the future of SV2SVT.",
      "abstract": "The speech domain prevails in the spotlight for several natural language processing (NLP) tasks while the singing domain remains less explored. The culmination of NLP is the speech-to-speech translation (S2ST) task, referring to translation and synthesis of human speech. A disparity between S2ST and the possible adaptation to the singing domain, which we describe as singing-voice to singing-voice translation (SV2SVT), is becoming prominent as the former is progressing ever faster, while the latter is at a standstill. Singing-voice synthesis systems are overcoming the barrier of multi-lingual synthesis, despite limited attention has been paid to multi-lingual songwriting and song translation. This paper endeavors to determine what is required for successful SV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT, performing lyrics translation from English to Japanese. A cascaded approach is proposed to establish a framework with a high degree of control which can potentially diminish the disparity between SV2SVT and S2ST. The performance of PolySinger is evaluated by a mean opinion score test with native Japanese speakers. Results and in-depth discussions with test subjects suggest a solid foundation for SV2SVT, but several shortcomings must be overcome, which are discussed for the future of SV2SVT.",
      "author_changes": "",
      "authors": [
        "Antonisen, Silas*",
        " L\u00f3pez-Espejo, Iv\u00e1n"
      ],
      "authors_and_affil": [
        "Silas Antonisen (University of Granada)*",
        " Iv\u00e1n L\u00f3pez-Espejo (University of Granada)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGM7B8C",
      "day": "3",
      "keywords": [
        "Applications -> music composition, performance, and production; Human-centered MIR -> human-computer interaction; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Creativity -> tools for artists"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1FHMaLM9kP_efWjgEbKSpw992jVUIfaF6/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1zaKJIe_X3jNlitv-0i71A42s2-yyIcN4/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-04-polysinger-singing-voice",
      "title": "PolySinger: Singing-Voice to Singing-Voice Translation from English to Japanese",
      "video": "https://drive.google.com/file/d/19Zc4xVJS-BOs4ZNWClZ5Pd2Z-lCAyiPt/view?usp=sharing"
    },
    "forum": "218",
    "id": "218",
    "pic_id": "https://drive.google.com/file/d/1z7L7IDjLpgsOxxR6AsxUalDx43dLPUDY/view?usp=sharing",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce Diff-A-Riff, a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website.",
      "abstract": "Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce Diff-A-Riff, a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website.",
      "author_changes": "We included missing references in the related work section as suggested by reviewers.\nUpdated the Real-Time Factor results for CPU inference (there was a mistake in the calculation but results are slightly better).\nClarified missing values in table 2\nFixed reference formatting\n",
      "authors": [
        "Nistal, Javier*",
        " Pasini, Marco",
        " Aouameur , Cyran",
        " Lattner, Stefan",
        " Grachten, Maarten"
      ],
      "authors_and_affil": [
        "Javier Nistal (Sony CSL)*",
        " Marco Pasini (Queen Mary University of London)",
        " Cyran Aouameur  (Sony CSL)",
        " Stefan Lattner (Sony Computer Science Laboratories, Paris)",
        " Maarten Grachten (Machine Learning Consultant)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MUP4JV",
      "day": "1",
      "keywords": [
        "Generative Tasks -> music and audio synthesis",
        "Applications -> music composition, performance, and production; Creativity -> human-ai co-creativity; Creativity -> tools for artists; Human-centered MIR -> human-computer interaction; MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "This was generally very well received by the reviewers, congrats for a wonderful work, authors!\n\nI would encourage to thoroughly go over every reviewer's comments, particularly regarding the missed references that reviewer #5 brought up.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/17kD9fcVGogcbAwqArNmF6T8HuJF_OU_5/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1QBTWFHw78yg2xAQcWLAIyXrpiDC2L7Rg/view?usp=sharing",
      "review_1": "Strengths:\n- The application of the proposed system - single instrument generation given a partial-production context - is highly practical and facilitates human creative processes.\n   \n- A thorough human study is performed and its results are aligned with the paper method and quantitative results.  \n\n- The ability to generate single instruments from a partial musical context, with or without specification through textual descriptions, is highly practical for music production - and is a major contribution.\n\n- The ability to condition on a reference single instrument audio - is highly practical for music production - and enhances controllability compared to prior work.\n\n- The reported human study, together with the supplementary demo page, demonstrate state-of-the-art quality of the generated samples. \n\n\nWeaknesses:\n- Lacks an ablation study on the multi source classifier free guidance sampling, and on its chosen coefficients.\n- Lacks an ablation study on the chosen pseudo stereo width.",
      "review_2": "Strengths:\n- Overall writing is high quality and easy to understand\n- Model architecture design is reasonably straightforward and builds upon existing diffusion design principles\n- Demo results seem particular strong, generating high quality musical accompaniments\n\nWeaknesses:\nOverall, the demo results are quite impressive, and the only current critiques are with regards to part of the evaluation: Though the paper's central goal is on building a TTM accompaniment model, the paper's evaluation does not fully assess this, in part due to a lack of comparison with relevant work. While omitting large-scale comparison to StemGen/SingSong is understandable given that they are closed-source, both models have a reasonable suite of public demo examples with the isolated context conditioning that could have been used in a listening study against the present work. More saliently, the paper omits comparison (and even mention) to the established Multi-Source Diffusion Models (MSDM), which is open source and is designed to perform tasks like accompaniment generation, and thus could be directly compared to Diff-a-Riff in large scale objective metrics (MSDM is not text-conditioned, though this shouldn't affect evaluation here). Without either comparison, it is hard to assess Diff-a-Riff's ability at its original goal of accompaniment generation relative to existing work, and such inclusions would improve the paper as a whole.",
      "review_3": "This paper is great, the novelty of this paper is okay, but the systems are very well designed and conducted. I don't have issues with this paper. The demo page looks great. Other than the major contributions, one tiny design of the pseudo stereo method is smart.",
      "session": [
        "2"
      ],
      "slack_channel": "p2-08-diff-a-riff",
      "title": "DIFF-A-RIFF: MUSICAL ACCOMPANIMENT CO-CREATION VIA LATENT DIFFUSION MODELS",
      "video": "https://drive.google.com/file/d/1dNMZ76lzlZFS_feWfNRjIv5Vkfx55749/view?usp=sharing"
    },
    "forum": "225",
    "id": "225",
    "pic_id": "https://drive.google.com/file/d/1uMTTqCZzqQnVJHHJGHm0EEFkEUzd60hu/view?usp=sharing",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6\\% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples available at: https://conference-demo-2024.github.io/demo/",
      "abstract": "The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6\\% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples available at: https://conference-demo-2024.github.io/demo/",
      "author_changes": "",
      "authors": [
        "Haseeb, Muhammad Taimoor*",
        " Hammoudeh, Ahmad",
        " Xia, Gus"
      ],
      "authors_and_affil": [
        "Muhammad Taimoor Haseeb (Mohamed bin Zayed University of Artificial Intelligence)*",
        " Ahmad Hammoudeh (Mohamed bin Zayed University of Artificial Intelligence)",
        " Gus Xia (Mohamed bin Zayed University of Artificial Intelligence)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F71DDM",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Applications -> music retrieval systems; Creativity -> human-ai co-creativity; Creativity -> tools for artists; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1V7_pi-xl8w6NaSXtx19TnfXuNi70nsF-/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/17LX_eqTjhyJwTVsoLxihZWu02qHnwpnY/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-09-deep-recombinant-transformer",
      "title": "DEEP RECOMBINANT TRANSFORMER: ENHANCING LOOP COMPATIBILITY IN DIGITAL MUSIC PRODUCTION",
      "video": "https://drive.google.com/file/d/15TzFNZPcOT9P5p_Kqe1lpvfH_j5duHDh/view?usp=sharing"
    },
    "forum": "231",
    "id": "231",
    "pic_id": "https://drive.google.com/file/d/1M3c7JEvlngUOURE-mbKB3vSuUBxqHUE3/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like In- Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs\u2019 potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX , an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
      "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like In- Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs\u2019 potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX , an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
      "author_changes": "",
      "authors": [
        "Deng, Qixin",
        " Yang, Qikai",
        " Yuan, Ruibin*",
        " Huang , Yipeng",
        " Wang, Yi",
        " Liu, Xubo",
        " Tian, Zeyue",
        " Pan, Jiahao",
        " Zhang, Ge",
        " Lin, Hanfeng",
        " Li, Yizhi",
        " MA, Yinghao",
        " Fu, Jie",
        " Lin, Chenghua",
        " Benetos, Emmanouil",
        " Wang, Wenwu ",
        " Xia, Guangyu",
        " Xue, Wei",
        " Guo, Yike"
      ],
      "authors_and_affil": [
        "Qixin Deng (University of Rochester)",
        " Qikai Yang (University of Illinois at Urbana-Champaign)",
        " Ruibin Yuan (CMU)*",
        " Yipeng Huang  (Multimodal Art Projection Research Community)",
        " Yi Wang (CMU)",
        " Xubo Liu (University of Surrey)",
        " Zeyue Tian (Hong Kong University of Science and Technology)",
        " Jiahao Pan (The Hong Kong University of Science and Technology)",
        " Ge Zhang (University of Michigan)",
        " Hanfeng Lin (Multimodal Art Projection Research Community)",
        " Yizhi Li (The University  of Sheffield)",
        " Yinghao MA (Queen Mary University of London)",
        " Jie Fu (HKUST)",
        " Chenghua Lin (University of Manchester)",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Wenwu  Wang (University of Surrey)",
        " Guangyu Xia (NYU Shanghai)",
        " Wei Xue (The Hong Kong University of Science and Technology)",
        " Yike Guo (Hong Kong University of Science and Technology)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM60SQ3F",
      "day": "3",
      "keywords": [
        "Creativity -> computational creativity; Creativity -> human-ai co-creativity",
        "Applications -> music composition, performance, and production"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1d9YrO-EbGtMbuDhXaOVwzRV8Ba-XnfO7/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1EJZNth3SLMRutsH9WVrPghGvzJk21EAK/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-02-composerx-multi-agent",
      "title": "ComposerX: Multi-Agent Music Generation with LLMs",
      "video": "https://drive.google.com/file/d/1APy8gF-5PJjQHv2uslozLB_80fP25UBO/view?usp=sharing"
    },
    "forum": "237",
    "id": "237",
    "pic_id": "https://drive.google.com/file/d/1P-CPFLWiu4vrHifYxTl3deiqOazzTsKs/view?usp=sharing",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.",
      "abstract": "Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.",
      "author_changes": "We would like to thank the reviewers for their detailed comments and suggestions. We have addressed these comments to the extent possible in the camera-ready submission. The changes include:\n\n1) Rephrasing the discussion on previous work in Sec. 1, specifically regarding the \u201ccontext-dependent\u201d nature of audio production raised by R2 and MR. This discussion aims to underline the limitations of previous works, both rule-based and machine learning systems, which treat audio production as a one-to-one mapping. This motivates style transfer systems, which adapt based on user input.\n\n2) Providing further details on the listening study as requested by R2. Participants were asked to rate each stimulus on a scale from 0 to 100, considering its similarity to a reference recording while ignoring differences in content and focusing on audio production.\n\n3) Clarifying the role of the Oracle, which all reviewers raised questions about. The Oracle simply takes the input recording and applies the same parameter configuration used to create the reference. Since the input recording may have a different \u201cstarting point\u201d from the unprocessed reference, simply applying the same parameters may not result in an ideal style transfer. This was reflected in our evaluation, with listeners sometimes rating the Oracle lower than other methods.\n\n4) Addressing important points about the audio effect representation (AFx-Rep) raised by R2 and MR regarding its lack of invariance to content. This is true given our pretraining setup. We added details in Sec. 2 to clarify this potential limitation. However, we further motivated this choice by stating that our method is more scalable, as we can leverage any audio data, including already processed audio. This proves beneficial as it exposes our model to a wider range of effects beyond those we synthetically apply, aiding in more generalized style transfer.\n\n5) Resolving a number of typos and rephrasing some passages based on reviewer suggestions.\n",
      "authors": [
        "Steinmetz, Christian J.*",
        " singh, Shubhr",
        " Comunita, Marco",
        " Ibnyahya, Ilias",
        " Yuan, Shanxin",
        " Benetos, Emmanouil",
        " Reiss, Joshua D."
      ],
      "authors_and_affil": [
        "Christian J. Steinmetz (Queen Mary University of London)*",
        " Shubhr singh (Queen Mary University of London)",
        " Marco Comunita (Queen Mary University of London)",
        " Ilias Ibnyahya (Queen Mary University of London)",
        " Shanxin Yuan (Queen Mary University of London)",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Joshua D. Reiss (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MV1GN5",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        "Generative Tasks -> transformations; MIR tasks -> music synthesis and transformation; MIR tasks -> similarity metrics; Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "TRUE",
      "meta_review": "This paper proposes an inference-time optimization approach to estimating parameters of an audio effect chain based on audio effect similarity between the output and query for audio effect style transfer.\n\nThe paper proposes a novel approach, conducts thorough experiments, and presents the ideas and results clearly. However, reviewers do point out some limitations of this work. In particular, the (missing) invariance of the embedding to the content is the biggest limitation. As the paper tries to model the audio quality instead of the underlying audio content, this (missing) invariance is an important issue and should be discussed in the paper.\n\nOverall, all reviewers and the independent meta-review liked the idea and contributions of this work, and recommended \"strong accept\" or \"weak accept\". One reviewer and the meta-review also recommended \"award quality\", and the other reviewers did not object this recommendation.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1UVtuCnE7K0tByHHGlVoMRKldDgf7me1Y/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1cT7XVw1JPxlybJYhaApaFAebYeY3u4l5/view?usp=sharing",
      "review_1": "The paper proposes a new approach for audio effect parameter estimation from a reference recording. The approach uses a learned deep embedding that should discriminate different \"styles\" while being invariant to content, which is also the main focus of the work. The actual optimization method is mostly treated as a black box, while a comprehensive evaluation is included to test the approach for two different tasks (style classification and style transfer).\n\nThis is a relevant contribution with a good contextualization, interesting experiments, and a meaningful selection of baselines. However, there are several issues that, in my opinion, should be addressed in a possible publication.\n\n(1) I'm not sure I understand how the pretext tasks ensures a disentanglement of (or an invariance to) the content of the audio signal. A thought experiment: If the MLP for a classifier task just learns to calculate $z_i - z_o$ in the first layer, the individual embeddings could theoretically still include all information about $x_i$ and $x_o$, respectively, while the classification can then focus on the style differences. So in a sense, $g$ is clearly encouraged to *include* information about the effects in the embedding but not discouraged to *exclude* content information? This could also explain the wide spread of cosine similarity of the Oracle condition in Fig. 5. I wonder if a different pretext task, like contrastive learning, could be beneficial here.\n\n(2) The subjective results are quite inconclusive, which is also congruent with my impression when listening to the provided audio examples. A few observations/thoughts on the listening test:\n* Why is the Oracle condition detected so inconsistently by the participants? Is there maybe a difference between individuals, so that if those who do not somewhat consistently identify the Oracle are excluded, results become more pronounced?\n* I think the music examples are not very good for the purpose, since the input itself already appears to be heavily processed.\n* Also, the actual task is a bit unclear from the paper. I assume that 100 means most similar to the reference and 0 means completely dissimilar, while participants where asked to consider only style and not content?\n\n(3) The parameter estimation experiment does not take any interaction between parameters (within and across effects) into account. While this isolated experiment is an interesting starting point, it does not allow any conclusions about the \"high-dimensional loss landscape\" that is described by the embedding distance. It would be interesting to have a meaningful objective evaluation for \"bad\" local minima, which clearly seem to exist.\n\n(4) The strategy of sampling parameter configurations for training the embedding model appears a bit odd to me. While this method makes reasonably sure that the 10 presets for each effect are quite distinct, there is no guarantee that \"perceptually diverse\" parameters are also \"perceptually meaningful\". Since some parameters are strongly interrelated (even between different plugins in the chain) and some parameters can be quite sensitive to change around their optimal working point (e.g. the delay speed around the actual tempo of the song), it may be better to train with manually tuned presets (which may be difficult to obtain to be fair)?\n\n(5) What is the meaning of the numbers in Table 1? Accuracy? F-Measure?\n\n(6) Is there a plan to publish code and/or pre-trained models? Especially the embedding model could be useful as a baseline or starting point for many different tasks related to audio effects.\n\n(7) Zero-shot style classification: Could forming the prototype as the mean embedding of multiple examples from each class instead of taking just one example be more robust?\n\n(8) I think it would have been good to reflect the focus on the learned embedding in the title of the paper. (Not sure if it can still be changed.)\n\n(9) Some minor issues:\n* How are the actual \"candidate parameters\" sampled in the CMA-ES method? Is this the mean of the population or the population member with the smallest distance to the reference?\n* l. 48: \"adapt based on the external context\" is unclear to me.\n* Figure 3 caption: $x_0$ should be $x_o$ in the second line.\n* l. 161: to be consistent with Fig. 3, it should be $z_o$ instead of $z_r$\u200b here, I think.\n* Eq. 3 and l. 205: Similarly, $z_i$ should be $z_o$ to be consistent with $x_o = f_c(...)$\u200b?\n* l. 295 space missing\n* Table 1: While the meaning of each abbreviation can be guessed, they are not introduced anywhere (and they are in a different order than introduced in the text)\n* l. 308: verb missing\n* l. 342: period missing\n* Table 2: How to obtain a \"correlation coefficient\" is only briefly explained in the table caption. This could be done more comprehensively in the text.\n* ll. 381-382: \"as good of\" -> \"a comparable\"\n* Fig. 6: The grouping of the subfigures makes it a bit difficult to compare related experiments.\n* Fig. 6: I assume that the grey bar is for the Input as in Fig. 5?",
      "review_2": "This paper introduces a novel system for audio effect style transfer. It thoroughly describes the method and compares it with most previous works in this field through relevant experimentation. Various audio production style transfer tasks are benchmarked, showcasing the effectiveness of the method. Real-world audio production scenarios for arbitrary effects are also explored and evaluated via a listening test.\n\nThe paper has significant contributions. An elusive audio effect metric similarity has been achieved, and gradient-free optimization is proposed as a great alternative to deal with the inherent difficulty of handling non-differentiable audio processors within learned-based systems.\n\nI believe the paper is of great interest for this conference, and thus my overall evaluation is Strong Accept.\n\nOne limitation that could be included in the discussion is that, due to the inherent nonlinearity of various audio effects, the computation of \"Oracle\" is ill-posed. This means that when having recordings with different content, A and B, applying an audio effects chain Fx with parameters Wx won't necessarily yield the same music production style. This depends on the type of processing (or lack of) that A and B have. Fx_Wx(A) is not (always) equal to Fx_Wx(B) in terms of audio effect style similarity. Of course, the system seems to be performing well when even with this being omitted, but it could be included in the discussion to further strengthen the robustness and scientific contribution of this paper.\n\nFor further discussion, the paper could benefit from authors commenting on the effectiveness of AFx-Rep for style transfer of combinations of various audio effects, even though this metric was trained only with one effect at a time. What insights or intuition do the authors have about the reported performance of the system, as one would expect that a metric trained with one effect at a time could struggle to encode information relevant to various audio effects being applied to an audio signal?\n\nAlso, it is reported that the system struggles for parameter estimation for chorus. Is there any insight from the authors that somehow time-varying modulation effects are harder to model? The paper could also benefit from a brief clarification of this.\n\nFinally, more details about the listening test could have been reported. What type of multiple stimulus test was performed? What question was asked to the participants? What were the listening conditions? Are any p-values relevant to the given results?\n\nMinor comments about the paper:\n\n- Figure 1: Could \"sim\" be replaced with AFx-Rep? This could highlight AFx-Rep as one of the main contributions of this work.\n\n- Section 1, Line 85: Add a reference to DeepAFx-ST.\n\n- Section 1, Line 91: The subjective listening test should be included as a contribution only if there is novelty in the subjective listening test presented, which I believe is not the case. Thus, I suggest removing this from the list of contributions.\n\n- Section 3, Line 249: It is mentioned before that 20,000 segments were taken, but then it is said that random crops are applied as augmentations. It is not clear how these crops are being applied to the input and/or output. Is it always applied to both? What type of crops? Clarifying this could ease reproducibility of the paper.\n\n- Section 5.1: Although the authors mention that it is difficult to draw conclusions with the audio production Style metric alone, readers would see that since ST-ITO models were trained using AFx-Rep, these models will perform better when measured via AFx-Rep. It could be interesting to also see how this evaluation goes when using the metrics that DeepAFx systems were trained with.",
      "review_3": "This work presents a self-supervised embedding trained to specifically attend to production style. This is coupled with a generic control strategy based on derivative-free optimization methods. The use of the embedding generalizes the control process, allowing it potentially to control novel sets of effects.\n\nThe proposed system is described quite clearly, employing many diagrams, which help to understand the technical models and experiments. The training and experiment procedures are described in detail.\n\nA classification experiment proves that the system can identify production styles by the embedding, better than existing embedding approaches. Furthermore, attempts are made to mimic real production styles. This is no small task. My impression is that the parameter settings found in some of the real-world style transfer do not fit as well as they could. This, if explored, could lead to system improvements.\n\nI have a doubt is regarding the Oracle condition in the \"Real world style transfer\" task. For instance, now I see that the reason that the Oracle has non-zero error, is because the Oracle is still being compared to the reference embedding, which is a signal with entirely different content. I believe this may hide estimation errors which may be under the threshold of inherent differences (due to content difference) between the reference and Oracle conditions.\n\nHowever, wouldn't it make sense to also compare the error (both in embedding space and parameter space) of the Oracle condition versus the algorithm conditions? In this way, every small deviation in parameter estimation would be visible (in either domain), potentially shedding more light on the performance of both the optimization algorithm, and the ability of the embedding to represent small changes in parameters.\n\nAs someone interested in optimization, I wonder about the landscape of the objective function itself. This could be shown with simple example with few parameters, plotting the cosine similarity directly. Furthermore, I wonder how capable the gradient-free method chosen (CMA-ES) is really able to solve the problem.\n\nIn future work, I think more could be done to deepen the evaluation. In my opinion, the optimization method could also have been better justified. All in all, I appreciate this paper for the clarity of its approach and descriptions, for a task as challenging as automatic style transfer.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-01-st-ito-controlling",
      "title": "ST-ITO: Controlling audio effects for style transfer with inference-time optimization",
      "video": "https://drive.google.com/file/d/1LgitojSd5dpzY2_71D0hoErCGiugaeo_/view?usp=sharing"
    },
    "forum": "249",
    "id": "249",
    "pic_id": "https://drive.google.com/file/d/1kTHqdjx8t_lvCG8cGeKZCIZWKenGOl-s/view?usp=sharing",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Inner Metric Analysis (IMA) is a method for symbolic music analysis that identifies strong and weak metrical positions according to coinciding periodicities within note onsets. These periodicities are visualized with bar graphs known as metric weight and spectral weight profiles. Analyzing these profiles for the presence of syncopation has thus far required manual inspection. In this paper, we propose a simple measure using chi-squared distance for quantifying the level of syncopation found in IMA weight profiles by considering each as a distribution to be compared against (1) a uniform distribution 'nominal' weight profile, and (2) a non-uniform distribution based on beat strength. We apply this measure to the task of predicting perceptual ratings of syncopation using the Song (2014) dataset of 111 single-bar rhythmic patterns and compare its performance to seven existing models of syncopation/complexity. Our results indicate that the proposed measure based on (1) achieves a moderately high Spearman rank correlation (r_s=0.80) to all ratings and is the only single measure that reportedly works across all categories. For so-called polyrhythms in 4/4, the measure based on (2) surpasses all other models and further outperforms five models for monorhythms in 6/8 and three models for monorhythms in 4/4.",
      "abstract": "Inner Metric Analysis (IMA) is a method for symbolic music analysis that identifies strong and weak metrical positions according to coinciding periodicities within note onsets. These periodicities are visualized with bar graphs known as metric weight and spectral weight profiles. Analyzing these profiles for the presence of syncopation has thus far required manual inspection. In this paper, we propose a simple measure using chi-squared distance for quantifying the level of syncopation found in IMA weight profiles by considering each as a distribution to be compared against (1) a uniform distribution 'nominal' weight profile, and (2) a non-uniform distribution based on beat strength. We apply this measure to the task of predicting perceptual ratings of syncopation using the Song (2014) dataset of 111 single-bar rhythmic patterns and compare its performance to seven existing models of syncopation/complexity. Our results indicate that the proposed measure based on (1) achieves a moderately high Spearman rank correlation (r_s=0.80) to all ratings and is the only single measure that reportedly works across all categories. For so-called polyrhythms in 4/4, the measure based on (2) surpasses all other models and further outperforms five models for monorhythms in 6/8 and three models for monorhythms in 4/4.",
      "author_changes": "",
      "authors": [
        "Bemman, Brian*",
        " Christensen, Justin"
      ],
      "authors_and_affil": [
        "Brian Bemman (Durham University)*",
        " Justin Christensen (The University of Sheffield)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1M20JW",
      "day": "2",
      "keywords": [
        "Computational musicology",
        "MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1yiNllHxKAmqCiJvBI-FrQHrt_nRxyMno/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1H5nfBlCS0ILmBV-1MTpzqmPvfKScKJSW/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-03-inner-metric-analysis",
      "title": "Inner Metric Analysis as a Measure of Rhythmic Syncopation",
      "video": "https://drive.google.com/file/d/1ynvu1NYUiKg5yi1GwiZtL4tpdbDhneeD/view?usp=drive_link"
    },
    "forum": "251",
    "id": "251",
    "pic_id": "https://drive.google.com/file/d/1RryR5iVASqN_UL-UFLDmAS-I8OTV6dQI/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",
      "abstract": "Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",
      "author_changes": "",
      "authors": [
        "Bindi, Giovanni*",
        " Esling, Philippe"
      ],
      "authors_and_affil": [
        "Giovanni Bindi (IRCAM)*",
        " Philippe Esling"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1M4G2E",
      "day": "4",
      "keywords": [
        "Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> music generation; MIR tasks -> music synthesis and transformation; MIR tasks -> sound source separation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ztKyMPIcPJo0E72ednIy3TDfApTNd_ag/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1DLqRUQZ_YjKq-obUOgLO4XaZ37s1cAGq/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-09-unsupervised-composable-representations",
      "title": "Unsupervised Composable Representations for Audio",
      "video": "https://drive.google.com/file/d/1-COJpeJIhvMJlZi-x-_KVgI1j8745Tui/view?usp=sharing"
    },
    "forum": "254",
    "id": "254",
    "pic_id": "https://drive.google.com/file/d/1o1nPt9tNpHYfLZPIl7sndn6JIQLO3Hzd/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",
      "abstract": "Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",
      "author_changes": "",
      "authors": [
        "Evans, Zach",
        " Parker, Julian D*",
        " Carr, CJ",
        " Zuckowski, Zachary",
        " Taylor, Josiah",
        " Pons, Jordi"
      ],
      "authors_and_affil": [
        "Zach Evans (Stability AI)",
        " Julian D Parker (Stability AI)*",
        " CJ Carr (Stability AI)",
        " Zachary Zuckowski (Stability AI)",
        " Josiah Taylor (Stability AI)",
        " Jordi Pons (Stability AI)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F7GLPR",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1XSTppMn8KRzwDzMMuHS_K4kW66TswNNt/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1yrCSDe2DdYEVK95jQy5tNf_4EUX6J6ns/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-08-long-form-music",
      "title": "Long-form music generation with latent diffusion",
      "video": "https://drive.google.com/file/d/1X2HlAu0ftKr0tp09ETAwQScU8pau8mME/view?usp=sharing"
    },
    "forum": "258",
    "id": "258",
    "pic_id": "https://drive.google.com/file/d/1H5kCABRvG2SjS2lmwcwqICsJIfE_UlGr/view?usp=sharing",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Although deep neural networks can estimate the key of a musical piece, their supervision incurs a massive annotation effort. Against this shortcoming, we present STONE, the first self-supervised tonality estimator. The architecture behind STONE, named ChromaNet, is a convnet with octave equivalence which outputs a ``key signature profile'' (KSP) of 12 structured logits. First, we train ChromaNet to regress artificial pitch transpositions between any two unlabeled musical excerpts from the same audio track, as measured as cross-power spectral density (CPSD) within the circle of fifths (CoF). We observe that this self-supervised pretext task leads KSP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured KSP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs.  We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision. We observe that this self-supervised pretext task leads PCP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured PCP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision.",
      "abstract": "Although deep neural networks can estimate the key of a musical piece, their supervision incurs a massive annotation effort. Against this shortcoming, we present STONE, the first self-supervised tonality estimator. The architecture behind STONE, named ChromaNet, is a convnet with octave equivalence which outputs a ``key signature profile'' (KSP) of 12 structured logits. First, we train ChromaNet to regress artificial pitch transpositions between any two unlabeled musical excerpts from the same audio track, as measured as cross-power spectral density (CPSD) within the circle of fifths (CoF). We observe that this self-supervised pretext task leads KSP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured KSP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs.  We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision. We observe that this self-supervised pretext task leads PCP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured PCP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision.",
      "author_changes": "",
      "authors": [
        "KONG, Yuexuan*",
        " Lostanlen, Vincent",
        " Meseguer Brocal, Gabriel",
        " Wong, Stella",
        " Lagrange, Mathieu",
        " Hennequin, Romain"
      ],
      "authors_and_affil": [
        "Yuexuan KONG (Deezer)*",
        " Vincent Lostanlen (LS2N, CNRS)",
        " Gabriel Meseguer Brocal (Deezer)",
        " Stella Wong (Columbia University)",
        " Mathieu Lagrange (LS2N)",
        " Romain Hennequin (Deezer Research)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F7L28P",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification",
        "Musical features and properties -> harmony, chords and tonality"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Nhy-9MFVDrdEab7l0P7VndGYameYnDp6/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1omJMBuixqU0dfrKeN0E2Uev99spaQy1f/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-18-stone-self-supervised",
      "title": "STONE: Self-supervised tonality estimator",
      "video": "https://drive.google.com/file/d/129ZAwEILwjS7R7PiOMPtnUo7PpjW1Wz3/view?usp=sharing"
    },
    "forum": "260",
    "id": "260",
    "pic_id": "https://drive.google.com/file/d/1VoXdy9aSzBbiKFSnFIOe4btBxx1GXjma/view?usp=sharing",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",
      "abstract": "We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",
      "author_changes": "",
      "authors": [
        "Sowula, Robert*",
        " Knees, Peter"
      ],
      "authors_and_affil": [
        "Robert Sowula (TU Wien)*",
        " Peter Knees (TU Wien)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MVH8TT",
      "day": "3",
      "keywords": [
        "Creativity -> tools for artists",
        "Applications -> music recommendation and playlist generation; Creativity -> creative practice involving MIR or generative technology ; Human-centered MIR -> music interfaces and services; MIR tasks -> similarity metrics; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1c_OY_5sI5pR0HLwObXqRUuE8RYipUOpU/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/14PHc1b8L-XG5xKNVewIRd7dXPUUp78Rw/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-04-mosaikbox-improving-fully",
      "title": "Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-based Stem Modification And Precise Beat-Grid Estimation",
      "video": "https://drive.google.com/file/d/1GZUMFnXUsYbkjjFZsAJkMCERw70Ra7wJ/view?usp=drive_link"
    },
    "forum": "261",
    "id": "261",
    "pic_id": "https://drive.google.com/file/d/1JtyOe4i6Vkr8-5X1FAVecxObcdRjA_8N/view?usp=drive_link",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately 3 hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to existing multi-track guitar datasets. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the suitability of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",
      "abstract": "Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately 3 hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to existing multi-track guitar datasets. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the suitability of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",
      "author_changes": "",
      "authors": [
        "Glytsos, Marios*",
        " Garoufis, Christos",
        " Zlatintsi, Athanasia",
        " Maragos, Petros"
      ],
      "authors_and_affil": [
        "Marios Glytsos (National Technical University of Athens)*",
        " Christos Garoufis (Athena Research Center)",
        " Athanasia Zlatintsi (Athena Research Center)",
        " Petros Maragos (National Technical University of Athens)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MVK7TK",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> evaluation metrics; MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1hmHE0nv8wZsj51ajCsSdN_UehrQKrMDt/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1wq-3SzXdmDQmELX5hNjycKnF-RSLLc2_/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-06-classical-guitar-duet",
      "title": "Classical Guitar Duet Separation using GuitarDuets - a Dataset of Real and Synthesized Guitar Recordings",
      "video": "https://drive.google.com/file/d/13Iw_NtHQmtN_Z4iend_Py8r4_fUG9F2x/view?usp=drive_link"
    },
    "forum": "262",
    "id": "262",
    "pic_id": "https://drive.google.com/file/d/1tT0xYZ7MVSJG6-Dj4asIYA6Ewm_d1oIz/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",
      "abstract": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",
      "author_changes": "Here is the summary of the changes we made to address the reviewers\u2019 critiques:\n- Global changes: we mostly replaced the words \"synthetic\" and \"spoofing\" by \"cloned\" throughout this paper. The former two words caused confusion amongst reviewers. As such, the title of the paper is now \"From Real to Cloned Singer Identification\". We also use percentages in all figures to match our other results\u2019 reporting. Finally, the words \"music streaming service\" are replaced by Deezer now that anonymity is not needed.\n- Section 3.1: we remove the term \"valid\" to describe a song, clarify that a \"unique singer\" here means a track with \"more than one singer\", highlight that \"singer\" annotations are collected to create the closed dataset, and edit out the reference to the current section.\n- Section 3.2: We explicitly list the positive stem pairs that can occur during the Hybrid model\u2019s pre-training. We also emphasise that only mixtures are used during the downstream singer identification task when the Hybrid backbone is used.\n- We clarify the train and validation splits used in Sections 3.2 and 3.3.\n- Section 3.3: we emphasise that the classifiers have the same architecture as the projector head.\n- We emphasise that the CLMR embeddings in Section 4.1 are used for both training and testing our classifiers for the real singer identification task on open datasets. We also better described the \"majority vote\" scheme used to generate classification results.\n- Section 4.2: We enlarge Figure 1 for better clarity. We also introduce a table to compare our results with others in the literature. As such, the paragraph that used to do so is drastically shortened. Finally, we add a description of the genres\u2019 distribution in Figure 2\u2019s caption.\n- Section 4.3: We clarify Figure 4\u2019s caption by directly referring to each plot\u2019s colour and legend.\nNote that we also slightly reformulated some sections of the paper and resized some figures and tables to make everything fit in six pages!",
      "authors": [
        "Desblancs, Dorian*",
        " Meseguer Brocal, Gabriel",
        " Hennequin, Romain",
        " Moussallam, Manuel"
      ],
      "authors_and_affil": [
        "Dorian Desblancs (Deezer Research)*",
        " Gabriel Meseguer Brocal (Deezer)",
        " Romain Hennequin (Deezer Research)",
        " Manuel Moussallam (Deezer)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1MJEBC",
      "day": "1",
      "keywords": [
        "Generative Tasks -> music and audio synthesis; MIR fundamentals and methodology -> music signal processing; MIR tasks -> sound source separation; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The reviews of this paper found the task to be important and the experimental validation to be thorough and rigorous. While there were comments about the paper's clarity and title that we hope the authors will address, we feel that this paper will make a strong candidate for publication at this year's ISMIR.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/view?usp=drive_link",
      "review_1": "This paper explores the use of contrastive learning for learning embeddings for the task of singer identification in three configurations: mixture, vocal and hybrid. The experiments show promising results, especially in the case of using the vocal model to identify real singers (as opposed to synthetic). When the instrumental stems are present in the pre-training (mixture or hybrid model) the performance is reduced. The models perform particularly worse in the case of identifying synthetic versions of the singers. These observations are explained and explored in the paper in detail. I just have a few small suggestions:\n\n- In line 129, annotations are mentioned. However, it is unclear to me what these annotations contain and how they were used in the experiments (I\u2019m assuming the name of the artists were used, but was there anything else?).\n- In Figure 1, the Closed dataset plot is a bit too jammed together. I\u2019d recommend plotting the accuracies of the models next to each other, rather than on top of each other.\n- The observation of the non-uniform performance over musical genres (lines 322--346) would benefit from mentioning the distribution of genres in the training datasets.\n- Figure 4 (as well as its caption) is a bit confusing. I\u2019d recommend to, instead of using numbers in the caption, mention the bars themselves (e.g. \u201cthe purple bars (test/other) show \u2026 \u201c). I\u2019d also suggest to put the Mixture, Hybrid, Vocal as subplot titles rather than on the right.\n\nAside from these minimal suggestions, I believe the paper is well written and complete, making it a very good contribution to ISMIR. ",
      "review_2": "The strengths of this paper is that (a) it provides a massive dataset (with detailed description), and (b) it confirms a system trained on the real inputs alone is not capable to perform well in the synethic inputs; This implies the system should also be trained with the synthetic inputs.\nThus, the major weakness of this paper is that it triggers the reader to ask why this paper does not also examine a singing identification system trained with synthetic inputs and both (real and synthetic). \nThus, in order to match the claim of this paper, below is the suggestion. Focus on 3 systems first (trained with (a) real, (b) synthetic, and (c) real + synthetic, and then their performance on 3 types of dataset (a) real alone, (b) synthetic alone, and (c) both real + synthetic), resulting 9 types of performance result. Given such performance results, and/or with the optional variety like genres, this paper would be more easier to read and its strengths would be strengthened. ",
      "review_3": "This paper trained the embedding of singing features by singer-level contrastive learning.\nUsing a singer identification model with the projector head removed, the paper reported the differences in classification  rates and characteristics between human singing voices and synthesized singing voices. NT-Xent was used for losses in embedded model training. Three types of models were trained in embedding learning: targeting separated singing voice, mixture, and their hybrid.\n\nIn an evaluation using the open dataset, the Mixture-based embedding model outperformed CLMR [36] as a conventional method. Compared to training the CLMR model, the proposed model used less than 1/20 of the number of tracks, indicating the effectiveness of singer-level sampling.\n\nIn the evaluation with the closed dataset, the paper experimented with a subset of 100 to 1000 classes. Although the performance was better with the singing voices separated, the difference in performance was small. Although it is difficult to compare performance with previous studies using inhouse datasets, this paper believe that the proposed model is at least as good. In addition, this paper showed that there can be differences in classification performance due to musical genres and difficulties in classification for singers with many songs.\n\nEvaluations using synthetic singing datasets showed that performance was lower than when targeting real singing voices. In particular, the performance of the Mixture or Hybrid model was degraded. To investigate the cause of the performance degradation, results on the cosine similarity of the embedding vectors were also reported, such as higher similarity for the instrumental embeddings in the Mixture or Hybrid models.\n\nThe paper is well written and has adequate references. The fact that the performance of this method is equal to or better than conventional methods when targeting human singing voices and the fact that it investigates the factors that cause performance degradation are good.\nFuture directions following this paper include efforts to improve performance on synthetic singing voices.\n\nThe following are the issues that need to be corrected.\n- Line 209: The details of the architecture for singer identification are not clear and need to be stated.\n- Line 400: Figure 4 is discussed in the text without explanation and is difficult to understand. The explanation is needed. The following comments are also relevant.\n- Line 405: \"the instrumental embeddings\" is the first occurrence of here, at least in the text, and it is unclear how this was obtained.\n- Figure 4: It is necessary to specify which legend each of the explanations 1) to 5) corresponds to.\n\nIn addition, the following are minor comments, but these should also be corrected.\n- Line 175: Regarding \"Finally, for our Hybrid model, these segments are randomly sampled from either the songs\u2019 mixtures or their vocal stems\", it is unclear whether it means that mixture-mixture and vocal-vocal pairs are mixed in B=128, or whether mixture-vocal is also possible.\n- Line 219: Regarding \"At least three tracks per singer are then used for training.\", different singers have different numbers of training data?\n- Figure 4: I find the notation of each legend a little hard to understand.\n",
      "session": [
        "2"
      ],
      "slack_channel": "p2-16-from-real-to",
      "title": "From Real to Cloned Singer Identification",
      "video": "https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/view?usp=drive_link"
    },
    "forum": "271",
    "id": "271",
    "pic_id": "https://drive.google.com/file/d/1QDJhWyLpD5Oheyeq0z7cFInOtFksVAxr/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",
      "abstract": "Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",
      "author_changes": "- We now only evaluate the latest version (v3) of our in-house model, as we agree that it is not warranted to have two versions of it in the paper.\n- As we are not able to provide further details about our model, we instead reduced mentions of our model in the text of the paper.\n- We have better separated fully open-source models from proprietary ones in our results tables, and highlighted the best open-source results.\n- Following the suggestions of R1 and R4, we have added evaluation of the same models on the original JamendoLyrics dataset (Section 4.2, Table 2).\n- We removed speculations/details about Whisper and OWSM in Section 4.1, deemed out of scope.\n- We updated the discussion, addressing a limitation pointed out by R1.\n- We updated the first paragraph of Section 3 to clarify our goals in designing the metrics in response to R1.\n- We now better introduce Section 4.4, presenting results on the SWD dataset.\n- We fixed/clarified minor issues pointed out by R3.\n- We added the benchmark name (Jam-ALT) and a link to the data and code.\n\nTo address other comments:\n\n> R3: \u201cComparing metrics only for the word tokens would further explain their performance and whether these in-house proposals improve mainly on the newly introduced punctuation part or the traditional tokens.\u201d\n\nWER only takes word tokens into account, as mentioned in the paper.\n\n> R1: \u201cIt is unclear to what extent do (and whether) the proposed evaluation metrics (WER\u2019, F_P, F_B, etc) reflect human\u2019s readability on the transcription. [...]\u201d\n\nNote that our goal was to design metrics that are generally applicable, and not tightly bound to the dataset or specific formatting rules. We now mention this in Sections 3 and 5.\n\nWe agree with the point about punishing different formatting choices, and we now mention this more clearly in the discussion.",
      "authors": [
        "C\u00edfka, Ond\u0159ej*",
        " Schreiber, Hendrik",
        " Miner, Luke",
        " St\u00f6ter, Fabian-Robert"
      ],
      "authors_and_affil": [
        "Ond\u0159ej C\u00edfka (AudioShake)*",
        " Hendrik Schreiber (AudioShake)",
        " Luke Miner (AudioShake)",
        " Fabian-Robert St\u00f6ter (AudioShake)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1MM13L",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data",
        "Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation metrics; Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The newly annotated dataset is considered a welcome contribution, and the paper was praised for its writing and relevance. There are concerns regarding the \"in-house\" system and how it was not properly presented in the paper. There are open questions on the proposed metrics. There is also an interesting comment regarding the two parts of the paper (dataset and benchmarking), which appear disconnected. Based on all the above, I would recommend an acceptance in this case, noting that there are a few minor comments that can be rectified at the camera-ready revision.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/18DaiaPQ46l9FPfs_IX5hI12y0WiaaZg0/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1GbM--7-vFptu10l8mbUFdaf2aYAsuYSy/view?usp=drive_link",
      "review_1": "Before going into the details of the main review, I would first like to mention an issue regarding the anonymity: I have read a very similar paper on arXiv and an ISMIR LBD. That is to say, the anonymity is not fully preserved.\n\nI personally do not have conflict of interest with the authors of that paper. I have contacted meta-reviewer regarding this issue, and the meta-reviewer said that \u201cSince you do not have a conflict of interest with the authors, you can proceed with your review\u201d, while also encouraging me to mention this issue in the review. Therefore, I decide to mention it here.\n\nIn the remainder of this review, I will pretend that I have not read that arXiv paper.\n\nStrengths: The new annotation of the multilingual JamendoLyrics dataset is clearly a contribution and should be credited. Also, the authors made an attempt to define general rules for readability-aware ALT and the corresponding metrics. Although I do have some concerns regarding them (see the weaknesses part), this should still be considered as a contribution.\n\nWeaknesses: \n\n1) It is unclear to what extent do (and whether) the proposed evaluation metrics (WER\u2019, F_P, F_B, etc) reflect human\u2019s readability on the transcription. This includes two issues that need to be addressed. First, as discussed in Section 5 (particularly L408-418), the annotation of line breaks may be ambiguous. In practice, will evaluate and even optimize an ALT system with such ambiguous metrics really do more good than harm? It would be better to delve deeper into this direction, as the novel evaluation metrics are an important part of this paper. Second, the proposed evaluation metrics require the automatic transcription to strictly match the 9 general rules in Section 2. Although these rules sound reasonable, it is unclear whether an alternative set of rules (e.g., always end a line with a comma or a period) would lead to a poor formatting from the perspective of human. If this is not the case, it would be weird to punish such an alternative formatting (as what the proposed metrics do).\n\n2) It would be better to also report the benchmark results on the original multilingual JamendoLyrics annotation. Different annotations (note that the two versions of annotations have a 11.1% WER difference) may lead to completely different evaluation results and conclusions.\n\n3) Adding the proposed in-house ALT systems in the comparison while almost completely not discussing the method behind these systems is clearly not a good idea, as it provides basically no insight. Readers may wonder how can these in-house ALT systems achieve superior performance and want to know more information about it. But the authors provided almost no detail on it.\n\nTo sum up, this paper has both strengths and weaknesses. I consider the lack of details on the in-house ALT systems a big weakness of this paper, but I do recognize the contribution of the novel dataset annotation, along with the efforts on devising novel readability-aware ALT metrics. Therefore, I tend accept this paper.\n\nThat being said, I strongly suggest the authors provide more details on the in-house ALT systems in their camera-ready version. That would greatly improve the overall quality of this paper.\n",
      "review_2": "I believe the paper is valuable for ISMIR and should be accepted. Next, I list some of the strengths and weaknesses I consider relevant:\n\nStrengths:\n1) Relevance. The paper introduces a novel benchmark for automatic lyrics transcription (ALT), emphasizing the inclusion of formatting and punctuation elements. It also proposes new evaluation metrics tailored to lyrics transcription based on industry guidelines, which adds value to the ALT field and addresses a gap in current benchmarks.\n\n2) Well structured. The paper is well-written and organized. The title and abstract reflect the content. As far as I know, it cites and compares related work and introduces the problem well. I would also highlight the visualizations, especially those showing word edit operations, which enhance the understanding of the results. The supplementary material also helped me understand the changes in the lyrics.\n\n3) Comprehensive error analysis. The error analysis and comparison provide insights into the strengths and weaknesses of current ALT systems.\n\nWeaknesses:\n1) Lack of details. The paper lacks details about the methods to replicate the results, particularly the in-house lyrics transcription methods used in the comparative analysis. I understand that they may be commercial/closed, but the absence of this information makes it challenging for peers to understand the direction of future research and development. We know there are better approaches, but we do not know what they are.\n\n2) Metric clarification. This is not a big issue, but the description of the new WER metric is somewhat unclear. On line 175, it seems just a \u201ccase-sensitive WER,\u201d but it also suggests a fixed pre-processing procedure. This will be solved since the implementation will be available.\n\n3) There are minor issues in terminology and formatting, such as the inconsistency in the number of token types listed (line 196 states four but lists five) and the use of symbols like \"S.^2\". Also, P is used for the Punctuation Token and the Precision metric in section 3.2, which might confuse readers.\n\n4) Model evaluation. The paper notes that Whisper sometimes outputs random text, but it doesn\u2019t explore the impact of excluding these outliers on the overall results. Comparing metrics only for the word tokens would further explain their performance and whether these in-house proposals improve mainly on the newly introduced punctuation part or the traditional tokens.\n\n5) Unknown details on variants. The authors' system v1 performed poorly on the SWD dataset, while v2 outperformed others, but it lacks an explanation for this improvement, leaving readers without insight into the advancements made.\n\nFinal comment:\nWhile I listed more weaknesses than strengths, I believe the paper significantly contributes to the ALT field, providing a benchmark and metrics that address aspects of lyrics transcription that traditional metrics overlook. Despite the lack of information on the in-house models and minor inconsistencies, the overall quality of the work justifies recommending the paper for acceptance.",
      "review_3": "\u2014 RESUME \u2014\nThis paper extends the LBD \"JAM-ALT: A FORMATTING-AWARE LYRICS TRANSCRIPTION BENCHMARK\" presented at ISMIR last year. It includes a revised version of the JamendoLyrics dataset, a common dataset used for evaluating automatic lyrics transcription (ALT), to align with industry guidelines. Additionally, the authors have suggested adapting two well-known metrics to better capture the specific aspects of lyrics that are not accounted for in speech transcription. Finally, several ATMs are compared, discussing their limitations and errors.\n\n\u2014 SCIENTIFIC CONTRIBUTION \u2014\nTheir main scientific contribution is the revised version of the JamendoLyrics dataset, which now aligns with industry standards. This will help to more accurately assess the performance of current ALT models. The newly proposed metrics provide better insights into the type of errors models do, enabling the development of improved models. In addition, they have benchmarked most of the current models. Their analysis is useful for understanding their performance and limitations.\n\n\u2014 REVISED JAMENDO AND METRICS \u2014\n\nThe paper first sets out formatting guidelines for lyrics based on industry standards. Then, it introduces a revised version of the JamendoLyrics dataset that adheres to these new standards. Moreover, the authors propose a modified version of the word error rate (WER) metric to accommodate case-sensitive errors, along with a method for measuring punctuation and line break errors. This provides a more comprehensive way to quantify ALT model accuracy. The most interesting part is that previous metrics did not take into account the line/section breaks, which are crucial for music, since they convey rhyme, rhythm, and structure in popular Western music.\n\n\n- BENCHMARK - \nIn this section, the authors evaluate the most common ALT models using their revised version of JamendoLyrics. This section reads more like a survey or benchmark paper, comparing different models rather than highlighting the benefits of the revised dataset and proposed metrics. Especially when it feels like the authors try to \"sell\" their in-house model, which is not explained whatsoever or cited. In fact, most of the trends observed (when comparing models) in the WER' are the same for WER, and all the analysis in section 4.1 is based on WER. The authors do not compare the performance obtained from the same model for the previous version of JamendoLyrics to assess the need to use their version rather than the previous one, for instance as an example it would have been more interesting to spot misleading high performances that do not reflect the real structure of a song when missing line breaks(?). The comparison between the revised version and the old version is one of the most interesting aspects of Table 1. This demonstrates that the main issue with the previous version is primarily the line break annotations (hence the section annotation). This is crucial for lyrics because these aspects are linked to rhyme, meter, rhythm, and musical phrasing. I would have expected a bit more analysis in this direction, i.e. in comparing the performance when using both versions of the dataset, rather than such a detailed analysis of the current models. For example, the use of source separation and its impact on the performance, while interesting, seems to be beyond the scope of the paper, it doesn't change anything in trends and simply boosts or decreases the metrics by a factor. Instead, they should have focused on the dataset and the metrics. This is why, I greatly appreciate section 4.2, which analyses the different models' error types with respect to the proposed metrics. However, the first part of this analysis (Figure 3) could have been done almost independently (except for the 'case' error) without the new dataset and proposed metrics. \n\nThe Schubert Winterisse Dataset section accentuates the feeling of disconnection from the previous sections and a swift transition into a full benchmark/survey paper. It is mostly a section to compare the different ALT rather than a showcase of the benefits of using the new metrics and has no reference to their proposed guideline nor their dataset.\n\n\n\u2014 FINAL COMMENT \u2014\nThe paper has two faces. The first part presents the authors' revised version of JamendoLyrics and their metrics to capture common errors not identified by speech transcription metrics and related to important token types for lyrics. The second part focuses on benchmarking ALT models, this part is longer than the first part. Both of them are valuable and will provide useful insight to the community, but the second part feels disconnected and not well aligned with the title and abstract of the paper. I expected an analysis that emphasizes the benefits of the revised version and demonstrates how evaluating the previous version could lead to misunderstandings about the real performance of an ATL. After reading the paper, I am still unsure whether a model that had good results in the previous Jamendo produces a particular type of mistake that I can now spot with this new version. (It is clear that the line breaks and paragraphs weren't there before but it's never pointed out in the paper). On the other hand, the benefit of the new evaluation metrics is clear, since they directly indicate the type of error with respect to each token type, especially the line break and paragraphs.\n\nI think it also misses an opportunity to provide a tool to verify if a given set of lyrics follows the guidelines or not to assess its quality, as well as an initiative/encouragement to adapt other datasets.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-11-lyrics-transcription-for",
      "title": "Lyrics Transcription for Humans: A Readability-Aware Benchmark",
      "video": "https://drive.google.com/file/d/1aO3TyMqL8iw7MRan4-Z4PEMnpLMu7YOy/view?usp=drive_link"
    },
    "forum": "272",
    "id": "272",
    "pic_id": "https://drive.google.com/file/d/1KeD6qyfmisYMpKuxn_HOed85pqGOg-J2/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Vocal concerts in Indian music are invariably associated with the   performers\u2019 hand gesticulations that are believed to convey emotion, music semantics as well as the individual style of the   performers. Video recordings, with one or more cameras, along with markerless human pose estimation algorithms can be employed to capture such movements, and thus potentially solve music information retrieval (MIR) queries.  Nevertheless, off-the-shelf algorithms are built for the most part for upright human configurations contrasting with seated positions in Indian vocal concerts and the upper body movements in the context of performing music.  Current state-of-the-art algorithms are black box neural network based and this calls for an investigation of the components of such algorithms.  Key decisions involve the choice of one or more cameras, the choice of 2D or 3D features, and relevant parameters. such as confidence thresholds in common machine learning methods. In this paper, we quantify the increase in the performance with 3 cameras on two music information retrieval tasks. We offer insights for single and multi-view processing of videos.",
      "abstract": "Vocal concerts in Indian music are invariably associated with the   performers\u2019 hand gesticulations that are believed to convey emotion, music semantics as well as the individual style of the   performers. Video recordings, with one or more cameras, along with markerless human pose estimation algorithms can be employed to capture such movements, and thus potentially solve music information retrieval (MIR) queries.  Nevertheless, off-the-shelf algorithms are built for the most part for upright human configurations contrasting with seated positions in Indian vocal concerts and the upper body movements in the context of performing music.  Current state-of-the-art algorithms are black box neural network based and this calls for an investigation of the components of such algorithms.  Key decisions involve the choice of one or more cameras, the choice of 2D or 3D features, and relevant parameters. such as confidence thresholds in common machine learning methods. In this paper, we quantify the increase in the performance with 3 cameras on two music information retrieval tasks. We offer insights for single and multi-view processing of videos.",
      "author_changes": "",
      "authors": [
        "Roychowdhury, Sujoy*",
        " Rao, Preeti",
        " Chandran, Sharat"
      ],
      "authors_and_affil": [
        "Sujoy Roychowdhury (Indian Institute of Technology Bombay)*",
        " Preeti Rao (Indian Institute of Technology  Bombay)",
        " Sharat Chandran (IIT Bombay)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGNDRUL",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "Applications -> music videos, multimodal music systems"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1OXrs5ZPmgM95rN72ylqc5FbHJPEXtAsH/view?usp=drive_link",
      "poster_pdf": "https://docs.google.com/presentation/d/1GEkVj9WJaooeTeIAOfutmOe7hqlRTwfk/edit?usp=drive_link&ouid=114902126096780857875&rtpof=true&sd=true",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-02-human-pose-estimation",
      "title": "Human Pose Estimation for Expressive Movement Descriptors in Vocal Musical Performance",
      "video": "https://drive.google.com/file/d/1D4GZP-2qjiJbvs8So2KYz19bxlBP0rJy/view?usp=drive_link"
    },
    "forum": "273",
    "id": "273",
    "pic_id": "https://drive.google.com/file/d/1u2zPhPPa1LwJJDKbwOmEYtN2ajYkjzak/view?usp=drive_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.",
      "abstract": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.",
      "author_changes": "Apart from problems residing in arguments used or claims, most of the critique was into not providing more experimentation and testing some of the hypothesis/proposals left for future work. We need to highlight that the main topic of this paper is to evaluate the SOTA two-tower systems and establish a baseline (or absence). Our results alongside the performance reported from most of the authors for the two-tower systems proposed provide evidence that we are far from properly utilizing extra modalities. As a novel step, we tried to pinpoint the cause of this issue and found that the text branch or alignment is the main issue, as the audio encoders perform greatly. The 6 pages format was a hindering aspect to further provide more experiments and test some of our hypothesis for potential improvements over the problems stated and this is mainly the reason why we abstained from including results as they would decrese the value of this paper as an evaluation one.\n\nFigure 2 was criticized but we argue that it clearly show the following two takeaways: Changing prompts lead to almost erratic performance changes and using Audio only information almost doubles the performance.\n\nWe generally included pretty bold claims, and in some cases unjustified (as claiming that we have an issue with using two-tower systems for multi-label zero-shot classification). We changed the phrasing to imply that this is just a stepping stone and we cannot have a strong conclusion given those experiments. Despite this fact, we still think its a great way of establishing a more thorough way of evaluating Deep Learning systems and sheding more light in the reasons why these systems might fail apart from presenting a table with metrics.",
      "authors": [
        "Vasilakis, Yannis*",
        " Bittner, Rachel",
        " Pauwels, Johan"
      ],
      "authors_and_affil": [
        "Yannis Vasilakis (Queen Mary University of London)*",
        " Rachel Bittner (Spotify)",
        " Johan Pauwels (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM61SQ85",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "meta_review": "Most reviewers agree that the paper provides a good overview of the problem space with one or two comments regarding some additional work that may be included there. Overall the introduction and definition of the problem is well done. Most reviewer criticisms come down to the interpretation of results, and more incorporation of future work into the current work. Specifically, many of the conclusions drawn in the text are not clearly evident in the results shown, and reviewer's raise the need for a clearer discussion of results in the text that specifically is proven in the results presented.\n\nPlease take into careful consideration the reviewer's comments to improve this work for the camera ready version.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/view?usp=drive_link",
      "review_1": "Summary:\n\nThe paper discusses the applicability of multimodal audio-text for musical instrument classification. Such models enable a user to measure the 'distance' between an audio clip and a text prompt. The authors determine that such models are suitable for tasks such as instrument classification due to the scarcity of large datasets containing many labeled instruments as well as the fluidity of the output label space, i.e., new instruments might be needed to be added to a trained classifier.\nThe authors aim to assess the zero-shot capability of such models for the task, specifically probing the importance of the text prompt used, and the power of so-called pre-joint and joint embedding spaces of three models. Through various experiments, they find that models do not utilize text context instead focusing on the label word within the input prompt. They also find that the text branch of these models perform significantly poorly compared to the audio-only branch. The conclusion of this study is that alignment between text and audio is lacking in the chosen music-text joint models and should be the focus of future research.\n\nStrengths:\n- The paper is discussing research questions that are well aligned with latest trends of machine learning research. Multimodal models are being heavily studied and there is a surge of audio/music/text models. The findinds in this paper seem to be helpful in guiding future direction of research in the space.\n- The authors have done a good job organizing the paper and covering various research questions that arise when evaluating the zero-shot capabilities of the models.\n\nWeaknesses:\n- I am not sure why the authors refer to the two \"musically informed\" prompts as such. I do not really see any semantic difference between the MusCALL prompt and the #1 prompt.\n- I believe the study would have been complete if the authors chose to include one of the experiments that has been left as future work: using some kind of prompt upsampling/augmentation to inject stochasticity to the training of such models. It is well-known that such methods improve the generalizability of these models and thus the findings of this paper might potentially have changed significantly using more robustly trained models.",
      "review_2": "The paper compares three different algorithms for two-tower multimodal systems which produce a joint audio and text embedding based on text and audio encoders. Although the data set used is rather simple and not very large, the authors justify very well its selection.\n\nThe paper is well written, all necessary literature is discussed. After some backgrounds into methodology several experiments are described in detail and supported with figures. Also the problematic issues of the used system are reported.\n\nHowever, the statistical evaluation of experiments can be done in a more thorough way: e.g., statistics over prompts for n different instruments could be reported with standard deviations (cf. Figure 2) and also statistical tests could be applied for some quantitative measurements / comparison of prompts and methods. For instance, the statement that \"MusCALL prompt .. leads to the highest top-1 accuracy\" is not clear: for Music/Speech CLAP it can be observed, but for Music CLAP the values seem to be very close, and for MusCALL it is even below the most of other prompts. ",
      "review_3": "The authors provide a thorough, well-organized analysis of the pre-joint and joint audio-text embedding spaces in the context of musical instrument recognition. Thanks for your contributions!\n\nStrengths:\n- Related work section is very strong and covers all of the bases, which is especially great in an analysis paper!\n- Very clear motivation and setup of experimentation.\n- Figures 3 and 4 were really interesting and illustrated some of your key discussion points well!\n\nComments:\n- Figure 2 was hard to digest. (1) The inclusion of the joint and pre-joint audio does not fit as the figure mostly illustrates prompt comparisons and while it states in 3.3 that it will be explained exactly what these columns mean in 3.4, I never quite could understand what this represented? (2) Could be helpful to have the prompts on the same page as this figure or even somewhere in the figure, as looking at \"prompt 1...etc\" means nothing to the reader without flipping back and forth\n- I think more qualitative analysis and discussion could enhance your analysis even further. The discussion is mostly centered on metrics for instrument recognition (which also makes sense), but it would be great to have a bit more discussion of what you really saw (qualitatively) in the recognition when using the different embeddings, similar to the discussion you have surrounding the importance of prompt engineering in the text encoder. \n- The metric for semantic meaningfulness based on instrument ontology is quite interesting in theory, but Figure 5 shows that across systems the results were quite similar. Is there a way the metric could be modified to emphasize wider spreads in some way?\n",
      "session": [
        "6"
      ],
      "slack_channel": "p6-11-i-can-listen",
      "title": "I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition",
      "video": "https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/view?usp=drive_link"
    },
    "forum": "275",
    "id": "275",
    "pic_id": "https://drive.google.com/file/d/1Ku9p2tOaHugEDAx1tH0ervgBcpre2E4M/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent text-to-music models have enabled users to generate realistic audio music with a simple command. However, editing music audios remains challenging due to conflicting desiderata: performing fine-grained alterations on the audio while maintaining a simplistic user interface. To address this challenge, we propose Audio Prompt Adapter (or AP Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion text-to-music model. With only 22M trainable parameters, AP Adapter empowers users to harness both global (e.g., style and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP Adapter on three tasks: timbre transfer, style transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",
      "abstract": "Recent text-to-music models have enabled users to generate realistic audio music with a simple command. However, editing music audios remains challenging due to conflicting desiderata: performing fine-grained alterations on the audio while maintaining a simplistic user interface. To address this challenge, we propose Audio Prompt Adapter (or AP Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion text-to-music model. With only 22M trainable parameters, AP Adapter empowers users to harness both global (e.g., style and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP Adapter on three tasks: timbre transfer, style transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",
      "author_changes": "",
      "authors": [
        "Tsai, Fang Duo*",
        " Wu, Shih-Lun",
        " Kim, Haven",
        " Chen, Bo-Yu",
        " Cheng, Hao-Chung",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Fang Duo Tsai (National Taiwan University)*",
        " Shih-Lun Wu (Carnegie Mellon University)",
        " Haven Kim (University of California San Diego)",
        " Bo-Yu Chen (National Taiwan University, Rhythm Culture Corporation)",
        " Hao-Chung Cheng (National Taiwan University)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM61UR5K",
      "day": "2",
      "keywords": [
        "Creativity -> computational creativity; Generative Tasks -> music and audio synthesis; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR and machine learning for musical acoustics; MIR tasks -> music generation",
        "MIR tasks -> music synthesis and transformation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1dDdDgd5TVYl0UorNvzj4f__AftQ9RJRw/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/13QtDnOhEOCD40rDFaCuChfIwGpIuCqys/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-18-audio-prompt-adapter",
      "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning",
      "video": "https://drive.google.com/file/d/1PkwkRsbws4zUxxy_vp0pMi6LWueUjzTY/view?usp=drive_link"
    },
    "forum": "278",
    "id": "278",
    "pic_id": "https://drive.google.com/file/d/1QvWRJ4PyIWLrEsovRVVVOJyKHTSIkvWw/view?usp=drive_link",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the \"main tempo,\" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes \"renotations\" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",
      "abstract": "Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the \"main tempo,\" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes \"renotations\" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",
      "author_changes": "",
      "authors": [
        "Condit-Schultz, Nathaniel*"
      ],
      "authors_and_affil": [
        "Nathaniel Condit-Schultz (Georgia Institute of Technology)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCWLQG6",
      "day": "4",
      "keywords": [
        "Computational musicology",
        "Evaluation, datasets, and reproducibility -> annotation protocols; Knowledge-driven approaches to MIR -> cognitive MIR; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR tasks -> music transcription and annotation; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/15s3hLyFhSpdkSLDpThGQ5teU1LuPotu_/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1Nrh_OGw3wV0r-3FnXVEaoTD_PFEQXib4/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-03-looking-for-tactus",
      "title": "Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow",
      "video": "https://drive.google.com/file/d/1SdhM--el63CU_awSBoQ1atUIbIxzplxz/view?usp=drive_link"
    },
    "forum": "280",
    "id": "280",
    "pic_id": "https://drive.google.com/file/d/12WK4b1sZvF38l3zGomcJAZIixSt8sPvH/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking.   Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not.   We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions).   We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",
      "abstract": "In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking.   Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not.   We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions).   We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",
      "author_changes": "We would like to thank the reviewers and the meta-reviewer for their valuable feedback and suggestions.\n\nWe have corrected all the typos and restructured the paragraphs based on the reviewers' and meta-reviewer's remarks.\n\nWe added details about PLP computation.\n\nRegarding the comments from Reviewers #1 and #2, we explained our decision to replace the linear probe and also included a reference to the section where we describe how we fed the probing network.\n\nReviewer #1:\n    - Added a reference as requested.\n\nReviewer #2:\n    - Added details about the safety window.\n    - Added details about discarding audio segments.\n    - Added details about the time-varying factor for data augmentation.\n    - Clarified section 3.1.2 to make the tatum unit clearer.\n\nReviewer #3:\n    - Corrected the figure.",
      "authors": [
        "Gagner\u00e9, Antonin*",
        " Essid, Slim",
        " Peeters, Geoffroy"
      ],
      "authors_and_affil": [
        "Antonin Gagner\u00e9 (LTCI - T\u00e9l\u00e9com Paris, IP Paris)*",
        " Slim Essid (  LTCI - T\u00e9l\u00e9com Paris, IP Paris)",
        " Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGNN5C4",
      "day": "1",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "All four reviewers agree on the acceptance of the paper. However, it is remarkable that all reviewers pick up the same weak points of the submission, and the authors have to address these in terms of clear comments and explanations (please see the detailed formulations in the reviews):\n1. Restrictions due to the oversimplistic binary meter model.\n2. With PLP as a basis, how does the system performance depend on genre, tempo stability. Also, more detail on the PLP parameters must be provided.\nAs meta-reviewer I would also add that you should explain how statistical significance was judged. ",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ZCbNqs3QV7vhfH-HGAD6iaFzzZRvch_g/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1RWkcjSZM-yUW2tDEPU8_q42I7t6igv2Z/view?usp=drive_link",
      "review_1": "This paper describes a contrastive pretraining method for beat tracking. The methods are highly novel. The use of a PLP for contrastive sample mining is very interesting and seemingly effective. The experiments are comprehensive on several datasets on both few-shot and full fine-tuning tasks.\n\nSome questions:\n\nMethodology:\nThe introduction of PLP helps solve the task of mining positive samples in beat tracking. The idea is highly novel and interesting. There seem to be some concerns about PLP: (1) How accurate it is for different genres? Will it be reasonably accurate if the music has rapid local tempo changes? (2) How likely are they to form binary-segmented tatums (i.e., 8-th notes) instead of ternary ones (i.e., 12-th notes)? 1-2 lines of description of preliminary experiments would help people get more sense of the feature.\n\nExperiments:\nSection 4.5: \"Instead of ... of layer sequences.\" What is the weighted sum of layer sequences? Also, why choose a different fine-tuning scheme? Is linear probing not as good? Or, is there a specified reason?\n\nRelated works:\nThe idea of utilizing the binary rhythmic structure for self-supervised learning was previously used in [1].\n\nThe methodology of the paper could be beneficial to other rhythm-related downstream tasks, or even general pretraining models for MIR. Considering the novelty of the method and the concrete results, I recommend a strong acceptance with the possibility of an award nomination.\n\n[1] Jiang, J., & Xia, G. (2023, June). Self-Supervised Hierarchical Metrical Structure Modeling. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.\n",
      "review_2": "This work proposed a novel scheme to pretrain a network in a self-supervised manner using contrastive loss and finetune the network for beat tracking with few-shot learning; the results are comparable to the SOTA supervised models and nicely demonstrate the effectiveness of the methods. Considering that there are not many works for self-supervised beat tracking, and that the proposed methods are novel and perform promising, I originally would like to give a strong accept. However, due to the issues I mentioned in the reusable insight section and the following minor flaws, I have to weaken my acceptance to this work. \n\n- Line 173: What does this binary structure mean here?\n- Line 182:This description is repeated several times in different ways; but the unit of the distance is not clear until this sentence. For example, Line 13,  line 165-169.\n- Line 409-416: The observation that the proposed model performs much better on Hainsworth is worthy of more discussions. Is it because that the model learns different knowledges or because of some special properties of Hainsworth? Without discussion, the reusable insight of this improvement is limited.\n- Line 462: rewrite the sentence: \" where our the selection of anchor, positive and negative peaks derives from a Predominant Local Pulse function.\"\n- Line 236: alpha = 4 tu?\n\n\n",
      "review_3": "The paper proposes a novel self-supervised learning (SSL) approach for rhythm analysis and tackles few-shot beat tracking. The learning strategy is based on contrastive learning and the pre-text task samples anchor, positive, and negative points from Predominant Local Pulse (PLP) maxima. These samples are used to train and encoder to contrast observations at beats (actually multiples of the tatum) from those that are not, using unlabelled data from FMA, MTT and MTG-Jamendo datasets. The pre-trained model is then fine-tuned with just a few annotated examples (i.e. few-shot) on the beat-tracking task. Results show the proposed approach outperforms an existing SSL method (Zero-Note Samba) and yiedls competitive results when compared to state-of-the-art supervised models.\n\nThe paper is well written and organized (despite some minor corrections listed below) and is a very good contribution to ISMIR. \n\nThe proposal has some clear limitations, though. First, the binary structure hypothesis does not hold for many music styles. Moreover, the sampling from the PLP makes the asumption that peaks are synchronized with 8-th notes and that the tracks are in 4/4. The authors acknowledge these are over-simplistic assumptions (line 174), but I encourage them to add some comments on how the proposal could be extended to non-binary music structures. One could think of other sampling strategies that may account for other meters, but this would probably require meter classification. \n\nIn adddition, relying on the PLP function has some drawbacks. Particularly, it can be noisy when there are tempo fluctuations or sudden tempo changes. The proposal deals with this kind of situation by filtering out tracks where the inter-peak distance of the PLP function is not almost constant (lines 375-380). Then, time-varing tempo is synthetically introduced through data augmentation during training.  This raises the question of to what extent the model can deal with tempo fluctuations within a song, so some insights on that would be welcome. \n\nExperimental results confirm that the pre-trained model after fine-tuning can produce very competitive results, which seems to confirm that for the music datasets considered the assumptions are correct. Nevertheless, it would be interesting to test the model with music for which the over-simplistic assumptions do not hold and perform a detailed analysis of the results.  \n\nThere is no supplementary material, so I could not access the code, but the paper indicates that it will be available. It would be important that the pre-trained models and the code for the experiments are available to enhance scientific reproducibility.\n\nMinor corrections\n\nIt seems that beat and tatum are never defined. A short clarification would be nice. \n\nLine 35 - \"at most a few thousands ...\" refers to data, so something is missing here.\n\nNote that there is an error in Figure 2. The time corresponding to yp should be in Ya, which means should be ya+ix\\alpha, but in the diagram of Figure 2 it is located in between ya+2\\alpha and ya+3\\alpha. \n\nLine 278 - typo: \"thee\" -> \"the\"\n\nLine 294 - remove \"performances\"\n\nLine 461 - \"our\" should be removed",
      "session": [
        "1"
      ],
      "slack_channel": "p1-19-a-contrastive-self",
      "title": "A Contrastive Self-Supervised Learning scheme for beat tracking amenable to few-shot learning",
      "video": "https://drive.google.com/file/d/1dBlcd_jNwcnBDoKOC1YZOkYYb90I1aSE/view?usp=drive_link"
    },
    "forum": "283",
    "id": "283",
    "pic_id": "https://drive.google.com/file/d/1BPmJ_BnweNHfHsrmFpmCzPF3Q7SdoMh9/view?usp=drive_link",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",
      "abstract": "Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",
      "author_changes": "Thank you all for your reviews. Please find below the changes we have made to take them into account.\n\nAs proposed, we modified the title to reflect the fact that we evaluate our approach on drum music.\n\nWe added a reference to get further insight on formal grammar and context-free grammar in the related section.\n\nWe clarified the Figure 2 and the associated text in order to make it more easily understandable.\n\nWe also explained that model hyperparameters, including depth and width of the architectures, as well as the number of bars considered for each encoding were obtained using a hyperparameter search on the validation set.\n\nFurthermore, we also added a note explaining that while we are indeed limited by some of qparse limitations, we aim at evaluating the rhythmic tree representation, regardless of how it has been built. Researching more robust ways of building such trees would be suited for some follow-up research.\n\nIn the supplementary material, we added some details on the grammar we used, and how the tree is built.\n\nAs proposed, we also added the confusion matrix of one of the models trained with the best architecture in the supplementary material, with some succinct interpretation.",
      "authors": [
        "G\u00e9r\u00e9, L\u00e9o*",
        " Audebert, Nicolas",
        " Rigaux, Philippe"
      ],
      "authors_and_affil": [
        "L\u00e9o G\u00e9r\u00e9 (Cnam)*",
        " Nicolas Audebert (IGN)",
        " Philippe Rigaux (Cnam)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F8BYB1",
      "day": "2",
      "keywords": [
        "Knowledge-driven approaches to MIR; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> representations of music",
        "MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The discussion of this paper was by and large positive. There is some question of the generality of the representation, and whether the rules extracted by qparse are sufficiently general to allow lots of different rhythmic possibilities. Its proximity to the qparse implementation is also concerning. The authors should spend some time improving their paper long the several directions identified by the reviewers.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13ylnqtKDbEBOQxFsCRlpaGPuK-c3RAPH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1d-G3JwglvXOhAmH8jcKVcKVuC6FRoSND/view?usp=drive_link",
      "review_1": "Summary: The authors develop a new tree-based MIDI representation of drumming data (linear rhythmic tree) and test it with 3 other representational models and present their results.  The LRT does the best.\n\nStrengths: This is a very good paper with reproducible results.  The major novel idea is the LRT.  I like that the new model is tested against other models as well, both with LSTMs and transformers.\n\nWeaknesses: Nothing is incredibly ground-breaking or earth-shattering. The results are incremental.",
      "review_2": "This paper uses grammatical representation of MIDI files for style classification. Experimental results show that comparable or significantly better performance is achieved using less model parameters. The description and results are convincing, but explaining more details could make this paper better.\n\n- Line #165: Please define \u201coptimization-based music transcription systems\u201d. Does it mean \u201ca music transcription system that invokes optimization algorithm\u201d? If yes, then deep models (line 168) are one type of such system, but in this sentence (While designed \u2026) they are viewed as different.\n- Line #299: Please give the durations of these sets if they are not roughly proportional to the original durations.\n- Line #381: Please give specs of CPU and GPU. Elapsed times are not meaningful enough if specs are unknown.\n",
      "review_3": "This paper presents a new representation for quantized MIDI files based on principles from a library called qparse. The approach is very interesting and the paper well-written. However, the paper also raises a number of questions that were left partly unanswered:\n- Partial Evaluation: The task is evaluated based on a single specific task without considering alternative approaches. The authors focus solely on this task, comparing only their own results. Evaluating the method against an established task and existing approaches would significantly strengthen the case for introducing a novel data representation.\n- Hard to interpret experiment tables: Table 1 contains a configuration study but the choice of bars and parameter size is not very intuitive. The choice of bars across tokenizations and models for example seems a bit arbitrary. Figure 4 and 5 also follow the same ambiguity, and raise the question of why the proposed representation using a transformer is compared to an LSTM, I would argue that some justification is missing.\n- Relevance of the Task: The task chosen for evaluation is too simplistic to effectively demonstrate a completely new representation, raising doubts about the general applicability of the new representation.\n- Generality of the representation: The new representation is very simple and clear but it does not cover all (or even broadly enough) musical durations such as composite durations, dotted notes, tuplets other than triplets, tied notes, etc.",
      "session": [
        "4"
      ],
      "slack_channel": "p4-11-improved-symbolic-drum",
      "title": "Improved symbolic drum style classification with grammar-based hierarchical representations",
      "video": "https://drive.google.com/file/d/1D8zWhXRLWjWukMzDXT6DLO8UKfg8IaO4/view?usp=drive_link"
    },
    "forum": "293",
    "id": "293",
    "pic_id": "https://drive.google.com/file/d/14RvMqnf57mymZFZ5vRsTRTR4aCVFpJ5Z/view?usp=drive_link",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice  performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",
      "abstract": "Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice  performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",
      "author_changes": "",
      "authors": [
        "Narang, Jyoti*",
        " Tamer, Nazif Can",
        " De La Vega, Viviana",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Jyoti Narang (Student)*",
        " Nazif Can Tamer (Universitat Pompeu Fabra)",
        " Viviana De La Vega (Escola Superior de M\u00fasica de Catalunya)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MW9G57",
      "day": "1",
      "keywords": [
        "Applications -> music training and education; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> automatic classification; MIR tasks -> music transcription and annotation; Musical features and properties -> representations of music",
        "Evaluation, datasets, and reproducibility -> annotation protocols"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1z976v11q-NVjF0m2I5cFTHqTVSwpA2mb/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/191TJ7easeXGMU9D4CsMKxHf4ENkISQfi/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-06-automatic-estimation-of",
      "title": "Automatic Estimation of Singing Voice Musical Dynamics",
      "video": "https://drive.google.com/file/d/1Rcn5m3PbMitV5QXrqKU9jmk9TG15t9gb/view?usp=drive_link"
    },
    "forum": "296",
    "id": "296",
    "pic_id": "https://drive.google.com/file/d/1tdp3X2iMEOhLyebL7aH4akayiPcMKB9T/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Mixing style transfer automates the generation of a multitrack mix for a given set of tracks by inferring production attributes from a reference song. However, existing systems for mixing style transfer are limited in that they often operate only on a fixed number of tracks, introduce artifacts, and produce mixes in an end-to-end fashion, without grounding in traditional audio effects, prohibiting interpretability and controllability. To overcome these challenges, we introduce Diff-MST, a framework comprising a differentiable mixing console, a transformer controller, and an audio production style loss function. By inputting raw tracks and a reference song, our model estimates control parameters for audio effects within a differentiable mixing console, producing high-quality mixes and enabling post-hoc adjustments. Moreover, our architecture supports an arbitrary number of input tracks without source labelling, enabling real-world applications. We evaluate our model's performance against robust baselines and showcase the effectiveness of our approach, architectural design, tailored audio production style loss, and innovative training methodology for the given task. We provide code, pre-trained models, and listening examples online.",
      "abstract": "Mixing style transfer automates the generation of a multitrack mix for a given set of tracks by inferring production attributes from a reference song. However, existing systems for mixing style transfer are limited in that they often operate only on a fixed number of tracks, introduce artifacts, and produce mixes in an end-to-end fashion, without grounding in traditional audio effects, prohibiting interpretability and controllability. To overcome these challenges, we introduce Diff-MST, a framework comprising a differentiable mixing console, a transformer controller, and an audio production style loss function. By inputting raw tracks and a reference song, our model estimates control parameters for audio effects within a differentiable mixing console, producing high-quality mixes and enabling post-hoc adjustments. Moreover, our architecture supports an arbitrary number of input tracks without source labelling, enabling real-world applications. We evaluate our model's performance against robust baselines and showcase the effectiveness of our approach, architectural design, tailored audio production style loss, and innovative training methodology for the given task. We provide code, pre-trained models, and listening examples online.",
      "author_changes": "",
      "authors": [
        "Vanka, Soumya Sai*",
        " Steinmetz, Christian J.",
        " Rolland, Jean-Baptiste",
        " Reiss, Joshua D.",
        " Fazekas, George"
      ],
      "authors_and_affil": [
        "Soumya Sai Vanka (QMUL)*",
        " Christian J. Steinmetz (Queen Mary University of London)",
        " Jean-Baptiste Rolland (Steinberg Media Technologies GmbH)",
        " Joshua D. Reiss (Queen Mary University of London)",
        " George Fazekas (QMUL)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCR6DKK2",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production; Generative Tasks -> artistically-inspired generative tasks ; MIR fundamentals and methodology -> music signal processing",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/16yvnUj2NZx2-4Dw9HX6OqBemRkT_-NBj/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1NJEN9xBHhcLPUYWEGRV8LTxD-1p6DzYY/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-08-diff-mst-differentiable",
      "title": "Diff-MST: Differentiable Mixing Style Transfer",
      "video": "https://drive.google.com/file/d/1tUSUuB2osgTX5-ME8kf_5SmE-s43mr25/view?usp=drive_link"
    },
    "forum": "299",
    "id": "299",
    "pic_id": "https://drive.google.com/file/d/14G-uNVzQEu1aG6MEMjq2Kj9DzXXTfka8/view?usp=drive_link",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",
      "abstract": "Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",
      "author_changes": "",
      "authors": [
        "Koops, Hendrik Vincent*",
        " Micchi, Gianluca",
        " Quinton, Elio"
      ],
      "authors_and_affil": [
        "Hendrik Vincent Koops (Universal Music Group)*",
        " Gianluca Micchi (Universal Music Group)",
        " Elio Quinton (Universal Music Group)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHEM5UCE",
      "day": "3",
      "keywords": [
        "MIR tasks -> automatic classification",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1a7Gf7efa-MPguct1Cfnz6Dnqr_adqbMH/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1azt9YjZHFULXu2BWk6FQ1413hSq3EIIu/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-16-robust-lossy-audio",
      "title": "Robust lossy audio compression identification",
      "video": "https://drive.google.com/file/d/1X8L6NyjPn0YTHVnGCtWLxm4UCZHDXQlU/view?usp=drive_link"
    },
    "forum": "304",
    "id": "304",
    "pic_id": "https://drive.google.com/file/d/10BWHAZtK81FDktXcLFQLNUXNaBiBla4J/view?usp=drive_link",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Are composers\u2019 emotional intentions conveyed to listeners through audio features? In the field of Music Emotion Recognition (MER), recent efforts have been made to predict listeners' time-varying perceived emotions using machine-learning models. However, interpreting these models has been challenging due to their black-box nature. To increase the explainability of models for subjective emotional experiences, we focus on composers\u2019 emotional intentions. Our study aims to determine which audio features effectively predict both composers' time-varying emotions and listeners' perceived emotions. Seven composers performed 18 piano improvisations expressing three types of emotions (joy/happiness, sadness, and anger), which were then listened to by 36 participants in a laboratory setting. Both composers and listeners continuously assessed the emotional valence of the music clips on a 9-point scale (1: 'very negative' to 9: 'very positive'). Linear mixed-effect models analysis revealed that listeners significantly perceived the composers' intended emotions. Regarding audio features, the RMS was found to modulate the degree to which the listener's perceived emotion resembled the composer's emotion across all emotions. Moreover, the significant audio features that influenced this relationship varied depending on the emotion type. We propose that audio features related to the emotional responses of composers-listeners can be considered key factors in predicting listeners' emotional responses.",
      "abstract": "Are composers\u2019 emotional intentions conveyed to listeners through audio features? In the field of Music Emotion Recognition (MER), recent efforts have been made to predict listeners' time-varying perceived emotions using machine-learning models. However, interpreting these models has been challenging due to their black-box nature. To increase the explainability of models for subjective emotional experiences, we focus on composers\u2019 emotional intentions. Our study aims to determine which audio features effectively predict both composers' time-varying emotions and listeners' perceived emotions. Seven composers performed 18 piano improvisations expressing three types of emotions (joy/happiness, sadness, and anger), which were then listened to by 36 participants in a laboratory setting. Both composers and listeners continuously assessed the emotional valence of the music clips on a 9-point scale (1: 'very negative' to 9: 'very positive'). Linear mixed-effect models analysis revealed that listeners significantly perceived the composers' intended emotions. Regarding audio features, the RMS was found to modulate the degree to which the listener's perceived emotion resembled the composer's emotion across all emotions. Moreover, the significant audio features that influenced this relationship varied depending on the emotion type. We propose that audio features related to the emotional responses of composers-listeners can be considered key factors in predicting listeners' emotional responses.",
      "author_changes": "",
      "authors": [
        "Oh, Eun Ji",
        " Kim, Hyunjae",
        " Lee, Kyung Myun*"
      ],
      "authors_and_affil": [
        "Eun Ji Oh (KAIST)",
        " Hyunjae Kim (KAIST)",
        " Kyung Myun Lee (KAIST)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ3CHBQA",
      "day": "1",
      "keywords": [
        "Musical features and properties -> musical affect, emotion and mood",
        "Human-centered MIR -> user behavior analysis and mining, user modeling; Human-centered MIR -> user-centered evaluation; Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1H6zjSdJ59mbyeEbN_4pd_4532fUJzNIU/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/17rWRaBd_DBuKdxFvvyaH0f9ILzAhRcgv/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-19-which-audio-features",
      "title": "Which audio features can predict the dynamic musical emotions of both composers and listeners?",
      "video": "https://drive.google.com/file/d/1lBkImXL2vrWGy3IQCwU5_f-SbAoJF3vs/view?usp=sharing"
    },
    "forum": "305",
    "id": "305",
    "pic_id": "https://drive.google.com/file/d/19BjCq9cRrQD_HWM3q-gzMv4wxHcY07r4/view?usp=drive_link",
    "position": "20",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach.  Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility\u2014retrieving, aligning, or generating a stem to match a given mix\u2014or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm.  We evaluate our model\u2019s performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.",
      "abstract": "This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach.  Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility\u2014retrieving, aligning, or generating a stem to match a given mix\u2014or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm.  We evaluate our model\u2019s performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.",
      "author_changes": "",
      "authors": [
        "Riou, Alain",
        " Lattner, Stefan",
        " Hadjeres, Ga\u00ebtan*",
        " Anslow, Michael",
        " Peeters, Geoffroy"
      ],
      "authors_and_affil": [
        "Alain Riou (Sony CSL Paris)",
        " Stefan Lattner (Sony Computer Science Laboratories, Paris)",
        " Ga\u00ebtan Hadjeres (Sony CSL)*",
        " Michael Anslow (Sony Computer Science Laboratories, Paris)",
        " Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGP2VN0",
      "day": "2",
      "keywords": [
        "Knowledge-driven approaches to MIR -> representations of music; Musical features and properties; Musical features and properties -> representations of music",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1nlqY1ruyjVURT8cD0cH5Avzhdz5JDGQO/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1WSTUYMmEIA5COiQycZITf_3HGiPp85Fp/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-17-stem-jepa-a",
      "title": "Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation",
      "video": "https://drive.google.com/file/d/13Tz5oxvfM9iJaOQQyYS64smicMkURT3e/view?usp=sharing"
    },
    "forum": "306",
    "id": "306",
    "pic_id": "https://drive.google.com/file/d/1Y6n_osKu4o_VOQizLTcrCaOc-bk_TJbT/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "We introduce Cadenza, a new multi-stage generative framework for predicting expressive variations of symbolic musical ideas as well as unconditional generations. To accomplish this we introduce a novel MIDI encoding method, PerTok (Performance Tokenizer) that captures minute expressive details whilst maintaining short sequence length and vocabulary sizes for polyphonic, monophonic and rhythmic tasks. The proposed framework comprises two sequential stages: 1) Composer and 2) Performer. The Composer model is a transformer-based Variational Autoencoder (VAE), with Rotary Positional Embeddings (RoPE) and an autoregressive decoder modified to more effectively integrate the latent codes of the input musical idea. The Performer model is a bidirectional transformer encoder that is separately trained to predict velocities and microtimings on MIDI sequences. Extensive human evaluations demonstrate Cadenza's versatile capabilities in both meeting and surpassing the musical quality of other state-of-the-art symbolic models in unconditional generation, and secondly, composing new, expressive ideas that are both stylistically related to the input whilst providing novel ideas to the user. Our framework is designed, researched and implemented with the objective of ethically providing inspiration for musicians.",
      "abstract": "We introduce Cadenza, a new multi-stage generative framework for predicting expressive variations of symbolic musical ideas as well as unconditional generations. To accomplish this we introduce a novel MIDI encoding method, PerTok (Performance Tokenizer) that captures minute expressive details whilst maintaining short sequence length and vocabulary sizes for polyphonic, monophonic and rhythmic tasks. The proposed framework comprises two sequential stages: 1) Composer and 2) Performer. The Composer model is a transformer-based Variational Autoencoder (VAE), with Rotary Positional Embeddings (RoPE) and an autoregressive decoder modified to more effectively integrate the latent codes of the input musical idea. The Performer model is a bidirectional transformer encoder that is separately trained to predict velocities and microtimings on MIDI sequences. Extensive human evaluations demonstrate Cadenza's versatile capabilities in both meeting and surpassing the musical quality of other state-of-the-art symbolic models in unconditional generation, and secondly, composing new, expressive ideas that are both stylistically related to the input whilst providing novel ideas to the user. Our framework is designed, researched and implemented with the objective of ethically providing inspiration for musicians.",
      "author_changes": "",
      "authors": [
        "Lenz, Julian",
        " Mani, Anirudh*"
      ],
      "authors_and_affil": [
        "Julian Lenz (Lemonaide )",
        " Anirudh Mani (Lemonaide)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USJDDWHJ",
      "day": "4",
      "keywords": [
        "Creativity -> human-ai co-creativity; Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> transformations; Musical features and properties -> representations of music",
        "Applications -> music composition, performance, and production"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Eg3tB-462vsXvSvZHSwJiBZBjiFeNofw/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1H-1dKOrp8XcAPWIJTjL0uNCZ7UtzVEOA/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-02-cadenza-a-generative",
      "title": "CADENZA: A Generative Framework for Expressive Musical Ideas and Variations",
      "video": "https://drive.google.com/file/d/1FWq2xYed6xqkDLhIsrTFfQvf6E2YhRol/view?usp=sharing"
    },
    "forum": "315",
    "id": "315",
    "pic_id": "https://drive.google.com/file/d/1cEmfh9XOnZbv6zmv-30c5wj9eqsT_n9c/view?usp=drive_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Deep generative models are now able to synthesize high-quality audio signals, shifting the critical aspect in their development from audio quality to control capabilities. Although text-to-music generation is getting largely adopted by the general public, explicit control and example-based style transfer are more adequate modalities to capture the intents of artists and musicians.   In this paper, we aim to unify explicit control and style transfer within a single model by separating local and global information to capture musical structure and timbre respectively. To do so, we leverage the capabilities of diffusion autoencoders to extract semantic features, in order to build two representation spaces. We enforce disentanglement between those spaces using an adversarial criterion and a two-stage training strategy. Our resulting model can generate audio matching a timbre target, while specifying structure either with explicit controls or through another audio example. We evaluate our model on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings and show that we outperform existing baselines in terms of audio quality and target fidelity. Furthermore, we show that our method can generate cover versions of complete musical pieces by transferring rhythmic and melodic content to the style of a target audio in a different genre.",
      "abstract": "Deep generative models are now able to synthesize high-quality audio signals, shifting the critical aspect in their development from audio quality to control capabilities. Although text-to-music generation is getting largely adopted by the general public, explicit control and example-based style transfer are more adequate modalities to capture the intents of artists and musicians.   In this paper, we aim to unify explicit control and style transfer within a single model by separating local and global information to capture musical structure and timbre respectively. To do so, we leverage the capabilities of diffusion autoencoders to extract semantic features, in order to build two representation spaces. We enforce disentanglement between those spaces using an adversarial criterion and a two-stage training strategy. Our resulting model can generate audio matching a timbre target, while specifying structure either with explicit controls or through another audio example. We evaluate our model on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings and show that we outperform existing baselines in terms of audio quality and target fidelity. Furthermore, we show that our method can generate cover versions of complete musical pieces by transferring rhythmic and melodic content to the style of a target audio in a different genre.",
      "author_changes": "",
      "authors": [
        "Demerl\u00e9, Nils*",
        " Esling, Philippe ",
        " Doras, Guillaume",
        " Genova, David"
      ],
      "authors_and_affil": [
        "Nils Demerl\u00e9 (IRCAM)*",
        " Philippe  Esling (IRCAM)",
        " Guillaume Doras (Ircam)",
        " David Genova (Ircam)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USGP5JMA",
      "day": "3",
      "keywords": [
        "Applications -> music composition, performance, and production; Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> interactions; Generative Tasks -> transformations",
        "Generative Tasks -> music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/14OoxZjA_1WmLAWntlWCimXWXb87q8lU_/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1t8X5IzAD2SbYWxEnY8AFEWvUR_rJ7mEy/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-08-combining-audio-control",
      "title": "Combining audio control and style transfer using latent diffusion",
      "video": "https://drive.google.com/file/d/1f9uMz_tRnnTpj-_TFScB7IMZIw4J-4R4/view?usp=drive_link"
    },
    "forum": "316",
    "id": "316",
    "pic_id": "https://drive.google.com/file/d/1L4upF6s4rMeCJ-NfTBB5KZ1d3jmsNSQG/view?usp=drive_link",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "The ListenBrainz listens dataset is a continually evolv- ing repository of music listening history events submitted by all ListenBrainz users. Currently totalling over 800 million entries, each datum within the dataset encapsu- lates a timestamp, a pseudonymous user identifier, track metadata, and optionally MusicBrainz identifiers facilitat- ing seamless linkage to external resources and datasets. This paper discusses the process of raw data acquisition, the subsequent steps of data synthesis and cleaning, the comprehensive contents of the refined dataset, and the di- verse potential applications of this invaluable resource. Al- though not the largest dataset in terms of music listening events (yet), its distinctiveness lies in its perpetual evolu- tion, with users contributing data daily. This paper under- scores the significance of the ListenBrainz listens dataset as a significant asset for researchers and practitioners alike, offering insights into music consumption patterns, user preferences, and avenues for further exploration in the fields of music information retrieval and recommendation systems.",
      "abstract": "The ListenBrainz listens dataset is a continually evolv- ing repository of music listening history events submitted by all ListenBrainz users. Currently totalling over 800 million entries, each datum within the dataset encapsu- lates a timestamp, a pseudonymous user identifier, track metadata, and optionally MusicBrainz identifiers facilitat- ing seamless linkage to external resources and datasets. This paper discusses the process of raw data acquisition, the subsequent steps of data synthesis and cleaning, the comprehensive contents of the refined dataset, and the di- verse potential applications of this invaluable resource. Al- though not the largest dataset in terms of music listening events (yet), its distinctiveness lies in its perpetual evolu- tion, with users contributing data daily. This paper under- scores the significance of the ListenBrainz listens dataset as a significant asset for researchers and practitioners alike, offering insights into music consumption patterns, user preferences, and avenues for further exploration in the fields of music information retrieval and recommendation systems.",
      "author_changes": "",
      "authors": [
        "Ohri, Kartik*",
        " Kaye, Robert"
      ],
      "authors_and_affil": [
        "Kartik Ohri (MetaBrainz Foundation Inc.)*",
        " Robert Kaye (MetaBrainz Foundation Inc.)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUHDYF5",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Applications -> digital libraries and archives; Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Dng1gCcWZzhNoohvmfIeV0pjkcpcXvV3/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/13M9Rm068n5C025eqBMzGE2lj360mL6ur/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-06-the-listenbrainz-listens",
      "title": "The ListenBrainz Listens Dataset",
      "video": "https://drive.google.com/file/d/1n4It45Un21eMOFehfIeOYa-CPUpipUYH/view?usp=drive_link"
    },
    "forum": "317",
    "id": "317",
    "pic_id": "https://drive.google.com/file/d/1X4n3yRyuXSuY15IwllmlsgtdrMKk7xIm/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pretraining on unlabeled data is therefore a promising means of improving performance on these tasks, especially because, during pretraining, a model may be expected to acquire latent representations of musical abstractions like keys and chords. However, existing deep learning models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks not involving explicit music theory, like composer or genre classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to predict labels for each note, then aggregate the predictions of simultaneous notes to obtain a single label at each time step. Conditioning the chord predictions on key predictions gives more coherent labels. The resulting model outperforms previous roman numeral analysis models by a substantial margin.",
      "abstract": "Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pretraining on unlabeled data is therefore a promising means of improving performance on these tasks, especially because, during pretraining, a model may be expected to acquire latent representations of musical abstractions like keys and chords. However, existing deep learning models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks not involving explicit music theory, like composer or genre classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to predict labels for each note, then aggregate the predictions of simultaneous notes to obtain a single label at each time step. Conditioning the chord predictions on key predictions gives more coherent labels. The resulting model outperforms previous roman numeral analysis models by a substantial margin.",
      "author_changes": "I respond to the main reviewer comments below. The paper has been updated to address issues of formatting, grammar, etc., without further comment here.\n\n# Reviewer 1\n\n> I am looking for a in-depth discussion on the choice of MusicBERT\n\nI added an explanation of why I chose to use MusicBERT. I agree that a comparative evaluation of MusicBERT with other models would be useful, but that will await future work. In any case, the main contribution of this paper is the general approach of using a pre-trained masked language model for token-level music theory tasks, which to our knowledge has not previously been applied to Roman numeral analysis or any similar task.\n\n> a model diagram illustrating its training and inference will be helpful\n\nUnfortunately we do not have space to add this while remaining within the space constraints of an ISMIR paper.\n\n# Reviewer 2\n\n> There isn't adequate justification about why the first 9 layers of BERT are frozen specifically. It would have been interesting to experiment with different layer freezing, and even parameter- and resource-efficient fine-tuning methods like LoRA.\n\nI now state how and why I chose to freeze the first 9 layers. I agree that LoRA is worth trying, but that will await future work.\n\n> acknowledge some compute-related information of your experiments, such as the finetuning and inference cost, in relation to existing work (at least through the parameter counts).\n\nThank you for mentioning this oversight. I added parameter counts.\n\n# Reviewer 3\n\n>  The claim that predicting the key is trivial warrants reevaluation.\n\nThe reviewer may have misunderstood one small claim. Key prediction is not trivial. What I suggest is \"trivial\" is the task of predicting a spelled key (like \"Db major\") given\n\n- an unspelled key (like \"1 major\" for a major key with tonic pc 1), and\n- spelled pitch inputs (like Db5 and Ab5) rather than unspelled inputs (like MIDInums 61 and 68).\n\nI revised the passage in question to clarify it with examples.",
      "authors": [
        "Sailor, Malcolm*"
      ],
      "authors_and_affil": [
        "Malcolm Sailor (Yale University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCXAFE2",
      "day": "3",
      "keywords": [
        "Computational musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; Musical features and properties -> harmony, chords and tonality",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "All of the reviwers are concerned about the lack of detail and evaluation of the fine-tuning process. We encourage the authors to expand this is any revision of the paper.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Gqyk2QoOrzJXOPd_LBgchAsnBDJxG6cz/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1shpkSE2PSIr-u-vcBxTryredYcCwPBRu/view?usp=drive_link",
      "review_1": "The idea of this paper is simple: using a pre-trained model for symbolic music, MusicBERT, to improve the performance for roman numeral analysis. Although it achieves better performances than previous approaches, I am looking for a in-depth discussion on the choice of MusicBERT: why it instead of other similar models like MidiBERT-Piano and others? This is an essential question that the authors need to answer in the camera-ready version (if it comes to that) and even better, if authors conduct a comparative study on how the choice of different pre-trained models (including MusicBERT) affects the performance of roman numeral analysis. Althought the latter may require extra experiments that won't make it to this paper, I strongly suggest the author at least present a reasonable review on pre-trained models for symbolic music and the choice of MusicBERT. Without it, this work won't be considered a solid contribution to the community.\n\nOther than that, this paper is generally easy to follow but lacks polishing in some occasions. For example, the formatting of the references is bad, and the caption of Table 1 is overly short without meaningful descriptions, hence lacking a period in the end. Also, a model diagram illustrating its training and inference will be helpful, especially on the post-processing part in Section 3.7, which I find difficult to follow. Although it is nice to open-source the code, as indicated in Footnote 1, the authors should be more discreet about anonymity as the username embedded in the github URL will easily give away the identity of this submission. There are also grammatical errors shown as follow. \n\nLine 85: [14] add -> [14] adds\nLine 103: including by -> including\n\nOverall, I will give weak accept for this paper, on the condition that (1) the authors work on the explanation of why MusicBERT is chosen and (2) polish its content further to make it a competent ISMIR publication.  ",
      "review_2": "Overall, this is a very good, precisely written, scientifically sound, and interesting paper. The general approach of finetuning for token classification itself is certainly not the most novel idea. However, the authors carefully consider and describe all parts involved to achieve that, from the dataset, preprocessing, and tokenization to postprocessing and other interesting tweaks (e.g. key conditioning). I was pleased to have each doubt that arose immediately answered in the paper through the side-experiments mentioned and ablations that were presented. Each part of the methodology is precisely described and design choices are justified from both a scientific and music-theoretic perspective. Results are also carefully and interestingly described from these lenses.\n\nThe writing is clear and precise and the paper is organized well. It is perhaps slightly unusual to see the experimental setup subsections in this order (usually I'd expect to move from model to data), but it works well and, importantly, there are no redundancies or ambiguities.\n\nIt would have been interesting to see a bit more work (and perhaps detail in the paper) in the choice of finetuning. There isn't adequate justification about why the first 9 layers of BERT are frozen specifically. It would have been interesting to experiment with different layer freezing, and even parameter- and resource-efficient fine-tuning methods like LoRA. All this is particularly interesting because of existing indications about how different the information between transformers' layers can be. In \"Comparative Layer-Wise Analysis of Self-Supervised Speech Models\u201d by Pasad et al., for example, layer similarity of voice models with phonemes and words is computed, yielding interesting results about the \"location\" of relevant features. It's not trivial to adapt this framework to symbolic music and RN analysis, but it would certainly be interesting.\n\nIt would have been nice (and it's still possible for the camera-ready) to acknowledge some compute-related information of your experiments, such as the finetuning and inference cost, in relation to existing work (at least through the parameter counts).\n\nThe citations are not well formatted and would need to be fixed for the camera ready. Please only include arxiv/zenodo versions if the paper isn't published already in a venue. Use consistent conference names and abbreviations (International Society [...] (ISMIR) Conference vs ISMIR) and conference locations and years. If you decide to provide links, please do so consistently when they are available and from appropriate sources.\n\nMinor comments:\n- l48: fine tune; l51: fine-tune\n- table 1: maybe some thousands separators on the numbers\n- table 2: somehow need to shorten the width\n- l291-292: is \"in particular\" or \"solely on Classical\" more accurate?\n- l341-347: it's understandable after a slow read, but would be better to try to split the sentence",
      "review_3": "This paper introduces a new approach to Roman numeral analysis that outperforms previous state-of-the-art models by leveraging pretraining on a Bert-like architecture.\n\nThe paper is well-written, and the results are highly promising, while also addressesing important questions regarding automatic Roman numeral analysis. Namely, the authors address the issue of potential multiple valid interpretations of a given segment or analysis.\n\nHowever, there are several areas that would require minor corrections and refinement:\n\n1. Missing Publication Years in References: A very important point is that, all references lack the publication year, which is essential for proper citation and a high standard of an academic proceedings paper.\n\n2. Inconsistency in References: A more minor point is the presentation of publishing venues for proceedings which are abbreviated or in a couple of case missing completely, for example pay attention to \"in ISMIR\": -> should be \"in Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)\" and likewise for others. In summary, it is highly suggested to the authors to fix the references section. \n\n3. Caution against Overstated Claims: Strong claims should be tempered. For instance, the assertion in footnote 8 implying the potential incorrectness of a human annotator's analysis should be approached with caution. Multiple valid interpretations can exist for a segment or Roman numeral, as highlighted in Section 4.2. Hence, it's advisable for the authors to avoid absolute statements regarding the absolute correctness of any single analysis.\n\n4. Standardization of Terminology: The capitalization of \"Roman numeral\" throughout the text varies. For consistency, it should be \"Roman numeral,\" with \"Roman\" capitalized as it serves as an adjective referencing a place of origin.\n\n5. Reevaluation of Key Prediction Triviality: The claim that predicting the key is trivial warrants reevaluation. While the paper suggests high accuracy in key prediction, it's crucial to consider the broader context. Many existing approaches, such as AugmentedNet, ChordGNN, and PKSpell, do not achieve near-perfect accuracy in predicting local keys or key signatures. Supporting this claim with evidence from publications demonstrating similar high accuracies would strengthen its validity. Alternatively, softening the claim to reflect the complexity and challenges associated with key prediction would be more prudent.\n\nAddressing these points will enhance the clarity, strength, and credibility of the paper.",
      "session": [
        "5"
      ],
      "slack_channel": "p5-17-rnbert-fine-tuning",
      "title": "RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis",
      "video": "https://drive.google.com/file/d/101uolZhDyW2dpOhoPqlgfgy_uM_kyBIu/view?usp=drive_link"
    },
    "forum": "322",
    "id": "322",
    "pic_id": "https://drive.google.com/file/d/1wLkwNpEi0tli-WbJLSHEaU8B3vOOW9zn/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",
      "abstract": "Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",
      "author_changes": "",
      "authors": [
        "Preniqi, Vjosa*",
        " Ghinassi, Iacopo",
        " Ive, Julia",
        " Kalimeri, Kyriaki",
        " Saitis, Charalampos"
      ],
      "authors_and_affil": [
        "Vjosa Preniqi (Queen Mary University of London)*",
        " Iacopo Ghinassi (Queen Mary University of London)",
        " Julia Ive (Queen Mary University of London)",
        " Kyriaki Kalimeri (ISI Foundation)",
        " Charalampos Saitis (Queen Mary University of London)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCXC8T0",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> web mining, and natural language processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1nTVfYNUTeLcioLm6T1-A3cCtG9ElEAVF/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1p0hDd03EtOchCgOQqB92j0bjd7Jy64Zy/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-15-automatic-detection-of",
      "title": "Automatic Detection of Moral Values in Music Lyrics",
      "video": "https://drive.google.com/file/d/1EfbWaqUcxnOqSdVEvL2HZbeoWe2FN7sA/view?usp=drive_link"
    },
    "forum": "326",
    "id": "326",
    "pic_id": "https://drive.google.com/file/d/1Eiz0NkthEE7ejBxQ_Z6CA558SfE8LCbr/view?usp=drive_link",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Methods based on deep learning have emerged as a dominant approach for cover song identification (CSI) literature over the past years, among which ByteCover systems have consistently delivered state-of-the-art performance across major CSI datasets in the field. Despite its steady improvements along previous generations from audio feature dimensionality reduction to short query identification, the system is found to be vulnerable to audios with noise and ambiguous melody when extracting musical information from constant-Q transformation (CQT) spectrograms. Although some recent studies suggest that incorporating lyric-related features can enhance the overall performance of CSI systems, this approach typically requires training a separate automatic lyric recognition (ALR) model to extract lyric-related features from music recordings. In this work, we introduce X-Cover, the latest CSI system that incorporates a pre-trained automatic speech recognition (ASR) module, Whisper, to extract and integrate lyrics-related features into modelling. Specifically, we jointly fine-tune the ASR block and the previous ByteCover3 system in a parameter-efficient fashion, which largely reduces the cost of using lyric information compared to training a new ALR model from scratch. In addition, a bag of tricks is further applied to the training of this new generation, assisting X-Cover to achieve strong performance across various datasets.",
      "abstract": "Methods based on deep learning have emerged as a dominant approach for cover song identification (CSI) literature over the past years, among which ByteCover systems have consistently delivered state-of-the-art performance across major CSI datasets in the field. Despite its steady improvements along previous generations from audio feature dimensionality reduction to short query identification, the system is found to be vulnerable to audios with noise and ambiguous melody when extracting musical information from constant-Q transformation (CQT) spectrograms. Although some recent studies suggest that incorporating lyric-related features can enhance the overall performance of CSI systems, this approach typically requires training a separate automatic lyric recognition (ALR) model to extract lyric-related features from music recordings. In this work, we introduce X-Cover, the latest CSI system that incorporates a pre-trained automatic speech recognition (ASR) module, Whisper, to extract and integrate lyrics-related features into modelling. Specifically, we jointly fine-tune the ASR block and the previous ByteCover3 system in a parameter-efficient fashion, which largely reduces the cost of using lyric information compared to training a new ALR model from scratch. In addition, a bag of tricks is further applied to the training of this new generation, assisting X-Cover to achieve strong performance across various datasets.",
      "author_changes": "",
      "authors": [
        "Du, Xingjian*",
        " Pei, Zou",
        " Liu, Mingyu",
        " Liang, Xia",
        " Liang, Huidong",
        " Chu, Minghang",
        " Wang, Zijie",
        " Zhu, Bilei"
      ],
      "authors_and_affil": [
        "Xingjian Du (University of Rochester)*",
        " Zou Pei (ByteDance)",
        " Mingyu Liu (ByteDance)",
        " Xia Liang (Bytedance)",
        " Huidong Liang (University of Oxford)",
        " Minghang Chu (Bytedance)",
        " Zijie Wang (ByteDance)",
        " Bilei Zhu (ByteDance AI Lab)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM62QLTX",
      "day": "1",
      "keywords": [
        "MIR tasks -> indexing and querying",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1OYqi9OjSIDEdoCORyvwZtlLMoR06XX7v/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1jFLymUg1dk9CMr4TBpvXjKe-6a_mF6nk/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-03-x-cover-better",
      "title": "X-Cover: Better music version identification system by integrating pretrained ASR model",
      "video": "https://drive.google.com/file/d/1qQLxjsDFUM2eXJwZ6G5xujt-sOOJyZds/view?usp=drive_link"
    },
    "forum": "328",
    "id": "328",
    "pic_id": "https://drive.google.com/file/d/1WQoRrfp2BSluPa2zVtN00s9RH1W91rz0/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.",
      "abstract": "Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.",
      "author_changes": "",
      "authors": [
        "Weck, Benno*",
        " Manco, Ilaria",
        " Benetos, Emmanouil",
        " Quinton, Elio",
        " Fazekas, George",
        " Bogdanov, Dmitry"
      ],
      "authors_and_affil": [
        "Benno Weck (Music Technology Group, Universitat Pompeu Fabra (UPF))*",
        " Ilaria Manco (Queen Mary University of London)",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Elio Quinton (Universal Music Group)",
        " George Fazekas (QMUL)",
        " Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2PM1SP3",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/18Sd4DvX2-GSRybRgjk5jRdHNTixovPlX/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1Au9_Bg1lF0KETm2fsN7GLRHfOStXgpkJ/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-01-muchomusic-evaluating-music",
      "title": "MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models",
      "video": "https://drive.google.com/file/d/1_mWEFUkb6g2TTWt6ZKNBk9KW0dQeIV2C/view?usp=drive_link"
    },
    "forum": "333",
    "id": "333",
    "pic_id": "https://drive.google.com/file/d/15IJY9NC5AT1ZXABUF19nPDH21FVRClQJ/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This study presents the Music Informatics for Radio Across the GlobE (MIRAGE) online dashboard, which allows users to access, interact with, and export metadata (e.g., artist name, song title) and musicological features (e.g., instrument list, voice type, key/mode) for 1 million songs streaming on 10,000 internet radio stations across the globe. Users can search for stations or songs according to several criteria, display, analyze, and listen to the selected station/song lists using interactive visualizations that include embedded links to streaming services, and finally export relevant metadata and visualizations for further study.",
      "abstract": "This study presents the Music Informatics for Radio Across the GlobE (MIRAGE) online dashboard, which allows users to access, interact with, and export metadata (e.g., artist name, song title) and musicological features (e.g., instrument list, voice type, key/mode) for 1 million songs streaming on 10,000 internet radio stations across the globe. Users can search for stations or songs according to several criteria, display, analyze, and listen to the selected station/song lists using interactive visualizations that include embedded links to streaming services, and finally export relevant metadata and visualizations for further study.",
      "author_changes": "",
      "authors": [
        "Nguyen, Ngan V.T.",
        " Acosta, Elizabeth",
        " Dang, Tommy",
        " Sears, David*"
      ],
      "authors_and_affil": [
        "Ngan V.T. Nguyen (University of Science, Vietnam Nation University Ho Chi Minh City)",
        " Elizabeth A.M. Acosta (Texas Tech University)",
        " Tommy Dang (Texas Tech University)",
        " David R.W. Sears (Texas Tech University)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1NQZMY",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> cognitive MIR; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        "Human-centered MIR -> music interfaces and services"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1zyarHqwIq80cXUHwfW76uEwoJQEFKOnL/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1FTAAUOg4remtlKF9jW9kjJfsjP4opVeI/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-09-exploring-internet-radio",
      "title": "Exploring Internet Radio Across the Globe with the MIRAGE Online Dashboard",
      "video": "https://drive.google.com/file/d/1l7YN-x0PXcnUosh5EZCv1x4PdzLjDOm6/view?usp=sharing"
    },
    "forum": "334",
    "id": "334",
    "pic_id": "https://drive.google.com/file/d/1aFqRSXlY6EzHKr0Cq_2YpTg7ztRCnA0X/view?usp=sharing",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener's reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personlization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually 'fuzzy' solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.",
      "abstract": "Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener's reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personlization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually 'fuzzy' solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.",
      "author_changes": "",
      "authors": [
        "Demetriou, Andrew M.*",
        " Kim, Jaehun",
        " Liem, Cynthia"
      ],
      "authors_and_affil": [
        "Andrew M. Demetriou (Delft University of Technology)*",
        " Jaehun Kim (Pandora / SiriusXM)",
        " Cynthia Liem (Delft University of Technology)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCXJX0E",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1UNtJ-Tf6HmkqHZxoN178HyZbGWMMiRLH/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1RzPyaKKiOooQhJgskFcMbTapg0OzSOeU/view?usp=share_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-12-towards-automated-personal",
      "title": "Towards Automated Personal Value Estimation in Song Lyrics",
      "video": "https://drive.google.com/file/d/10QODdTfivKPKUHNKsqQHtqYetjjVPpdg/view?usp=share_link"
    },
    "forum": "335",
    "id": "335",
    "pic_id": "https://drive.google.com/file/d/1ZOuRfzixPfpDKnYsXJl4XuO8GV7NeSsh/view?usp=share_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",
      "abstract": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",
      "author_changes": "",
      "authors": [
        "Manco, Ilaria*",
        " Salamon, Justin",
        " Nieto, Oriol"
      ],
      "authors_and_affil": [
        "Ilaria Manco (Queen Mary University of London)*",
        " Justin Salamon (Adobe)",
        " Oriol Nieto (Adobe)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ0TV4HH",
      "day": "3",
      "keywords": [
        "Knowledge-driven approaches to MIR -> representations of music",
        "Applications -> music retrieval systems; Applications -> music videos, multimodal music systems; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1QU9KGnGv96vGy6Jvh-OgdfcY2fPMdXHT/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14QtD9aUQH8b5oBxNlgMwsBMZZnfVywFD/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-16-augment-drop-swap",
      "title": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning",
      "video": "https://drive.google.com/file/d/1lqBFpk6ik1wOBxaBmTtFuFqfSnooydtj/view?usp=sharing"
    },
    "forum": "336",
    "id": "336",
    "pic_id": "https://drive.google.com/file/d/1-ozbdUylrOzfa6KpGiSK5JzLVYCzkie1/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "We present ``The Concatenator,'' a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or ``audio mosaicing'') technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.",
      "abstract": "We present ``The Concatenator,'' a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or ``audio mosaicing'') technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.",
      "author_changes": "",
      "authors": [
        "Tralie, Christopher J*",
        " Cantil, Ben"
      ],
      "authors_and_affil": [
        "Christopher J. Tralie (Ursinus College)*",
        " Ben Cantil (DataMind Audio)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ3D59GS",
      "day": "3",
      "keywords": [
        "Generative Tasks -> real-time considerations",
        "Creativity -> tools for artists; Generative Tasks -> artistically-inspired generative tasks ; Generative Tasks -> interactions; Generative Tasks -> qualitative evaluations; Generative Tasks -> transformations"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1NZam-5Qy57d8hFXBd-OEOvSP4N9mgsCz/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1ndrkyCDu26e6CgWB--S80RU0a1lj1jsv/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-08-the-concatenator-a",
      "title": "The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing",
      "video": "https://drive.google.com/file/d/1Vu1SncA4mhAmcPPw9KQQ-IEsPtSbF-A9/view?usp=sharing"
    },
    "forum": "340",
    "id": "340",
    "pic_id": "https://drive.google.com/file/d/1ArR5u9D4MHZvWG5mFmKd9CtiGnNTchSD/view?usp=sharing",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",
      "abstract": "Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",
      "author_changes": "",
      "authors": [
        "Fang, Kun*",
        " Wang, Ziyu",
        " Xia, Gus",
        " Fujinaga, Ichiro"
      ],
      "authors_and_affil": [
        "Kun Fang (McGill University)*",
        " Ziyu Wang (NYU Shanghai)",
        " Gus Xia (New York University Shanghai)",
        " Ichiro Fujinaga (McGill University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MX6L3T",
      "day": "4",
      "keywords": [
        "Creativity -> creative practice involving MIR or generative technology",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1faSCXbSbmPlzCZxPF14QMZT8EALvlXf4/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1aCrLu7Ribt416pix7FTkXvv5OuzEzgXr/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-04-exploring-gpt-s",
      "title": "Exploring GPT's Ability as a Judge in Music Understanding",
      "video": "https://drive.google.com/file/d/1HUb7E9n3UhZXhSkVTqGJ6sIgazG9fxxv/view?usp=sharing"
    },
    "forum": "345",
    "id": "345",
    "pic_id": "https://drive.google.com/file/d/1vlIVKi9CQqPQwyrkA7BXPjpj-CebbHA7/view?usp=sharing",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",
      "abstract": "The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",
      "author_changes": "In a nutshell, the suggestions pointed out by the reviewers and the meta-reviewer were related to two main points: improve the clarity of the text and strengthen the experimental part of the paper. The first point was addressed by two modifications: writing a small sentence at the beginning of Section 4 indicating that the matrices there described aim to recover the proposed model (Eqs. 5-8) via Eqs. 9 and 10; regenerating Figure 1 with different markers, and not only different colors, to improve the readability of the paper on its black and white printed version. As to the second point (improving the experimental part of the paper), unfortunately the limitation of space does not allow us to properly discuss any other additional experiment. However, we acknowledge the issues raised by the reviewers and the meta-reviewer on the last paragraph of Section 5, indicating their resolution in future works.",
      "authors": [
        "Carvalho, Hugo T*",
        " Li, Min Susan",
        " Di Luca, Massimiliano",
        " Wing, Alan M."
      ],
      "authors_and_affil": [
        "Hugo T. Carvalho (Federal University of Rio de Janeiro)*",
        " Min S. Li (University of Birmingham)",
        " Massimiliano Di Luca (University of Birmingham)",
        " Alan M. Wing (University of Birmingham)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9F9FQLF",
      "day": "2",
      "keywords": [
        "MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> expression and performative aspects of music; Musical features and properties -> rhythm, beat, tempo",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The reviewers all agree that the music syntonization problem is well formulated reconciling music cognition and the KF approach is a promising approach. Reviewer #3 emphasized that this framework has a lot of potential in other MIR tasks as well. At the same time, all reviewers pointed out that the evaluation part is weak. Specifically, the experiment was conducted on a small dataset in limited music scenarios. The results are presented qualitatively without model comparison. Nevertheless, all reviewers are generally in favor of this paper, mainly because of its potential impact on the ISMIR community.",
      "paper_presentation": "Remote",
      "pdf_path": "https://drive.google.com/file/d/1xbuQojY3c7U5aBISEmYQbcgdSrHE7IMA/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1PaMyuIzsGJRpviccxxaaSnHvOku7Wjg3/view?usp=sharing",
      "review_1": "The paper is well written and clear. I'm not an expert on how to analyze temporal alignment and synchronization between musician in an ensemble, but it was not difficult to follow the technical details, also thankfully for the clarity of the exposition. The Kalman Filter is ubiquitous and it has been proven effective also in studying how precise temporal synchronization between performers in a musical ensemble is achieved. The phase correction strength seems to be a good indicator for knowing which instruments are the leaders, and which are the followers. The results show on the Op. 74 no. 1 by Joseph Haydn are quite impressive to me. As I mentioned I'm not an expert on this very specific topic, so, unfortunately I do not know the literature, and nor the state of the art of the syncrony analysis. Give that, I red the paper with pleasure and it flows like a familiar topic to me. It would be nice to have a thorough evaluation also on other music ensemble examples. But I understand that the dataset can be a problem since this is a very specific ground truth that is not simple to obtain. Moreover, I can imagine some implication also in other MIR topics, like score followers and also generative models can use the phase correction gain to model the \"humanity\" of the timing of a computer generated music.",
      "review_2": "The paper presents a linear model for ensemble synchronization using Kalman Filter. It essentially uses time-dependent version of ADAM, where instead of bGLS model, KF is used.\n\nThe paper evaluates the model using a simulation of normal performance, tempo change and deadpan condition, and shows that the filtered state variables seems to be consistent with the nature of the pieces.\n\nThe model is really exciting, bridging the gap between music cognition and interactive music systems.  It will be immsensely useful for automatic accompaniment systems, beat tracking systems, or any system in which timing interaction between human musicians (or machines) are useful.\n\nOne issue is the evaluation, where the authors only provide the trajectory of the estimated parameters to describe why the model makes sense qualitatively.  I understand that the result seems to make sense, but I would have like to see for example comparison with other methods for parameter estimation.  For example, in all cases it takes about 20-30 onsets for the parameters to converge from the initial value.  Are these because the process noise is so small for alpha, or are these because the correction values really change after starting the performance?  \n\nThere are a few questions that I found might be beneficial for the authors to discuss in the paper.\n\n1. My understanding is that linear models for phase/period synchronization has constraints that model relationship between parameters like timekeeper and motor variance, the constraints of which necessitates the use of bGLS algorithm instead of simple regression models.   In the proposed method, however, the model is solved using KF, meaning such hard constraints cannot be handled.  Without using more elaborate variants like Unscented KF, do the filtered estimates \"make sense\" from music cognition perspective?  In other words, is the proposed method just a convenient state-space model for music ensemble modeling, or is it something that can provide insights to music cognition studies using ADAM?  Please discuss some limitations regarding the parameter estimates, if any.\n2. Is smoothing the period/phase correction useful?  Fig. 1 seems that there is an excessive smoothing of the parameters, which hides the underlying interaction that is going on.  It would have been interesting to see how the correction parameters alpha and beta vary by changing the process noise of the corresponding parameters, or perhaps considering them independent of each other and using multiple takes of the same piece to identify the parameters.\n\nA comment\n\n- In the evaluation, all of the smoothed estimates start with 0.25 which I presume is a hardcoded initial value.   I believe if the initial state covariance estimate is set to a very large value, the smoothed state estimates will have less influence on the choice of the initial value.",
      "review_3": "This paper builds on the phase correction model and period correction model presented in [9-11], discussing how these can be optimized using Kalman filter assumptions. While a significant portion of the paper is dedicated to deriving equations, it is disappointing that the advantages of the methodology are not sufficiently demonstrated through experiments. Specifically, there is a lack of discussion on how this approach could be applied to general music scenarios. Additionally, it is unclear whether this methodology can only be applied to phase correction or period correction models, leaving the paper's significance in general synchronization contexts uncertain.",
      "session": [
        "4"
      ],
      "slack_channel": "p4-16-a-kalman-filter",
      "title": "A Kalman Filter model for synchronization in musical ensembles",
      "video": "https://drive.google.com/file/d/1YKoc_qGkB-_kV3iYWly-RBG5QGwl3cnE/view?usp=sharing"
    },
    "forum": "347",
    "id": "347",
    "pic_id": "https://drive.google.com/file/d/1Q_1EwrIcYXlMDQR39wgg7zMQEd4ECInT/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",
      "abstract": "Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",
      "author_changes": "- We have updated the naming convention of the dataset for clarity, and we have been consistent throughout the paper.\n- We have improved the discussion on the video gathering to address the raised comments and questions by the Reviewers.\n- We have included separation results of fine-tuned Spleeter using Saraga and Saraga Audiovisual (the new dataset) and improved the discussion on the relevance of these benchmarking experiments. We do not report fine-tuning experiments on the combination of Saraga and Saraga Audiovisual as our intention is to study the impact of each data collection process separately.\n- We have included missing details in the perceptual test results.\n\nLet us further comment on the questions about the video footage:\n- The video gesture experiments are performed only on voice only excerpts called the alapana. These sections are not metered and also do not contain any rhythmic accompaniment.\u00a0 The demo video is used only to show excerpts from the dataset and not correlated to the videos used for the gesture experiments. Figure 2 in the paper demonstrates the vocalist singing a composition in a rhythmic meter, but is illustrative of the video data in general and is not indicative of the alapana section on which the gesture experiments are perfomed. Figure 3 illustrates the vocalist singing an alapana.\n- The video recordings are recorded from concert venues in the usual stage setting of the artists. In Carnatic music, the vocalist receives most prominence. Keeping in mind that the microphone placed directly in front of the vocalist causes visual occlusions of the mouth and the gestures made, several pose estimation models were tested before selecting the MMPose 2D-Top down model. This model manages to capture regions prone to occlusions like the singer\u2019s mouth and the hands with a good level of accuracy. ",
      "authors": [
        "Sivasankar, Adithi Shankar*",
        " Plaja-Roglans, Gen\u00eds",
        " Nuttall, Thomas",
        " Rocamora, Mart\u00edn",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Adithi Shankar (Music Technology Group- Universitat Pompeu Fabra)*",
        " Gen\u00eds Plaja-Roglans (Music Technology Group)",
        " Thomas Nuttall (Universitat Pompeu Fabra, Barcelona)",
        " Mart\u00edn Rocamora (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MXCN65",
      "day": "1",
      "keywords": [
        "Applications -> music videos, multimodal music systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> computational ethnomusicology; MIR tasks -> pattern matching and detection; MIR tasks -> sound source separation",
        "Computational musicology"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The intention to augment the available Carnatic music dataset is laudable. The dataset will be undoubtedly useful for future MIR work on Carnatic music. However, the annotation and validation analyses seemed to have been completed in a hurry.  Please go through the comments by the 3 reviewers who have pointed out several aspects that need attention in order to make this a truly valuable resource.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1EQXLNuxVj3e60R2Eovj8U4saxajWlXac/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1dSina5Drt9bNlrD4KEr4FXYmpbSTOkyh/view?usp=sharing",
      "review_1": "The paper proposes Saraga Carnatic 2.0: a large multimodal open data collection for analyzing Carnatic music. The paper reads well; however, the organization of the Experiments section could be reworked as some of the content is parallel and does not adhere to a causal flow. The authors refer to the existing dataset by 3 different names: Saraga, original Saraga, and Saraga1.0, which should be consistent. Innuendos such as \u201cnature of Carnatic music\u201d (line 459) may be avoided as this is a loaded statement, and a Spleeter model to learn such nuance while preserving the knowledge warrants more detailed discourse on the process.\n\nI have reservations about the argument that melodic motif annotations are unreasonable for manifold reasons. On the one hand, the authors acknowledge improving motif recognition as a useful task (line 188) and identify regions of repeated melodic motifs (line 312). On the other hand, advocating the lack of importance of the annotation task is counterintuitive. Most newly added ragas have only 1 occurrence, as evident from Figure 1. In the absence of several and balanced instances of a raga class, it is imperative that this dataset is not suitable for a raga classification task. This defeats the claim of new raga additions scaling the same (line 187).\n\nThe video analysis of the gesture modeling is well-written. However, the demo video shows that the performer keeps the meter by clapping gesture, with one active hand (right) and the other complementing. In this scenario, one would expect the kinetics to be related to stress points in the rhythmic progression. Thus, it would be interesting to have inferences on the individual differences between Ashwin and Prithvi on the vast difference of the same correlation values of 0.36 and 0.11, respectively. My final concern is about the effectiveness of calling the proposed dataset as an extension of Saraga. Like the authors think about having an independent identity of the instrumental dataset, calling this Saraga2.0 also warrants a thorough demonstration of the improvements expanding on Section 3.4, especially on the melody re-computation aspect.",
      "review_2": "The paper introduces an extended version of the Carnatic part of the Saraga dataset, presenting experimental results on music-motion relation analysis and music source separation.\n\nStrengths\nThe proposed dataset offers a large amount of data with various ragas and talas. The addition of video recordings and automatic pose estimation results can be beneficial for research on the relationship between music and motion, which has recently gained popularity in Indian art music research. The paper also includes feedback from the research community on the previous version of the dataset, demonstrating exemplary progress in open science. The reproduction of Pearson et al.'s analysis using the new dataset shows the usefulness of automatically annotated pose information.\n\nWeakness\n- The fine-tuning result for the music source separation model, presented in section 4.2, does not strongly support the validation of this dataset's usefulness. This is not only because the fine-tuning degraded the model's performance on vocal artifacts, but also because similar experiments and results were already provided with Saraga 1.0 [17]. MSS can be fine-tuned for Carnatic music with Saraga 1.0, so it is not the exclusive usefulness of Saraga 2.0. The authors should explicitly show the advantage of using additional data from Saraga 2.0, such as by comparing a model trained only with Saraga 1.0 and a model trained with both Saraga 1.0 and 2.0.\n- Including video is an interesting aspect of this dataset. However, the camera angle is not appropriate for capturing the posture of the violinist or mridangist. While the main interest in motion analysis might be the singer, the microphone and the stand obscure the singer's hand, as mentioned by the authors. In this sense, the dataset is not ideal for video analysis compared to other previous video recordings of vocal performances in Indian art music [16]. The authors should report the stability of the MMPose results, such as whether there was a sudden jump in hand location in the estimated gestures.\n- It is not clear, but it seems that the paintings in the background are also detected as human postures. Additionally, it would be better for users if the pose estimation was provided separately for the singer and other players.\n\nMinor comments\n- In Table 1, it is not clear how the sum of Saraga 1.0 and 2.0 would look in terms of the total number of ragas or talas.\n- I think the explanation on excluding instrumental Carnatic music can be shorten into one single sentence. It is worth mentioning that the current dataset does not cover instrumental Carnatic music, but I don't think the author has to justify why they are focusing on vocal-centered music with extensive paragraphs. The comparison with slakh sounds bit unnatural to me, as slakh used synthesizer. \n- It is not clear how many subjects participated in the listening test in Table 3. Also, the standard deviation or confidence interval has to be provided. \n- Saraga 1.0 also includes Hindustani music, but it is not clear how this will be handled in dataset access \n- There are duplicated sentences in line 224\n- Line 353: \"for which the p-value is less than our significance level of 0.00001 are excluded\" \u2013 Should this be \"greater than the significance level\"?\n- Some words in the References, such as Carnatic and Turkish, need to be capitalized in the camera-ready version.",
      "review_3": "The manuscript presents extension to Saraga Carnatic dataset. Along with increasing coverage of various ragas, concerts, the new version introduces video recordings of the concerts for multimodal MIR research. The paper is well organised and well written, certain tasks are also benchmarked with the extended dataset.",
      "session": [
        "1"
      ],
      "slack_channel": "p1-02-saraga-audiovisual-a",
      "title": "Saraga Audiovisual: a large multimodal open data collection for the analysis of Carnatic Music",
      "video": "https://drive.google.com/file/d/1Hn3rqQxlbgnb69IGcw8hZj53CRzNeqYm/view?usp=share_link"
    },
    "forum": "352",
    "id": "352",
    "pic_id": "https://drive.google.com/file/d/1WLVY2eot7RHjB-PrJXrM5Q-JEQ2kt0uB/view?usp=share_link",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to fine-tuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",
      "abstract": "Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to fine-tuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",
      "author_changes": "",
      "authors": [
        "Vaidyanathapuram Krishnan, Venkatakrishnan*",
        " Alben, Noel",
        " Nair , Anish  A",
        " Condit-Schultz, Nathaniel"
      ],
      "authors_and_affil": [
        "Venkatakrishnan Vaidyanathapuram Krishnan (Georgia Institute of Technology)*",
        " Noel Alben (Georgia Institute Of Technology)",
        " Anish Nair (Georgia Institute of Technology)",
        " Nathaniel Condit-Schultz (Georgia Institute of Technology)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MXEVLH",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1xxZ4RvR9q_4mIj3QBAs3Ey32bl6g2Bqg/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1JuGwSJvFXexbfTXoILktGru4ABzJctxR/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-06-sanidha-a-studio",
      "title": "Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music",
      "video": "https://drive.google.com/file/d/1I5hfnKGshAW6HBgw2YkUG2a9yttdxHN8/view?usp=sharing"
    },
    "forum": "354",
    "id": "354",
    "pic_id": "https://drive.google.com/file/d/1cs7FlC0Wky9WGM0FiKQaryvD70Q5qXAX/view?usp=sharing",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs\u2014this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% \u2192 82%).",
      "abstract": "We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs\u2014this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% \u2192 82%).",
      "author_changes": "Updated Table 1 to reflect the final state of evaluation.\nDecided against moving Section 5 before the system description to avoid confusion regarding some repeat annotations. For instance, the repeat annotations in MeSA-13 come from a different interface (not ours).\nUpon being asked to include a setting with only ground truth repeat annotations in Table 1, we changed our wording in the caption to show that the human-labeled repeats are in fact also ground truth repeats.\nChanged wording to clarify the experimental setting in Section 4.5.\nExplained the shape of S_i in L240 in the same paragraph. \nProvided motivation for why we consider a half measure radius in our evaluation metric.\nFixed variables in L239/240/242.\nClarified that C major is always the default key.\nAdded clarification in Section 4.3 to emphasize that Shan et al. used the same model for piano transcription that we used, Onsets and Frames, however a different representation (MIDI transcription directly obtained from the model).\nAdded Table 2 to report results for the evaluation of different audio feature representations on both M13 and SMR.\nDecided against reporting results from additional baselines since our focus in this paper is on showing that human-labeled repeats increase alignment accuracy considerably. Due to this, we did not evaluate the effect of minor algorithmic changes, but a more detailed analysis could be of interest in future work.\nWe also do not implement a system/line-level evaluation metric because even though that would allow us to evaluate our baseline\u2019s performance in a different setting, our workflow aims for improvement on measure-level alignment, so we keep our focus on measure-level evaluation.\nIncluded citations suggested by reviewers.\nIncluded links to supplementary videos and our code for reproducibility as promised.\nReferences are fixed to have consistent formatting and display all necessary information.\nThe SMR dataset (subset) is updated to contain 60 pieces instead of 49.",
      "authors": [
        "Bukey, Irmak*",
        " Feffer, Michael",
        " Donahue, Chris"
      ],
      "authors_and_affil": [
        "Irmak Bukey (Carnegie Mellon University)*",
        " Michael Feffer (Carnegie Mellon University)",
        " Chris Donahue (CMU)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHEN7T2S",
      "day": "4",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality; MIR tasks -> optical music recognition; MIR tasks -> pattern matching and detection",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The paper proposes a simple solution to aligning audio files with sheet music that have structural differences (such as repeats or no repeats): simply annotate them. The authors present a quick workflow for this task and demonstrate the effectiveness of the extra annotations (particularly the repeats) in their evaluation and provided videos.\n\nMain Strengths:\n- Clarity and Practicality: The paper is well-written, easy to understand, and presents a practical solution to the problem of aligning audio files with sheet music, especially when there are structural differences such as repeats or jumps.\n- Effectiveness: Demonstrates a clear improvement over baseline systems when using annotated information on repeats and jumps, which highlights the practical application and potential benefits in real-world scenarios.\n- Human-in-the-Loop Approach and Workflow Integration: Emphasises the value of human annotations in improving alignment accuracy, which is often overlooked in favour of fully automatic solutions. This approach is both quick and low in human effort. The workflow is cohesive and integrates well with existing state-of-the-art approaches, making it a robust engineering solution.\n- Reproducibility: The promise to release the code upon acceptance ensures that the results can be reproduced and verified by others in the community.\n\nMain Weaknesses:  \n- Lack of Novelty: The approach lacks significant novelty from a research perspective. The idea of labelling repeats and using annotated jumps is not new, and the improvements over existing methods are not groundbreaking.\n- Evaluation Scope: The evaluations, while showing improvements, lack depth in analysis. For instance, there is a need for a more detailed explanation of the significant accuracy jump attributed to feature representations.\n- Dataset Limitations: The datasets used for evaluation are limited, particularly in the diversity of music types. The evaluation predominantly focuses on piano music, which may not fully represent the system\u2019s capabilities.\n\nFurther Comments:\n- The measure-aware alignment and evaluation make intuitive sense but could be perceived as arbitrary due to the variability in measure lengths.\n- The proposed system\u2019s baseline comparison should also consider using similar features to those of existing methods to ensure a fair evaluation.\n\nRequested Improvements:  \n- References: The references are inconsistently formatted and incomplete in some cases. Please resolve this for a potential final version. Also, some additional references of papers addressing jumps/repeats in the alignment context should be added (see the individual reviews).\n- Features: Improvements in feature representation claims should be supported by evaluations on larger datasets like SMR, in addition to the relatively small M13 dataset. The paper could benefit from a more detailed analysis of why the proposed feature representations supposedly lead to significant improvements.\n\n\nPlease also consider the more detailed comments by the individual reviewers.\n\nDespite the lack of significant novelty, the paper presents a practical and effective solution to audio-to-sheet music alignment. Its clear writing, practical workflow, and demonstrated improvements in alignment accuracy make it a valuable contribution.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1UvKKzTuptPSWcUG8eLtoRAW27nLrJMCU/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/10SNBKkgmZ93IS5WNXThWLvpxRXXQpTk_/view?usp=sharing",
      "review_1": "In this paper, the authors propose an audio-to-music-sheet synchronization system that combines several state-of-the-art approaches to detect notes and staff lines [16,17], measure positions [21], music transcription [3], and standard DTW [22]. On top of this engineering solution, the authors developed an interface to annotate jumps and repeats and feed the system with this information.\n\nEvaluation has been performed at the measure level using the MeSA-13 and a subset from the SMR dataset. The proposed system is compared with the method in [17], which uses Hierarchical DTW to automatically account for the jumps and repeats. Results demonstrate the superior performance of the proposed system, especially when using the information from jumps and repeats. Other information, such as ground-truth measure and staff metadata, has been evaluated, showing a slight improvement.\n\nThe paper is well-structured and easy to read. The context of the paper is well presented, and several demonstrations of the technology are provided.\n\nIn my opinion, there is little novelty in the proposal from a research point of view, but I see the potential application from the technological side with several applications to the community and market (Musescore, page-turning systems, etc.). ",
      "review_2": "The motivation is clear and reasonable: to manually handle the jumps in alignment. The repetition labeling process brings significant improvement in accuracy with relatively low human effort. With the collected annotations, it is possible to automate the process with a model in the future.\n\nI have a few minor comments:\n1. I suggest moving Section 5 before the system description, as it does not depend on the alignment system (in fact, it\u2019s the opposite).\n2. In Table 1, it would be helpful to include results with only ground truth repeat annotations for comparison with the human labels.\n3. The experimental settings for comparisons among different representations (L399-L402) are unclear. I guess this is related to different combinations of score features and audio features. The authors might want to clarify this.\n4. The authors need to pay attention to the references: use a consistent format (proceeding names, conference names, etc.), fill in missing fields (authors, pages, years), and replace arXiv versions with proceedings versions where applicable.\n\nOverall, the novelty and impact are not very high; therefore, I recommend a weak accept.",
      "review_3": "Short Paper Summary:\nThis paper proposes an audio-to-score alignment approach to align arbitrary music performances to sheet images. The approach extends and improves upon existing audio and score representations (bootleg score) for this task and further relies on minimal human intervention in the form of annotated jumps/repeats in the sheet image. This can usually be done quickly and is shown to significantly improve the alignment quality.\n\t\t\nNote on Reproducibility:\nThe authors promise to release their code upon acceptance (and assumingly their annotation tool as well). Alongside the information on which pieces have been used for evaluation (given in the supplementary material) it should be possible to reproduce and verify the results.\n\nMain Review:\nWhile the basic components of the approach are not novel (bootleg-score inspired score representation, Onset & Frames to get a piano roll-like representation of the audio, DTW), I still think the practicality of the approach warrants an acceptance. The paper is clearly written and for the most part easy to follow along. That being said, some aspects of the paper could be improved.\n\nIn Section 3.1, it is not immediately clear to me how the shape of S_i (line 240) is derived. 88 for the pitch dimension is clear, but what\u2019s the reasoning behind 48 and how would this impact the resolution of the score representation, e.g. with respect to extremely fast note runs?\nI think the (index) variable i is used for two different things in lines 239/240/242. To make the difference more clear, I\u2019d maybe suggest using another variable.\nIn the same section, when describing how the bootleg score is converted into a piano roll. Is C major always assumed as the default key? I might be reading the sentence (starting at line 253) wrong, or the end of the insertion mark is missing. \n\nIn Section 3.2, the use of the onset predictions coming from the Onset & Frames model is introduced. It\u2019s worth noting here that Shan and Tsai [16, 17] already used the onsets derived from the MIDI transcription of Onset & Frames for their bootleg score representation of the audio. I assume this is then similar to what is later on tested in Section 4.5 as onset predictions (with the difference of staying in a \u201cpiano roll space\u201d), which has a similar performance as the onset probabilities. However, the evaluation for that seems to be done only on the relatively small M13 dataset. In order to really claim that this refined audio representation results in an improvement, I\u2019d suggest also doing an evaluation on the larger SMR dataset.\n\nOverall, I think the comparison and the claim around superiority \u201cdue to our refinements to feature representations\u201d is not entirely fair and clearly shown in the experiment. Which is already acknowledged to some extent in the paper itself due to the proposed measure-wise evaluation.\nIf I\u2019m not mistaken, nothing prevents the proposed approach to use similar features, i.e. same bootleg score representation as Shan and Tsai [16, 17]. Still performing measure segmentation, but basically stopping before the conversion to piano rolls described in 3.1. Such a model could be used as an additional baseline/vanilla version.\nAlternatively, it might be possible to additionally extend the existing measure-aware evaluation to a system-level evaluation which would allow for a more direct and fairer comparison (although maybe not as useful wrt. to the overall alignment precision as the measure-aware version). \nIn any case, I\u2019m not doubting that the proposed approach is able to yield more precise results, but I think such a comparison could benefit the paper and better support the claims made. \n\nMinor Remarks:\n- Line 493: \u201c... that offers has an interactive \u2026\u201d -> \u201c... that offers an interactive \u2026\u201d\n- In the related work section, when writing \u201cWe diverge from them by considering a range of different types of raw score images and audio (such as ones with instrumentation beyond solo piano)\u201d (lines 472 onwards). While this is shown in the supplementary material with some examples, the datasets used for evaluation only contain 1 or 2 samples that aren\u2019t strictly piano music. Even though limited to piano music and also in the score following domain, another potentially related line of research to check out could be Henkel and Widmer \u201dReal-Time Music Following in Score Sheet Images via Multi-Resolution Prediction\u201d as they also experiment with raw sheet image scans.\n- I ticked \u2018yes\u2019 for \u201cthe paper adheres to ISMIR 2024 submission guidelines\u201d, which is the case for the most part, but I want to make clear that the references are not well formatted. Please make sure to cite the proper conference version of papers instead of the arxiv ones, e.g. [1, 3, 12, 13, 15, 16, 37, 38] were published at ISMIR. Also, there are missing venues for [26, 29] and [19] shows a placeholder date.\n",
      "session": [
        "7"
      ],
      "slack_channel": "p7-16-just-label-the",
      "title": "Just Label the Repeats for In-The-Wild Audio-to-Score Alignment",
      "video": "https://drive.google.com/file/d/1w4trgsz-MIdKaRspq3X0EEUYFMWbYweP/view?usp=sharing"
    },
    "forum": "357",
    "id": "357",
    "pic_id": "https://drive.google.com/file/d/11zO68fEWu30iyUTulZET7OrLb0i5Aa32/view?usp=sharing",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "We introduce a series of transdisciplinary corpus studies aimed at investigating cross-cultural trends in time-line-based music traditions. Our analyses concentrate on a compilation of field recordings from the Centre de Recherche en Ethnomusicologie (CREM) sound archive. To demonstrate the value of an interdisciplinary approach combining ethnomusicology and music information research to rhythmic analysis, we propose a case study on the bell patterns used in the musical practices of Candombl\u00e9, an Afro-Brazilian religion. After removing vocals from the recordings with a deep learning source separation technique, we further process the instrumental segments using non-negative matrix factorization and select the bell components. Then, we compute a tempo-agnostic rhythmic feature from the bell track and use it to cluster the data. Finally, we use synthesized patterns from the musicological literature about Candombl\u00e9 as references to propagate labels to the rhythmic clusters in our data. This semi-supervised approach to pattern analysis precludes the need for downbeat and cycle annotations, making it particularly suited for extensive archive investigations. Lastly, by comparing bell patterns in Candombl\u00e9 and a West African music tradition, we lay the foundation for our future cross-cultural research and observe the potential application of this methodology to other time-line-based music.",
      "abstract": "We introduce a series of transdisciplinary corpus studies aimed at investigating cross-cultural trends in time-line-based music traditions. Our analyses concentrate on a compilation of field recordings from the Centre de Recherche en Ethnomusicologie (CREM) sound archive. To demonstrate the value of an interdisciplinary approach combining ethnomusicology and music information research to rhythmic analysis, we propose a case study on the bell patterns used in the musical practices of Candombl\u00e9, an Afro-Brazilian religion. After removing vocals from the recordings with a deep learning source separation technique, we further process the instrumental segments using non-negative matrix factorization and select the bell components. Then, we compute a tempo-agnostic rhythmic feature from the bell track and use it to cluster the data. Finally, we use synthesized patterns from the musicological literature about Candombl\u00e9 as references to propagate labels to the rhythmic clusters in our data. This semi-supervised approach to pattern analysis precludes the need for downbeat and cycle annotations, making it particularly suited for extensive archive investigations. Lastly, by comparing bell patterns in Candombl\u00e9 and a West African music tradition, we lay the foundation for our future cross-cultural research and observe the potential application of this methodology to other time-line-based music.",
      "author_changes": "",
      "authors": [
        "Maia, Lucas S*",
        " Namballa, Richa",
        " Rocamora, Mart\u00edn",
        " Fuentes, Magdalena",
        " Guedes, Carlos"
      ],
      "authors_and_affil": [
        "Lucas S Maia (Universidade Federal do Rio de Janeiro)*",
        " Richa Namballa (New York University)",
        " Mart\u00edn Rocamora (Universidad de la Rep\u00fablica)",
        " Magdalena Fuentes (New York University)",
        " Carlos Guedes (NYU Abu Dhabi)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCSXQQTS",
      "day": "4",
      "keywords": [
        "Applications -> music heritage and sustainability; Creativity -> humanistic discussions; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music; Philosophical and ethical discussions -> legal and societal aspects of MIR",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1FEBBNQF6eV4wxgOTzXmIcyEYtQ9PR0UQ/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1_OYp2YWSaG6tqPTJAoKcLsZSz6QX_VxM/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-17-investigating-time-line",
      "title": "Investigating Time-Line-Based Music Traditions with Field Recordings: A Case Study of Candombl\u00e9 Bell Patterns",
      "video": "https://drive.google.com/file/d/1N-NXiV5hUmDU4GbbMMXyN8f7T89Tw8Cv/view?usp=sharing"
    },
    "forum": "360",
    "id": "360",
    "pic_id": "https://drive.google.com/file/d/158UbPnkqeYdglOF6iEf3aoz93GZgaJMq/view?usp=sharing",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 496 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. In this paper we also propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",
      "abstract": "Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 496 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. In this paper we also propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",
      "author_changes": "",
      "authors": [
        "Yu, Huiran*",
        " Duan, Zhiyao"
      ],
      "authors_and_affil": [
        "Huiran Yu (University of Rochester)*",
        " Zhiyao Duan (University of Rochester)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UPUJCBEX",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1Guv-I2ac5iDOWntRs6M5MdZs2FiPLK48/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1eKvu4ZSNFuwX4KW91A-KyGFHRCYjjZrt/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-17-note-level-transcription",
      "title": "Note-Level Transcription of Choral Music",
      "video": "https://drive.google.com/file/d/1tcFjNk0p0z14T5IvU-1h-kK3Ms3LR3Wq/view?usp=drive_link"
    },
    "forum": "364",
    "id": "364",
    "pic_id": "https://drive.google.com/file/d/1sTC_5dpgBbtQ6UWcsolKyXFrZwIQlJSp/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcription-based lyrics embeddings (TLE). These estimate `ground-truth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, `ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",
      "abstract": "The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcription-based lyrics embeddings (TLE). These estimate `ground-truth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, `ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",
      "author_changes": "",
      "authors": [
        "Kim, Jaehun*",
        " Henkel, Florian",
        " Landau, Camilo",
        " Sandberg, Samuel E.",
        " Ehmann, Andreas F."
      ],
      "authors_and_affil": [
        "Jaehun Kim (Pandora / SiriusXM)*",
        " Florian Henkel (SiriusXM + Pandora)",
        " Camilo Landau (Pandora / SiriusXM)",
        " Samuel E. Sandberg (SiriusXM + Pandora)",
        " Andreas F. Ehmann (SiriusXM + Pandora)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USJEDVGC",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data",
        "Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> multimodality; MIR tasks -> automatic classification; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/13bjL3Kt-vHf_PaWqoTbIxu6DNnD7W3PK/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1x7bDU2EIR_cbGPvGTegxp6deDMf933BJ/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-12-transcription-based-lyrics",
      "title": "Transcription-based lyrics embeddings: simple extraction of effective lyrics embeddings from audio",
      "video": "https://drive.google.com/file/d/1hX06JcKiPBAFV2-1O6VsUpYluNbqE6WP/view?usp=drive_link"
    },
    "forum": "365",
    "id": "365",
    "pic_id": "https://drive.google.com/file/d/1javSHd7p_gz4nV107bjWb4al7WrnltSH/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Progress in the task of symbolic music generation may be lagging behind other tasks like audio and text generation, in part because of the scarcity of symbolic training data. In this paper, we leverage the greater scale of audio music data by applying pre-trained MIR models (transcription, beat tracking, structure analysis, etc.) to extract symbolic events and encode them into token sequences. To the best of our knowledge, this work is the first to demonstrate the feasibility of training symbolic generation models solely from extensive transcribed audio data. Furthermore, to enhance the controllability for the trained model, we introduce SymPAC (Symbolic Music Language Model with Prompting And Constrained Generation), which is a combination of (a) prompt bars in encoding and (b) a technique called Constrained Generation via Finite State Machines (FSMs) during inference time. We show the flexibility and controllability that this approach affords, which may be critical in making music AI useful to creators and users.",
      "abstract": "Progress in the task of symbolic music generation may be lagging behind other tasks like audio and text generation, in part because of the scarcity of symbolic training data. In this paper, we leverage the greater scale of audio music data by applying pre-trained MIR models (transcription, beat tracking, structure analysis, etc.) to extract symbolic events and encode them into token sequences. To the best of our knowledge, this work is the first to demonstrate the feasibility of training symbolic generation models solely from extensive transcribed audio data. Furthermore, to enhance the controllability for the trained model, we introduce SymPAC (Symbolic Music Language Model with Prompting And Constrained Generation), which is a combination of (a) prompt bars in encoding and (b) a technique called Constrained Generation via Finite State Machines (FSMs) during inference time. We show the flexibility and controllability that this approach affords, which may be critical in making music AI useful to creators and users.",
      "author_changes": "",
      "authors": [
        "Chen, Haonan*",
        " Smith, Jordan B. L.",
        " Spijkervet, Janne",
        " Wang, Ju-Chiang",
        " Zou, Pei",
        " Li, Bochen",
        " Kong, Qiuqiang",
        " Du, Xingjian"
      ],
      "authors_and_affil": [
        "Haonan Chen (Bytedance Inc.)*",
        " Jordan B. L. Smith (TikTok)",
        " Janne Spijkervet (University of Amsterdam)",
        " Ju-Chiang Wang (ByteDance)",
        " Pei Zou (Bytedance Inc.)",
        " Bochen Li (University of Rochester)",
        " Qiuqiang Kong (Byte Dance)",
        " Xingjian Du (University of Rochester)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM63JPRT",
      "day": "4",
      "keywords": [
        "Generative Tasks -> artistically-inspired generative tasks",
        "Creativity -> creative practice involving MIR or generative technology ; Generative Tasks -> evaluation metrics; Generative Tasks -> interactions; Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1ga7Z_lhyOq_3rckSOfKyss1qmcBSEjfK/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1eIjaFHViy2t8lRVANMAxMuKMvgJP5TPv/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-08-sympac-scalable-symbolic",
      "title": "SymPAC: Scalable Symbolic Music Generation With Prompts And Constraints",
      "video": "https://drive.google.com/file/d/1F4TgiHVhcLfouWgDset3dnXn8ouKlk6K/view?usp=drive_link"
    },
    "forum": "366",
    "id": "366",
    "pic_id": "https://drive.google.com/file/d/1SiKyNUXHZvFjZYzhpmwX1v2gdA_OdOf0/view?usp=drive_link",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Schenkerian Analysis (SchA) is a uniquely expressive method of music analysis, combining elements of melody, harmony, counterpoint, and form to describe the hierarchical structure supporting a work of music. However, despite its powerful analytical utility and potential to improve music understanding and generation, SchA has rarely been utilized by the computer music community. This is in large part due to the paucity of available high-quality data in a computer-readable format. With a larger corpus of Schenkerian data, it may be possible to infuse machine learning models with a deeper understanding of musical structure, leading to more \"human\" results.  To encourage further research in Schenkerian analysis and its potential benefits for music informatics and generation, this paper presents three main contributions: 1) a new and growing dataset of SchAs, the largest in human- and computer-readable formats to date (>140 excerpts), 2) a novel software for visualization and collection of SchA data, and 3) a novel, flexible representation of SchA as a heterogeneous-edge graph data structure.",
      "abstract": "Schenkerian Analysis (SchA) is a uniquely expressive method of music analysis, combining elements of melody, harmony, counterpoint, and form to describe the hierarchical structure supporting a work of music. However, despite its powerful analytical utility and potential to improve music understanding and generation, SchA has rarely been utilized by the computer music community. This is in large part due to the paucity of available high-quality data in a computer-readable format. With a larger corpus of Schenkerian data, it may be possible to infuse machine learning models with a deeper understanding of musical structure, leading to more \"human\" results.  To encourage further research in Schenkerian analysis and its potential benefits for music informatics and generation, this paper presents three main contributions: 1) a new and growing dataset of SchAs, the largest in human- and computer-readable formats to date (>140 excerpts), 2) a novel software for visualization and collection of SchA data, and 3) a novel, flexible representation of SchA as a heterogeneous-edge graph data structure.",
      "author_changes": "We added information about the analysts, availability of the dataset and notation software, and technology used for the notation software. Additionally, several small clarity edits were made, such as added plot ticks, rearranging of figure location, etc.",
      "authors": [
        "Hahn, Stephen*",
        " Xu, Weihan",
        " Yin, Zirui",
        " Zhu, Rico",
        " Mak, Simon",
        " Jiang, Yue",
        " Rudin, Cynthia"
      ],
      "authors_and_affil": [
        "Stephen Hahn (Duke)*",
        " Weihan Xu (duke)",
        " Zirui Yin (Duke University)",
        " Rico Zhu (Duke University)",
        " Simon Mak (Duke University)",
        " Yue Jiang (Duke University)",
        " Cynthia Rudin (Duke)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCR7PY6L",
      "day": "3",
      "keywords": [
        "Human-centered MIR -> music interfaces and services; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> structure, segmentation, and form",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "meta_review": "All the reviewers appreciated that the paper is well-written and clear, with an appropriate bibliography, and is scientifically sound, making it a valuable contribution to the community.\n\nSome remarks were done about the availability of the data (all reviewers) and some suggestions of improvement were made, to explain more in details the dataset, its metadata and its links to other datasets (R2, R3, MR), to provide more details on the notation software (R2) and to improve and clarify the figures (all reviewers).\n\nPlease take into account those comments for the final version of your paper.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/19RtCI-wJZy7RKj7oEz7gD-94TMkdRPIb/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ME_Waz-jgKrb5fOavvJYK97jwteeVLYf/view?usp=drive_link",
      "review_1": "I greatly congratulate the authors on their work! I am pleased to see initiatives that move from black-box models to generative tasks and focus on more interpretable models, especially applied to complex and often subjective tasks like Schenkerian analysis. The brief explanation of Schenkerian analysis in Section 1.1 helps the reader who is not an expert in the subject to understand minimally what the process entails. Furthermore, throughout the work, the authors take care to discuss musically their points, always using the same example.\n\nI have a few minor suggestions for the authors:\n\n* In lines 219 to 221, the authors mention that the dataset consists of pieces from the common practice period, Gentle Giant, and others. I must admit, I am very curious to know what falls in between! For example, are there rock pieces with simpler melodies/harmonies, like Elvis Presley, Beatles? As a big fan of Gentle Giant, I'm aware that they make extensive use of ideas from common practice in their pieces (e.g., Knots, In Reflection), but with a significant harmonic and melodic modernization. To what extent can the usual Schenkerian analysis capture these nuances? I ask this because I am aware of a work of Schenkerian analysis applied to Brazilian Popular Music (https://www.scielo.br/j/pm/a/Q93rFfZWy49cm9xjb3mwCQt/?format=pdf in Portuguese -- I hope the authors can read it, as I do not know their nationality), and some adaptations are necessary to capture specific nuances of the aesthetic.\n\n* Will the software be made public? Open-source? Freeware? I understand that the software may still be unsuitable for widespread use in its current state, but it is important to mention the future of the software for readers who might want to use it.\n\n* I know that positioning figures in LaTeX is difficult, but Figs. 2 and 3 appear after Fig. 4. It would be good to fix that.\n\n* Using a single color in the bar graph of Fig. 3 would make it easier to read.\n\n* In Fig. 4, having the x-axis ticks every 1 would be helpful.\n\n* The proposal could also be used to assist in teaching Schenkerian analysis! Perhaps this is something important to mention.\n\nI conclude my comments once again by congratulating the authors for tackling a complex problem!",
      "review_2": "The paper is well-written and clearly organized, with a thorough bibliography. The context is well set on Schenkerian analysis. The encoding formats and graphical representations (fig.2b) are convincing. It has three main contributions. Whereas the graph structure is well detailed in Section 3 (and supported by Figure 5, which is enlightening), I would expect more details for both the dataset and especially the notation software.\n\nThe dataset:\n- Its process of creation can be slightly more detailed. Regardless of the name of the analysts (who could even remain anonymous after the publication), it can be useful to know more about their respective experience (experts in music composition, musicology, Schenkerian analysis? ; level/years of practice?). Any annotation guidelines specific to the format or tool used, any potential source of disagreement (potentially reduced by a reviewing process between analysts for example)?\n- Indicate where the dataset would be available, even in a submission, with an fake/anonymous URL. As the dataset is meant to grow, it also becomes important to number the versions/releases of your dataset to make experiments reproducible (for possible \"We trained our model on 'SchA' Dataset V1.1.7 (180 samples)\") and keep track of the evolution.\n  - In that sense, I recommend saving JSON format (which is already a sensible choice for machine- and human-readability) in unfolded versions to make them more human-readable, and easier to track changes in version control systems. (In supplementary material, provided samples are one-liners)\n- Stick with the current number of analyses: 145 -- even if future versions will include more of them.\n- The analyses of some statistics of the dataset is appreciated. Some comments in l.243-268 are important to understand the figures/table, and should be added in their captions, to make the figures more readable, more autonomous.\n- Provide more information about the metadata. If not in the paper, at least in a summary file in the dataset location.\n\nThe notation software:\n- In current state, the dataset and graph representation are the main contributions of the paper; and they are valuable. The software appears primarily as a tool that the authors (or analysts) used to produce and read the data. But it is not well documented, although it appears in the title.\n- Among other questions:\n  - What technology or language is used?\n  - Is it a script to run, an executable, a web application? This can be critical for accessibility, if the goal is to facilitate SchA annotation at scale.\n  - How the user interact with the system? If it displays the graphical representation of an JSON annotation, it is already interesting, but it needs to be explicitly mentioned. Can users interact the graphical representation, with JSON or both? What is an example of \"simple commands\" (l.275)?\n- l.279: In this context, the formulation \"We are currently working on...\" leaves a feeling of unfinished work. Nevertheless, it is completely okay to indicate what the features of current version are, and then evoke separately other features to be included in a future release. (minor issue)\n\nI believe the contributions of this paper are valuable, but it has to be made clear whether:\n- 1. the software can be used as it is by the community, and it is a major : in this case, it needs more details to be understood *how*.\n- or 2. it is a tool to facilitate building the dataset: in this case, it is a program which is part of the creation process of the dataset; it is not the main topic of this paper, and can be removed from the title. This framework is still precious and the code usable by the community.\n\nMinor comments on the form:\n- If possible, improve the position of figures so that more references are on the same pages as their figure, especially Figures 2, 3 and 4. Try splitting 3a and 3b in two separate figures (even if they stay at the same position). As mentioned before, captions can be more explicit.\n- Some use of quotes sometimes put unnecessary distance. l.126 \"fractal\" is justified if it comes from the reference (same for \"goodness metric\" l.153), but \"big data\" l.23, \"atonal\" l.74, \"human\" l.13 (?), \"ground truth\" l.198 are better established concepts that can be in plain text, or in italic if it really needs to stand out.\n- 1.1 and 1.2 can form an autonomous Section 2 \"History of computational SchA / State of the art / Related work\", separate from the introduction, which sounds concluding after the outlines in l.101-105.",
      "review_3": "This paper introduces an extension to a previously not so explored dataset, incorporating Schenkerian analyses of classical music. The newly added data focuses mostly on monophonic short fugal themes from the Baroque era. In my view, any open-source dataset constitutes a valuable asset for the community. Moreover, the availability of expert musicological annotations for Schenkerian analysis is both rare and precious.\n\nHere are some comments and suggestions for the authors:\n\n1. Availability of Data: It is mentioned in the paper that the dataset contains 145 new excerpts, but there is no clear indication in the paper regarding whether this data will be made publicly available. If it is intended to be public, I recommend including an anonymized link in the paper or explicitly stating its public availability in the final version.\n\n2. Clarity on Dataset Composition: From the examples provided and the supplementary material, it is implied that most of the data consists of short monophonic melodies. It would be beneficial for clarity if the authors explicitly state this information in the text if this is the case, otherwise I would highly suggest to include a more complete polyphonic example since space allows it. The excerpts depicted in Figure 4 may not fully convey this aspect, hence clarifying it within the text would be a plus for clarity.\n\n3. Clarification on Clustering Example: The clustering example presented in Figure 5 prompts the question of whether only pairs of note contractions are allowed for each level transition. In Schenkerian analysis, multiple notes can be reduced from the Foreground to the Middleground in a single step. While this doesn't pose a significant issue, it may be prudent to mention this fact within the paper to enhance clarity.\n\n4. Integration with Pre-existing Datasets: A key aspect of any new dataset paper is linking the new data with existing datasets. This is particularly crucial for smaller-scale musicological datasets. Given that the dataset in this paper includes fugal themes from the Well-Tempered Clavier (WTC), it could complement the ALGOMUS Bach fugues dataset effectively. I strongly recommend that the authors associate any related fugal themes from their dataset with existing datasets in the future (and add it as future work), as multi-level analyses are especially valuable in sparse musicological datasets.",
      "session": [
        "6"
      ],
      "slack_channel": "p6-06-a-new-dataset",
      "title": "A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis",
      "video": "https://drive.google.com/file/d/1mx1xF6y1fZBStu2DGS_5gp6RSQ9qIIUX/view?usp=drive_link"
    },
    "forum": "367",
    "id": "367",
    "pic_id": "https://drive.google.com/file/d/18VRICoSc5LG6v_g1NaXDOciRyR9IBUa0/view?usp=drive_link",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Predicting a listener\u2019s experience of music based solely on audio features has its limitations due to the individual variability in responses to the same music. This study examines the effectiveness of electroencephalogram (EEG) in predicting the subjective experiences while listening to music, including arousal, valence, familiarity, and preference. We collected EEG data alongside subjective ratings of arousal, valence, familiarity, and preference from both fans (N=20) and non-fans (N=34) of the K-pop idol group, NCT127 to investigate response variability to the same NCT127 music. Our analysis focused on determining whether the inclusion of EEG alongside audio features could enhance the predictive power of linear mixed-effects models for these subjective ratings. Specifically, we employed stimulus-response correlation (SRC), a recent approach in neuroscience correlating stimulus features with EEG responses to the ecologically valid stimuli. The results showed that familiarity and preference was significantly higher in the fan group. Furthermore, the inclusion of SRC significantly enhanced the prediction of familiarity compared to models based solely on audio features. However, the impact of SRC on predictions of arousal and valence exhibited variation depending on the correlated audio features, with certain SRCs improving predictions while others diminished them. For preference, only a few SRCs negatively affected model performance. These results suggest that correlations of EEG responses and audio features can provide information of individual listeners\u2019 subjective responses, particularly in predicting familiarity.",
      "abstract": "Predicting a listener\u2019s experience of music based solely on audio features has its limitations due to the individual variability in responses to the same music. This study examines the effectiveness of electroencephalogram (EEG) in predicting the subjective experiences while listening to music, including arousal, valence, familiarity, and preference. We collected EEG data alongside subjective ratings of arousal, valence, familiarity, and preference from both fans (N=20) and non-fans (N=34) of the K-pop idol group, NCT127 to investigate response variability to the same NCT127 music. Our analysis focused on determining whether the inclusion of EEG alongside audio features could enhance the predictive power of linear mixed-effects models for these subjective ratings. Specifically, we employed stimulus-response correlation (SRC), a recent approach in neuroscience correlating stimulus features with EEG responses to the ecologically valid stimuli. The results showed that familiarity and preference was significantly higher in the fan group. Furthermore, the inclusion of SRC significantly enhanced the prediction of familiarity compared to models based solely on audio features. However, the impact of SRC on predictions of arousal and valence exhibited variation depending on the correlated audio features, with certain SRCs improving predictions while others diminished them. For preference, only a few SRCs negatively affected model performance. These results suggest that correlations of EEG responses and audio features can provide information of individual listeners\u2019 subjective responses, particularly in predicting familiarity.",
      "author_changes": "",
      "authors": [
        "Park, Seokbeom",
        " Kim, Hyunjae",
        " Lee, Kyung Myun*"
      ],
      "authors_and_affil": [
        "Seokbeom Park (KAIST)",
        " Hyunjae Kim (KAIST)",
        " Kyung Myun Lee (KAIST)*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHENKTPG",
      "day": "3",
      "keywords": [
        "Musical features and properties",
        "Human-centered MIR -> user behavior analysis and mining, user modeling; Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1If3mdYW1r3a--u0ZgAKO4ztpln3Ob5Wy/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1jE0qVn5E-CRo04kFo1oEt8aqvN64Alex/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-03-enhancing-predictive-models",
      "title": "Enhancing predictive models of music familiarity with EEG: Insights from fans and non-fans of K-pop group NCT127",
      "video": "https://drive.google.com/file/d/1qcY7f82eJ1PSS8UMGiZYx1joANjzfeXS/view?usp=sharing"
    },
    "forum": "373",
    "id": "373",
    "pic_id": "https://drive.google.com/file/d/1tyyYBiGqaYePfWS9SDsJ66i-tYDGNgC4/view?usp=drive_link",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Developing a versatile deep neural network to model music audio is crucial in MIR. This task is challenging due to the intricate spectral variations inherent in music signals, which convey melody, harmonics, and timbres of diverse instruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based model featuring two key designs: a novel Mel-band Projection module at the front-end to enhance the model's capability to capture informative features across multiple frequency bands, and interleaved RoPE Transformers to explicitly model the frequency and time dimensions as two separate sequences. We apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and vocal melody transcription, aimed at isolating singing voices from audio mixtures and transcribing their lead melodies, respectively. Despite their shared focus on singing signals, these tasks possess distinct optimization objectives. Instead of training a unified model, we adopt a two-step approach. Initially, we train a vocal separation model, which subsequently serves as a foundation model for fine-tuning for vocal melody transcription. Through extensive experiments conducted on benchmark datasets, we showcase that our models achieve state-of-the-art performance in both vocal separation and melody transcription tasks, underscoring the efficacy and versatility of Mel-RoFormer in modeling complex music audio signals.",
      "abstract": "Developing a versatile deep neural network to model music audio is crucial in MIR. This task is challenging due to the intricate spectral variations inherent in music signals, which convey melody, harmonics, and timbres of diverse instruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based model featuring two key designs: a novel Mel-band Projection module at the front-end to enhance the model's capability to capture informative features across multiple frequency bands, and interleaved RoPE Transformers to explicitly model the frequency and time dimensions as two separate sequences. We apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and vocal melody transcription, aimed at isolating singing voices from audio mixtures and transcribing their lead melodies, respectively. Despite their shared focus on singing signals, these tasks possess distinct optimization objectives. Instead of training a unified model, we adopt a two-step approach. Initially, we train a vocal separation model, which subsequently serves as a foundation model for fine-tuning for vocal melody transcription. Through extensive experiments conducted on benchmark datasets, we showcase that our models achieve state-of-the-art performance in both vocal separation and melody transcription tasks, underscoring the efficacy and versatility of Mel-RoFormer in modeling complex music audio signals.",
      "author_changes": "",
      "authors": [
        "Wang, Ju-Chiang*",
        " Lu, Wei-Tsung",
        " Chen, Jitong"
      ],
      "authors_and_affil": [
        "Ju-Chiang Wang (ByteDance)*",
        " Wei-Tsung Lu (New Your University)",
        " Jitong Chen (ByteDance)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM63P4TF",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        "MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1XEo3yfUAx6ISDCYN67ygpMc6zSu4FY8S/view?usp=drive_link",
      "poster_pdf": "https://drive.google.com/file/d/1ni7Grl9p8OQYLbsJHJJk3R-nOcURBOR0/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-12-mel-roformer-for",
      "title": "Mel-RoFormer for Vocal Separation and Vocal Melody Transcription",
      "video": "https://drive.google.com/file/d/1gARjs4Ww0qBo9NH-c1o0Ue9jT70g1rya/view?usp=drive_link"
    },
    "forum": "374",
    "id": "374",
    "pic_id": "https://drive.google.com/file/d/1_qodZGdIbsylkBF8QUBS4Lpbb9UbF8DV/view?usp=drive_link",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape  valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.",
      "abstract": "Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape  valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.",
      "author_changes": "",
      "authors": [
        "Huang, Jingyue*",
        " Chen, Ke",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Jingyue Huang (New York University)*",
        " Ke Chen (University of California San Diego)",
        " Yi-Hsuan Yang (National Taiwan University)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MXSA49",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "Musical features and properties -> expression and performative aspects of music; Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1DD8pEMU4ladZZ6I-uhE5aa7qkGikQNqf/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1u_bBXAgQDTiTWfHiEhRCyaPDXZk7R4iM/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-17-emotion-driven-piano",
      "title": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation",
      "video": "https://drive.google.com/file/d/17ZHRJcK-nu5-nYVTEHhnQhYz70-up6uC/view?usp=sharing"
    },
    "forum": "376",
    "id": "376",
    "pic_id": "https://drive.google.com/file/d/1QYtTDuGUT2ZvswQDMTF-TTzoxf9HuYIy/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed time intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. This paper introduces a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only structured non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset.",
      "abstract": "The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed time intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. This paper introduces a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only structured non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset.",
      "author_changes": "",
      "authors": [
        "Yan, Yujia*",
        " Duan, Zhiyao"
      ],
      "authors_and_affil": [
        "Yujia Yan (University of Rochester)*",
        " Zhiyao Duan (University of Rochester)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UM63SRNH",
      "day": "4",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "TRUE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1gMc5ubAqdEcvUbcer9u5d59u_yi014t3/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1ArJhcjqNBA7D00oz8_Hs7DX0h0o9BNsB/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-01-scoring-time-intervals",
      "title": "Scoring Time Intervals Using Non-Hierarchical Transformer for Automatic Piano Transcription",
      "video": "https://drive.google.com/file/d/11_86V7lHMEzbzDg9dBcqwO5L4DNtMv6D/view?usp=drive_link"
    },
    "forum": "382",
    "id": "382",
    "pic_id": "https://drive.google.com/file/d/1EnZC4AvxXCxtBRES8CQ_H1OUJCyng-Bk/view?usp=drive_link",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create -- LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results",
      "abstract": "A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create -- LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results",
      "author_changes": "",
      "authors": [
        "Doh, Seungheon*",
        " Choi, Keunwoo",
        " Kwon, Daeyong",
        " Kim, Taesoo",
        " Nam, Juhan"
      ],
      "authors_and_affil": [
        "Seungheon Doh (KAIST)*",
        " Keunwoo Choi (Genentech)",
        " Daeyong Kwon (KAIST)",
        " Taesoo Kim (KAIST)",
        " Juhan Nam (KAIST)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9FA5DMM",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        "MIR tasks -> indexing and querying"
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/11H7CUJNQ0ZKToLGjuvm46d9q0aGfxQyq/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/1gF09iN985cGJMCuVSXhldyBnfCqODT_B/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-17-music-discovery-dialogue",
      "title": "Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Model",
      "video": "https://drive.google.com/file/d/10AwIg1xgUeN5UzeT-69VoKQdXn-ZLa5l/view?usp=sharing"
    },
    "forum": "396",
    "id": "396",
    "pic_id": "https://drive.google.com/file/d/1giCjNyrAIR16lhNMkcIREsY5JB3qf3xA/view?usp=drive_link",
    "position": "18",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",
      "abstract": "The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",
      "author_changes": "We thank the reviewers and meta-reviewer for their valuable insight and feedback on our work. We address here the main concerns that emerged through the reviewing process:\n\n- Lack of context about the kind of music assumed to be the target of the method: We added a mention in our contribution that this work addresses the analysis of structure mostly for western popular music. This decision was based upon the availability of annotated data and the label taxonomy employed in previous work, so as to ease comparison with our method. In fact, the extension of the approach to different types of musical structures is a research direction we aim to undertake, and mention it in the conclusion of the paper. \n\n- Lack of certainty that the evaluation is fair: Our evaluation process has tried to closely follow that of previous work, where baseline systems are evaluated both in cross-validation and cross-dataset settings. However, comparisons should still be cautiously apprehended as some baselines used additional training data or augmentation strategies. Concerning the reported baseline results, we added a row in Table 1 to include the different configurations of one of them (SpecTNT). \n\n- Lack of motivation for using graph neural networks, instead of a simpler Transformer approach: The graph neural network framework is actually a generalization of the transformer approach when applied on a fully-connected graph. We added however a few words as to why graph neural networks are employed in our method (mainly to include link features into the attention coefficients calculation and the frame feature update).\n\n- Lack of depth in the ablation study: Our initial ablation study aimed at focusing on the most peculiar aspects of our method. However, we ran additional ablation experiments by also discarding the remaining steps of our method and updated Figure 4 accordingly. ",
      "authors": [
        "Buisson, Morgan*",
        " McFee, Brian",
        " Essid, Slim"
      ],
      "authors_and_affil": [
        "Morgan Buisson (Telecom-Paris)*",
        " Brian McFee (New York University)",
        " Slim Essid (Telecom Paris - Institut Polytechnique de Paris)"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MY243B",
      "day": "1",
      "keywords": [
        "",
        "Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "FALSE",
      "meta_review": "The reviewers ranged widely in their view of this paper. All had constructive criticisms, on different aspects of the paper. The concerns included:\n\n- lack of context about the kind of music assumed to be the target of the method;\n- lack of certainty that the evaluation is fair;\n- lack of motivation for using graph neural networks, instead of a simpler Transformer approach;\n- lack of depth in the ablation study.\n\nIn the discussion between reviewers, we all agreed that these were valid concerns, but we disagreed about their severity. We hope that the authors can take all of the suggestions on board as they revise this paper.",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://drive.google.com/file/d/1J7VODLY0uirvo78S5oTCZ2Sj1uZWwc8H/view?usp=sharing",
      "poster_pdf": "https://drive.google.com/file/d/14wfben86kWD1tsG3bRS-8e_RcM5kjQ6V/view?usp=sharing",
      "review_1": "The paper proposes a music structure analysis technique based on pairwise link prediction using graph neural networks. A general criticism I have of this work is a problem that is noticeable in various studies in this field: before starting a proposal for music structure analysis, there is never a preliminary musical discussion. Specifically:\n\n1- Repertoire as a corpus: What is the repertoire to be used as corpus? This only appears in the discussion of the experiment.\n2 - Notion of musical structure: Given a specific corpus, what is the notion of musical structure and what should be examined in the corpus to highlight this structure?\n3 - Detection difficulties and tools: Given a specific corpus, what are the difficulties in detecting structure and what tools have already been used to address this challenge?\n\nFor example, the audio representation used in this case is a MEL spectrogram representation with slices centred around each detected beat position. This means that there is an extremely limited universe of corpora that this algorithm can analyse, i.e. music where the beat is a crucial element.\n\nTherefore, I strongly suggest a better contextualisation of the tasks and the corpus for which this tool is intended, trying not to ignore the problems of musical nature involved in tasks of musical structure analysis. ",
      "review_2": "The paper contains a novel approach, is mostly well-written and has high-quality Figures (and very estetically pleasant). However, as I wrote above, there are two points which I find very problematic. I believe that correcting them could make for a very strong paper. In the following there is a list of other minor problems and suggestions.\n\n11: \u201cas whether connecting elements from the same segment or section\u201d. Something sounds wrong in this sentence. I would suggest changing it to \u201cas belonging to the same section (or segment) or not\u201d.\n15: \u201cboundary locations between musical segments and section labels\u201d. This is confusing for me. Is the music consisting of a an alternance of musical segments and section labels?\nThe terms \u201csegment\u201d and \u201csection\u201d and \u201cstructural entity\u201d are used in the abstract without specifying if they are synonyms, or if they are not, in what they differ. This is a bit confusing for a reader that is approaching the field. I suggest using only one term if possible, or explaining the differences between the terms.\n29: typo. \u201cThese\u201d refer to a plural but \u201ccorpus\u201d is singular. The authors can use either \u201ccorpuses\u201d or \u201ccorpora\u201d.\n36: what is an \u201cevent\u201d? A sound from an instrument? And is the term \u201cmusical observation\u201d used in the next sentence synonym of \u201cevent\u201d? The part up to line 40 is not very clear to me.\n38: what is the \u201cmulti-level dependency\u201d. With the term \u201cdependency\u201d I imagine a link between two things. How can it be multi-level? The paper [1] speaks about \u201cmultiple timescales, but not about dependencies. This term is used further in the Section, but I can\u2019t understand what it means.\nSection 1.1 or 1.2: the gap sentence is missing. What is the problem in the current approaches that is solved in this paper? This is the core point of scientific papers, and without it, this paper loses a lot of its interest. The sole goal of \u201cdoing something different\u201d is not so appealing to a reader. I invite the authors to find something that their model can do better than others and to write it very clearly.\n42: how is the paper [7] relevant to to the point of the paragraph? This is a graph neural network paper that doesn\u2019t deal with music. Moreover, is [16] also part of this group of papers that uses self-attention? If yes, it should be added.\n85: a new term \u201caudio observation\u201d without a very intuitive meaning is introduced. Do the authors mean \u201caudio frame\u201d? In this case, I would use this more common term.\n\nI\u2019m extremely confused about the use of the GNN. A GNN is useful when there is a predefined graph structure to leverage. When connecting everything to everything (like \n\n180-185: How can an MLP limit the oversmoothing problem? In my understanding, if the representations are too similar, there is nothing an MLP (which receives such representations as input) can do. A citation could help here if this is true. \n\nThe claim that the system is lightweight and has low parameters should be supported by numbers. But the number of parameters, or the training time for other papers is missing.",
      "review_3": "\u2014 RESUME \u2014\nThis paper presents a novel structure analysis method that outputs each section's boundaries and labels. The main idea is to exploit the pairwise links between audio segments, i.e., predict whether a pair of audio segments below has the same structure. The authors suggest aggregating all these connections into a graph, which is then used to address the tasks in supervised learning.SSM\n\n\u2014 SCIENTIFIC CONTRIBUTION \u2014\nThe main contribution lies in applying graph neural networks for modelling music structures, a novel approach that opens up new avenues for music analysis and machine learning. \n\n\u2014 METHODOLOGY \u2014\n\nThe authors propose a three-step method:\n1- Feature extraction: this step creates the best feature for capturing the pairwise links and builds a self-similarity matrix (SSM) as an adjacency matrix. This process consists of:\n\n\ta) Audio frame selection: it reduces the length of the audio sequence by selecting frames around the beats. Beats are obtained via a pertaining beat detection method. It's not explicitly said, but I guess this is due to memory limitations. Otherwise, why is there interest in doing that? And what's the reason that backs up the hypnosis that beat frames are more informative than off-beat frames regarding structure detections?  \n\tb) Frame encoder transforms the audio signal into a feature vector that captures structure information. This block is based on previous works and applies self-supervised independently of the following blocks. It outputs a feature vector X. \n\tc) Feature refinement: This step involves making each frame exchange information between all the other frames in the track. The final features are called X'. This step is quite exciting and introduces the first graph network block. It is based on the hypothesis that the features obtained from the frame encoder are independent and, thus, won't capture proper pairwise links nor share any information between them. While this sounds intuitive, there are no experiments to back up this hypothesis since no ablation study has been conducted to test the importance of this feature refinement block.\n\nOn top of these feature vectors, the author gets an adjacency matrix that resumes all the pairwise connections between audio frames. They called this matrix A'.\n\n2- A method to find and classify each link between features. The authors define three types of pairwise links: same segment, same section, and different section. I don't see an interest in differentiating between segments that belong to the same segment from those that belong to the same section since the former is a subcategory of the latter. Why is this distinction needed?  \n\ta) The authors propose a CNN block to process the SSM to find regular homogeny areas repeated over time. The idea is that these areas correspond to the final structure of the song. This step aims to categorise (and impose) pairwise links between the frames w.r.t. the overall song structure. The output has the same dimension as A', called E'.\n\tb) They add positional embedding to distinguish between the same segment/section category. I need clarification about why this is required and its utility is never shown in the ablation studies. \n\nHere, we have the first loss, which classifies each E' component into one of the three categories they define. It is not detailed how the cross-entropy loss is applied in this context. I'm assuming the loss is computed per row (?), where the index row serves as a reference point, and the loss evaluates the rest of the components within the row. \n\n3- The final block combines the acoustics information X' with the matrix E', a refined version highlighting mutual information. This block consists of a graph attention mechanism. The output is a final feature vector X'' used for the final boundary and label classifications. They add a final regularisation term to encourage orthogonality between label classes. The author fixed the number of possible classes to K for the whole dataset. \n\nOne missing explanation is that one song is equal to a batch to be sure that all the selected points after the beat track step below the same song to avoid refining them with features from other songs. How does this affect the training? i.e. since all the points in a batch share the same musical traits, isn't that problematic when propagating the gradient? \n\n- RESULTS AND DISCUSSION - \n\nThe authors validate their model on two datasets, RWC-Pop and Harmonix. However, they don't use the Jazz Structure Dataset (JSD) or Salami, which are commonly used to benchmark the music structure task.\nIt is good to see a k-fold cross-validation study and a cross-dataset evaluation. This helps assess the model's actual performance while providing proper metric ranges.\n\nI appreciate the ablation study, but it misses some important experiments. I miss some further studies of each step independently:\n\n1- On the feature extraction step to address the quality of the SSM matrices obtained. This could have been done by comparing them directly with the ground-truth annotation. Moreover, the benefit of the Feature refinement block has yet to be tested. How much does it contribute to the overall performance? Is it better than computing the adjacency matrix on top of the feature produced by the feature encoder? The fact that the results drop so much when removing the Link Feature block may indicate that the obtained SSM A' is not informative enough for detecting structures. \n\n2\u2014 Link Feature block: Since this block can process any SSM, how powerful is it? I would have been curious to see the performance when training this block on top of another SSM. For instance, a good baseline could have been to compare the performance obtained with SSM obtained from well-known musical feature descriptors such as chroma, MFCC, or tempo grams vs. their proposed feature extraction method. \n\n3- Combining link features with acoustics features. Since this block consists of an attention mechanism that combines X' and E', how does it work for the ablation experiment where the link feature (which produces E') is removed? Similarly, how informative is the E' matrix w.r.t the final tasks? Why do we need a combination of both?\n\nA good point is the small size of the model\u2014only 330K. To better understand its significance, it would have been more informative to include the number of parameters of the compared model.  \n\nThe significant insight here is that their model can effectively generate a feature vector X'' that captures structural section information. This is an interesting observation because their feature vector is correlated with meaningful musical sections. I'm curious about how widely applicable this insight is. The authors have set the number of sections to K = 7 (they don't mention their taxonomy), which may not be representative of all possible music sections but is likely sufficient to cover most popular Western music.\n\n\u2014 FINAL COMMENT \u2014\nThe utilization of graph neural networks is quite fascinating. The pairwise relationship hypothesis is intriguing, and the outcomes emphasize the grouping capabilities of their final feature vector. However, they introduce a three-step model that is primarily tested as a whole, making it difficult to evaluate the contribution of each component as well as their reusability out of their full formalisation. Furthermore, many of the design choices made need to be justified or properly evaluated. ",
      "session": [
        "1"
      ],
      "slack_channel": "p1-20-using-pairwise-link",
      "title": "Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis",
      "video": "https://drive.google.com/file/d/1ItBVYLES5QsKMp0kVJ6DAfRvT3JgHrqa/view?usp=drive_link"
    },
    "forum": "405",
    "id": "405",
    "pic_id": "https://drive.google.com/file/d/1DZNsWdPBl_rb51o_SxePiDnzqOt7bPY4/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Lucas S. Maia, Mart\u00edn Rocamora, Luiz W. P. Biscainho, Magdalena Fuentes"
      ],
      "authors_and_affil": [
        "Lucas S. Maia, Mart\u00edn Rocamora, Luiz W. P. Biscainho, Magdalena Fuentes*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UQ1Q1WE6",
      "day": "1",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/170/files/66434fd260250.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Mu4TZR7CyZvRZ-fBeQhDMZbxingjkQFr/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-10-selective-annotation-of",
      "title": "Selective Annotation of Few Data for Beat Tracking of Latin American Music Using Rhythmic Features",
      "video": "https://drive.google.com/file/d/1EoxlhQcN2jUC6ZIATBrke-TrjLdZOk1B/view?usp=drive_link"
    },
    "forum": "510",
    "id": "510",
    "pic_id": "https://drive.google.com/file/d/1XVG8U5QsIu466JvgbKwrd3neXkqM5SnB/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji"
      ],
      "authors_and_affil": [
        "Stefan Uhlich, Giorgio Fabbro*, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9H0EK2B",
      "day": "1",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/172/files/661fc9bdf1264.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1nlSc9U3nQnb0YIbopOuJ9zJRorr_g7-J/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-10-the-sound-demixing",
      "title": "The Sound Demixing Challenge 2023 \u2013 Cinematic Demixing Track",
      "video": "https://drive.google.com/file/d/1BuSK6EAat0nq9HwatIOL6XnZ30CJe8Uy/view?usp=drive_link"
    },
    "forum": "511",
    "id": "511",
    "pic_id": "https://drive.google.com/file/d/1CDwFB9U5aO_1ZaCjtAgaPFhiiqn9Dwck/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Yigitcan \u00d6zer, Simon Schw\u00e4r, Vlora Arifi-M\u00fcller, Jeremy Lawrence, Emre Sen, and Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Yigitcan \u00d6zer, Simon Schw\u00e4r, Vlora Arifi-M\u00fcller, Jeremy Lawrence, Emre Sen, and Meinard M\u00fcller*"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07USJF0M8C",
      "day": "2",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/160/files/6501b44ff32d9.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1kIG7BA9W4UW50walYN7YM4-XdJ2BUnWq/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-10-piano-concerto-dataset",
      "title": "Piano Concerto Dataset (PCD): A Multitrack Dataset of Piano Concertos",
      "video": "https://drive.google.com/file/d/1Sa42785sl1JOyLRjWGbr4ACCeY43lp1A/view?usp=drive_link"
    },
    "forum": "512",
    "id": "512",
    "pic_id": "https://drive.google.com/file/d/1ruV3sh4cOKTG8JRcs2HYbyVsvfbKOHhH/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Simon Schw\u00e4r, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Simon Schw\u00e4r*, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard M\u00fcller"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9FACZDM",
      "day": "2",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/166/files/65d89725d616e.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1CYaVjfKHx3wKkEBvak4wmi4tq-YXcEit/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-10-a-dataset-of",
      "title": "A Dataset of Larynx Microphone Recordings for Singing Voice Reconstruction",
      "video": "https://drive.google.com/file/d/1Kri3iUu08qgw67fAQw4VzPZXXzTxuGcB/view?usp=drive_link"
    },
    "forum": "513",
    "id": "513",
    "pic_id": "https://drive.google.com/file/d/1enDWiGur_9rUoPcshOAsAKkfaUjz5XOV/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Christof Wei\u00df, Vlora Arifi-M\u00fcller, Michael Krause, Frank Zalkow, Stephanie Klauk, Rainer Kleinertz, and Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Christof Wei\u00df, Vlora Arifi-M\u00fcller*, Michael Krause, Frank Zalkow, Stephanie Klauk, Rainer Kleinertz, and Meinard M\u00fcller"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07UHCYTLQN",
      "day": "3",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/161/files/653900bdd2cd7.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1sh50djvbVTCB05xMBW0jjxO3ecRWoHjp/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-10-wagner-ring-dataset",
      "title": "Wagner Ring Dataset: A Complex Opera Scenario for Music Processing and Computational Musicology",
      "video": "https://drive.google.com/file/d/10V8FLOa9-pdpmTe-rrlAN8AEo8poxBiY/view?usp=drive_link"
    },
    "forum": "514",
    "id": "514",
    "pic_id": "https://drive.google.com/file/d/1vBUJ9LxCcaOu8fUhDEDcpLypQS9lFl8u/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Gen\u00eds Plaja-Roglans, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron"
      ],
      "authors_and_affil": [
        "Gen\u00eds Plaja-Roglans*, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07VCR8EVC0",
      "day": "3",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/137/files/6499954ec1b0b.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1c0Csr3IZrEEsF8klvIve7SypcBMUssWL/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-10-repertoire-specific-vocal",
      "title": "Repertoire-Specific Vocal Pitch Data Generation for Improved Melodic Analysis of Carnatic Music",
      "video": "https://drive.google.com/file/d/14GqI2ULCkTiWLrm3iUpAdkqLV0t4fyW6/view?usp=drive_link"
    },
    "forum": "515",
    "id": "515",
    "pic_id": "https://drive.google.com/file/d/1kxxmWxBNn2foiu-p7gMGOiR37tnskogh/view?usp=drive_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Yu Zhang, Ziya Zhou, Xiaobing Li, Feng Yu, Maosong Sun"
      ],
      "authors_and_affil": [
        "Yu Zhang, Ziya Zhou*, Xiaobing Li, Feng Yu, Maosong Sun"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2PNJ6FK",
      "day": "4",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/146/files/64aea0bb77021.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1GaX3bTk4UDG-OZ9GM14wZpFqoV42bxr-/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-10-ccom-huqin-an",
      "title": "CCOM-HuQin: An Annotated Multimodal Chinese Fiddle Performance Dataset",
      "video": "https://drive.google.com/file/d/1Bu5VrSktbhzFZoOEPL4RT25PfTjv6D9O/view?usp=sharing"
    },
    "forum": "516",
    "id": "516",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Drew Edwards, Simon Dixon, Emmanouil Benetos"
      ],
      "authors_and_affil": [
        "Drew Edwards*, Simon Dixon, Emmanouil Benetos"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07U9FAKZ7Z",
      "day": "4",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/162/files/650445222bf95.pdf",
      "poster_pdf": "https://drive.google.com/file/d/13YThwqFz8HwvvAMICm-kWlHjswQs5TmO/view?usp=sharing",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-18-pijama-piano-jazz",
      "title": "PiJAMA: Piano Jazz with Automatic MIDI Annotations",
      "video": "https://drive.google.com/file/d/1aTJAffJoOecS_Iady6BqKgQSwjXlPzRB/view?usp=sharing"
    },
    "forum": "517",
    "id": "517",
    "pic_id": "https://drive.google.com/file/d/1IOZKuKEfq_De9g9a1atWw3czP4dGbG9N/view?usp=sharing",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Silvan David Peter, Carlos Eduardo Cancino-Chac\u00f3n, Francesco Foscarin, Andrew Philip McLeod, Florian Henkel, Emmanouil Karystinaios, Gerhard Widmer"
      ],
      "authors_and_affil": [
        "Silvan David Peter*, Carlos Eduardo Cancino-Chac\u00f3n, Francesco Foscarin, Andrew Philip McLeod, Florian Henkel, Emmanouil Karystinaios, Gerhard Widmer"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2MYGCLR",
      "day": "3",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/149/files/6499a04b4a67d.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ngBHOzpFe6KJqf-SFGONyqUD_gjTotfp/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-18-automatic-note-level",
      "title": "Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset",
      "video": "https://drive.google.com/file/d/1z3jlHFGtFEDkQvFKlT5RWwy8_bD4ZICM/view?usp=drive_link"
    },
    "forum": "518",
    "id": "518",
    "pic_id": "https://drive.google.com/file/d/1PPCSE4er97h5bdquGakChSz-4sxCZflI/view?usp=drive_link",
    "position": "19",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "author_changes": "",
      "authors": [
        "Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Mart\u00ednez-Ram\u00edrez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert St\u00f6ter, Alexandre D\u00e9fossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji"
      ],
      "authors_and_affil": [
        "Giorgio Fabbro*, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Mart\u00ednez-Ram\u00edrez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert St\u00f6ter, Alexandre D\u00e9fossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji"
      ],
      "channel_url": "https://ismir2024.slack.com/archives/C07V2PNPH33",
      "day": "3",
      "keywords": [
        ""
      ],
      "long_presentation": "FALSE",
      "meta_review": "",
      "paper_presentation": "San Francisco",
      "pdf_path": "https://transactions.ismir.net/articles/171/files/66210ffd7fc02.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1RPTcdizaKOjQxTpTvB4feswN9ifXgamK/view?usp=drive_link",
      "review_1": "",
      "review_2": "",
      "review_3": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-20-the-sound-demixing",
      "title": "The Sound Demixing Challenge 2023 \u2013 Music Demixing Track",
      "video": "https://drive.google.com/file/d/13IaTpOvaDBR_4L3rzcblrNTXql_TZVY9/view?usp=drive_link"
    },
    "forum": "519",
    "id": "519",
    "pic_id": "https://drive.google.com/file/d/1Atvao35NkyxwDwPrAE6QG4teWZb9-40b/view?usp=drive_link",
    "position": "21",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  }
]
