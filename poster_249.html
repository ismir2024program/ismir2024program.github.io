

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            ST-ITO: Controlling audio effects for style transfer with inference-time optimization
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Steinmetz, Christian J.*"
               class="text-muted"
            >Steinmetz, Christian J.*</a
            >,
            
            <a href="papers.html?filter=authors&search= singh, Shubhr"
               class="text-muted"
            > singh, Shubhr</a
            >,
            
            <a href="papers.html?filter=authors&search= Comunita, Marco"
               class="text-muted"
            > Comunita, Marco</a
            >,
            
            <a href="papers.html?filter=authors&search= Ibnyahya, Ilias"
               class="text-muted"
            > Ibnyahya, Ilias</a
            >,
            
            <a href="papers.html?filter=authors&search= Yuan, Shanxin"
               class="text-muted"
            > Yuan, Shanxin</a
            >,
            
            <a href="papers.html?filter=authors&search= Benetos, Emmanouil"
               class="text-muted"
            > Benetos, Emmanouil</a
            >,
            
            <a href="papers.html?filter=authors&search= Reiss, Joshua D."
               class="text-muted"
            > Reiss, Joshua D.</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MV1GN5" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p5-01-st-ito-controlling</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; music signal processing"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; music signal processing</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; transformations; MIR tasks -&gt; music synthesis and transformation; MIR tasks -&gt; similarity metrics; Musical features and properties -&gt; timbre, instrumentation, and singing voice"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; transformations; MIR tasks -&gt; music synthesis and transformation; MIR tasks -&gt; similarity metrics; Musical features and properties -&gt; timbre, instrumentation, and singing voice</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1UVtuCnE7K0tByHHGlVoMRKldDgf7me1Y/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1cT7XVw1JPxlybJYhaApaFAebYeY3u4l5/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1LgitojSd5dpzY2_71D0hoErCGiugaeo_/preview" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>This paper proposes an inference-time optimization approach to estimating parameters of an audio effect chain based on audio effect similarity between the output and query for audio effect style transfer.</p>
<p>The paper proposes a novel approach, conducts thorough experiments, and presents the ideas and results clearly. However, reviewers do point out some limitations of this work. In particular, the (missing) invariance of the embedding to the content is the biggest limitation. As the paper tries to model the audio quality instead of the underlying audio content, this (missing) invariance is an important issue and should be discussed in the paper.</p>
<p>Overall, all reviewers and the independent meta-review liked the idea and contributions of this work, and recommended "strong accept" or "weak accept". One reviewer and the meta-review also recommended "award quality", and the other reviewers did not object this recommendation.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>The paper proposes a new approach for audio effect parameter estimation from a reference recording. The approach uses a learned deep embedding that should discriminate different "styles" while being invariant to content, which is also the main focus of the work. The actual optimization method is mostly treated as a black box, while a comprehensive evaluation is included to test the approach for two different tasks (style classification and style transfer).</p>
<p>This is a relevant contribution with a good contextualization, interesting experiments, and a meaningful selection of baselines. However, there are several issues that, in my opinion, should be addressed in a possible publication.</p>
<p>(1) I'm not sure I understand how the pretext tasks ensures a disentanglement of (or an invariance to) the content of the audio signal. A thought experiment: If the MLP for a classifier task just learns to calculate $z_i - z_o$ in the first layer, the individual embeddings could theoretically still include all information about $x_i$ and $x_o$, respectively, while the classification can then focus on the style differences. So in a sense, $g$ is clearly encouraged to <em>include</em> information about the effects in the embedding but not discouraged to <em>exclude</em> content information? This could also explain the wide spread of cosine similarity of the Oracle condition in Fig. 5. I wonder if a different pretext task, like contrastive learning, could be beneficial here.</p>
<p>(2) The subjective results are quite inconclusive, which is also congruent with my impression when listening to the provided audio examples. A few observations/thoughts on the listening test:
* Why is the Oracle condition detected so inconsistently by the participants? Is there maybe a difference between individuals, so that if those who do not somewhat consistently identify the Oracle are excluded, results become more pronounced?
* I think the music examples are not very good for the purpose, since the input itself already appears to be heavily processed.
* Also, the actual task is a bit unclear from the paper. I assume that 100 means most similar to the reference and 0 means completely dissimilar, while participants where asked to consider only style and not content?</p>
<p>(3) The parameter estimation experiment does not take any interaction between parameters (within and across effects) into account. While this isolated experiment is an interesting starting point, it does not allow any conclusions about the "high-dimensional loss landscape" that is described by the embedding distance. It would be interesting to have a meaningful objective evaluation for "bad" local minima, which clearly seem to exist.</p>
<p>(4) The strategy of sampling parameter configurations for training the embedding model appears a bit odd to me. While this method makes reasonably sure that the 10 presets for each effect are quite distinct, there is no guarantee that "perceptually diverse" parameters are also "perceptually meaningful". Since some parameters are strongly interrelated (even between different plugins in the chain) and some parameters can be quite sensitive to change around their optimal working point (e.g. the delay speed around the actual tempo of the song), it may be better to train with manually tuned presets (which may be difficult to obtain to be fair)?</p>
<p>(5) What is the meaning of the numbers in Table 1? Accuracy? F-Measure?</p>
<p>(6) Is there a plan to publish code and/or pre-trained models? Especially the embedding model could be useful as a baseline or starting point for many different tasks related to audio effects.</p>
<p>(7) Zero-shot style classification: Could forming the prototype as the mean embedding of multiple examples from each class instead of taking just one example be more robust?</p>
<p>(8) I think it would have been good to reflect the focus on the learned embedding in the title of the paper. (Not sure if it can still be changed.)</p>
<p>(9) Some minor issues:
* How are the actual "candidate parameters" sampled in the CMA-ES method? Is this the mean of the population or the population member with the smallest distance to the reference?
* l. 48: "adapt based on the external context" is unclear to me.
* Figure 3 caption: $x_0$ should be $x_o$ in the second line.
* l. 161: to be consistent with Fig. 3, it should be $z_o$ instead of $z_r$​ here, I think.
* Eq. 3 and l. 205: Similarly, $z_i$ should be $z_o$ to be consistent with $x_o = f_c(...)$​?
* l. 295 space missing
* Table 1: While the meaning of each abbreviation can be guessed, they are not introduced anywhere (and they are in a different order than introduced in the text)
* l. 308: verb missing
* l. 342: period missing
* Table 2: How to obtain a "correlation coefficient" is only briefly explained in the table caption. This could be done more comprehensively in the text.
* ll. 381-382: "as good of" -&gt; "a comparable"
* Fig. 6: The grouping of the subfigures makes it a bit difficult to compare related experiments.
* Fig. 6: I assume that the grey bar is for the Input as in Fig. 5?</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper introduces a novel system for audio effect style transfer. It thoroughly describes the method and compares it with most previous works in this field through relevant experimentation. Various audio production style transfer tasks are benchmarked, showcasing the effectiveness of the method. Real-world audio production scenarios for arbitrary effects are also explored and evaluated via a listening test.</p>
<p>The paper has significant contributions. An elusive audio effect metric similarity has been achieved, and gradient-free optimization is proposed as a great alternative to deal with the inherent difficulty of handling non-differentiable audio processors within learned-based systems.</p>
<p>I believe the paper is of great interest for this conference, and thus my overall evaluation is Strong Accept.</p>
<p>One limitation that could be included in the discussion is that, due to the inherent nonlinearity of various audio effects, the computation of "Oracle" is ill-posed. This means that when having recordings with different content, A and B, applying an audio effects chain Fx with parameters Wx won't necessarily yield the same music production style. This depends on the type of processing (or lack of) that A and B have. Fx_Wx(A) is not (always) equal to Fx_Wx(B) in terms of audio effect style similarity. Of course, the system seems to be performing well when even with this being omitted, but it could be included in the discussion to further strengthen the robustness and scientific contribution of this paper.</p>
<p>For further discussion, the paper could benefit from authors commenting on the effectiveness of AFx-Rep for style transfer of combinations of various audio effects, even though this metric was trained only with one effect at a time. What insights or intuition do the authors have about the reported performance of the system, as one would expect that a metric trained with one effect at a time could struggle to encode information relevant to various audio effects being applied to an audio signal?</p>
<p>Also, it is reported that the system struggles for parameter estimation for chorus. Is there any insight from the authors that somehow time-varying modulation effects are harder to model? The paper could also benefit from a brief clarification of this.</p>
<p>Finally, more details about the listening test could have been reported. What type of multiple stimulus test was performed? What question was asked to the participants? What were the listening conditions? Are any p-values relevant to the given results?</p>
<p>Minor comments about the paper:</p>
<ul>
<li>
<p>Figure 1: Could "sim" be replaced with AFx-Rep? This could highlight AFx-Rep as one of the main contributions of this work.</p>
</li>
<li>
<p>Section 1, Line 85: Add a reference to DeepAFx-ST.</p>
</li>
<li>
<p>Section 1, Line 91: The subjective listening test should be included as a contribution only if there is novelty in the subjective listening test presented, which I believe is not the case. Thus, I suggest removing this from the list of contributions.</p>
</li>
<li>
<p>Section 3, Line 249: It is mentioned before that 20,000 segments were taken, but then it is said that random crops are applied as augmentations. It is not clear how these crops are being applied to the input and/or output. Is it always applied to both? What type of crops? Clarifying this could ease reproducibility of the paper.</p>
</li>
<li>
<p>Section 5.1: Although the authors mention that it is difficult to draw conclusions with the audio production Style metric alone, readers would see that since ST-ITO models were trained using AFx-Rep, these models will perform better when measured via AFx-Rep. It could be interesting to also see how this evaluation goes when using the metrics that DeepAFx systems were trained with.</p>
</li>
</ul></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This work presents a self-supervised embedding trained to specifically attend to production style. This is coupled with a generic control strategy based on derivative-free optimization methods. The use of the embedding generalizes the control process, allowing it potentially to control novel sets of effects.</p>
<p>The proposed system is described quite clearly, employing many diagrams, which help to understand the technical models and experiments. The training and experiment procedures are described in detail.</p>
<p>A classification experiment proves that the system can identify production styles by the embedding, better than existing embedding approaches. Furthermore, attempts are made to mimic real production styles. This is no small task. My impression is that the parameter settings found in some of the real-world style transfer do not fit as well as they could. This, if explored, could lead to system improvements.</p>
<p>I have a doubt is regarding the Oracle condition in the "Real world style transfer" task. For instance, now I see that the reason that the Oracle has non-zero error, is because the Oracle is still being compared to the reference embedding, which is a signal with entirely different content. I believe this may hide estimation errors which may be under the threshold of inherent differences (due to content difference) between the reference and Oracle conditions.</p>
<p>However, wouldn't it make sense to also compare the error (both in embedding space and parameter space) of the Oracle condition versus the algorithm conditions? In this way, every small deviation in parameter estimation would be visible (in either domain), potentially shedding more light on the performance of both the optimization algorithm, and the ability of the embedding to represent small changes in parameters.</p>
<p>As someone interested in optimization, I wonder about the landscape of the objective function itself. This could be shown with simple example with few parameters, plotting the cosine similarity directly. Furthermore, I wonder how capable the gradient-free method chosen (CMA-ES) is really able to solve the problem.</p>
<p>In future work, I think more could be done to deepen the evaluation. In my opinion, the optimization method could also have been better justified. All in all, I appreciate this paper for the clarity of its approach and descriptions, for a task as challenging as automatic style transfer.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We would like to thank the reviewers for their detailed comments and suggestions. We have addressed these comments to the extent possible in the camera-ready submission. The changes include:</p>
<p>1) Rephrasing the discussion on previous work in Sec. 1, specifically regarding the “context-dependent” nature of audio production raised by R2 and MR. This discussion aims to underline the limitations of previous works, both rule-based and machine learning systems, which treat audio production as a one-to-one mapping. This motivates style transfer systems, which adapt based on user input.</p>
<p>2) Providing further details on the listening study as requested by R2. Participants were asked to rate each stimulus on a scale from 0 to 100, considering its similarity to a reference recording while ignoring differences in content and focusing on audio production.</p>
<p>3) Clarifying the role of the Oracle, which all reviewers raised questions about. The Oracle simply takes the input recording and applies the same parameter configuration used to create the reference. Since the input recording may have a different “starting point” from the unprocessed reference, simply applying the same parameters may not result in an ideal style transfer. This was reflected in our evaluation, with listeners sometimes rating the Oracle lower than other methods.</p>
<p>4) Addressing important points about the audio effect representation (AFx-Rep) raised by R2 and MR regarding its lack of invariance to content. This is true given our pretraining setup. We added details in Sec. 2 to clarify this potential limitation. However, we further motivated this choice by stating that our method is more scalable, as we can leverage any audio data, including already processed audio. This proves beneficial as it exposes our model to a wider range of effects beyond those we synthetically apply, aiding in more generalized style transfer.</p>
<p>5) Resolving a number of typos and rephrasing some passages based on reviewer suggestions.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;249&#39;, &#39;session&#39;: &#39;5&#39;, &#39;position&#39;: &#39;02&#39;, &#39;forum&#39;: &#39;249&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1kTHqdjx8t_lvCG8cGeKZCIZWKenGOl-s/view?usp=sharing&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;ST-ITO: Controlling audio effects for style transfer with inference-time optimization&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;TRUE&#39;, &#39;authors&#39;: [&#39;Steinmetz, Christian J.*&#39;, &#39; singh, Shubhr&#39;, &#39; Comunita, Marco&#39;, &#39; Ibnyahya, Ilias&#39;, &#39; Yuan, Shanxin&#39;, &#39; Benetos, Emmanouil&#39;, &#39; Reiss, Joshua D.&#39;], &#39;authors_and_affil&#39;: [&#39;Christian J. Steinmetz (Queen Mary University of London)*&#39;, &#39; Shubhr singh (Queen Mary University of London)&#39;, &#39; Marco Comunita (Queen Mary University of London)&#39;, &#39; Ilias Ibnyahya (Queen Mary University of London)&#39;, &#39; Shanxin Yuan (Queen Mary University of London)&#39;, &#39; Emmanouil Benetos (Queen Mary University of London)&#39;, &#39; Joshua D. Reiss (Queen Mary University of London)&#39;], &#39;keywords&#39;: [&#39;MIR fundamentals and methodology -&gt; music signal processing&#39;, &#39;Generative Tasks -&gt; transformations; MIR tasks -&gt; music synthesis and transformation; MIR tasks -&gt; similarity metrics; Musical features and properties -&gt; timbre, instrumentation, and singing voice&#39;], &#39;abstract&#39;: &#39;Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.&#39;, &#39;TLDR&#39;: &#39;Audio production style transfer is the task of processing an input recording to impart the stylistic elements from a reference recording. Existing approaches for this task often train a neural network to estimate control parameters for a set of audio effects. However, generalization of these systems is limited due to their reliance on synthetic training data and differentiable audio effects. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a four-part benchmark to comprehensively evaluate both audio production style metrics and style transfer systems. This evaluation demonstrates that our approach enables more expressive style transfer and improved generalization, highlighting the limitations of synthetic training data and differentiable audio effects.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1cT7XVw1JPxlybJYhaApaFAebYeY3u4l5/view?usp=sharing&#39;, &#39;session&#39;: [&#39;5&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1UVtuCnE7K0tByHHGlVoMRKldDgf7me1Y/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1LgitojSd5dpzY2_71D0hoErCGiugaeo_/view?usp=sharing&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MV1GN5&#39;, &#39;slack_channel&#39;: &#39;p5-01-st-ito-controlling&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#39;The paper proposes a new approach for audio effect parameter estimation from a reference recording. The approach uses a learned deep embedding that should discriminate different &#34;styles&#34; while being invariant to content, which is also the main focus of the work. The actual optimization method is mostly treated as a black box, while a comprehensive evaluation is included to test the approach for two different tasks (style classification and style transfer).\n\nThis is a relevant contribution with a good contextualization, interesting experiments, and a meaningful selection of baselines. However, there are several issues that, in my opinion, should be addressed in a possible publication.\n\n(1) I\&#39;m not sure I understand how the pretext tasks ensures a disentanglement of (or an invariance to) the content of the audio signal. A thought experiment: If the MLP for a classifier task just learns to calculate $z_i - z_o$ in the first layer, the individual embeddings could theoretically still include all information about $x_i$ and $x_o$, respectively, while the classification can then focus on the style differences. So in a sense, $g$ is clearly encouraged to *include* information about the effects in the embedding but not discouraged to *exclude* content information? This could also explain the wide spread of cosine similarity of the Oracle condition in Fig. 5. I wonder if a different pretext task, like contrastive learning, could be beneficial here.\n\n(2) The subjective results are quite inconclusive, which is also congruent with my impression when listening to the provided audio examples. A few observations/thoughts on the listening test:\n* Why is the Oracle condition detected so inconsistently by the participants? Is there maybe a difference between individuals, so that if those who do not somewhat consistently identify the Oracle are excluded, results become more pronounced?\n* I think the music examples are not very good for the purpose, since the input itself already appears to be heavily processed.\n* Also, the actual task is a bit unclear from the paper. I assume that 100 means most similar to the reference and 0 means completely dissimilar, while participants where asked to consider only style and not content?\n\n(3) The parameter estimation experiment does not take any interaction between parameters (within and across effects) into account. While this isolated experiment is an interesting starting point, it does not allow any conclusions about the &#34;high-dimensional loss landscape&#34; that is described by the embedding distance. It would be interesting to have a meaningful objective evaluation for &#34;bad&#34; local minima, which clearly seem to exist.\n\n(4) The strategy of sampling parameter configurations for training the embedding model appears a bit odd to me. While this method makes reasonably sure that the 10 presets for each effect are quite distinct, there is no guarantee that &#34;perceptually diverse&#34; parameters are also &#34;perceptually meaningful&#34;. Since some parameters are strongly interrelated (even between different plugins in the chain) and some parameters can be quite sensitive to change around their optimal working point (e.g. the delay speed around the actual tempo of the song), it may be better to train with manually tuned presets (which may be difficult to obtain to be fair)?\n\n(5) What is the meaning of the numbers in Table 1? Accuracy? F-Measure?\n\n(6) Is there a plan to publish code and/or pre-trained models? Especially the embedding model could be useful as a baseline or starting point for many different tasks related to audio effects.\n\n(7) Zero-shot style classification: Could forming the prototype as the mean embedding of multiple examples from each class instead of taking just one example be more robust?\n\n(8) I think it would have been good to reflect the focus on the learned embedding in the title of the paper. (Not sure if it can still be changed.)\n\n(9) Some minor issues:\n* How are the actual &#34;candidate parameters&#34; sampled in the CMA-ES method? Is this the mean of the population or the population member with the smallest distance to the reference?\n* l. 48: &#34;adapt based on the external context&#34; is unclear to me.\n* Figure 3 caption: $x_0$ should be $x_o$ in the second line.\n* l. 161: to be consistent with Fig. 3, it should be $z_o$ instead of $z_r$\u200b here, I think.\n* Eq. 3 and l. 205: Similarly, $z_i$ should be $z_o$ to be consistent with $x_o = f_c(...)$\u200b?\n* l. 295 space missing\n* Table 1: While the meaning of each abbreviation can be guessed, they are not introduced anywhere (and they are in a different order than introduced in the text)\n* l. 308: verb missing\n* l. 342: period missing\n* Table 2: How to obtain a &#34;correlation coefficient&#34; is only briefly explained in the table caption. This could be done more comprehensively in the text.\n* ll. 381-382: &#34;as good of&#34; -&gt; &#34;a comparable&#34;\n* Fig. 6: The grouping of the subfigures makes it a bit difficult to compare related experiments.\n* Fig. 6: I assume that the grey bar is for the Input as in Fig. 5?&#39;, &#39;review_2&#39;: &#39;This paper introduces a novel system for audio effect style transfer. It thoroughly describes the method and compares it with most previous works in this field through relevant experimentation. Various audio production style transfer tasks are benchmarked, showcasing the effectiveness of the method. Real-world audio production scenarios for arbitrary effects are also explored and evaluated via a listening test.\n\nThe paper has significant contributions. An elusive audio effect metric similarity has been achieved, and gradient-free optimization is proposed as a great alternative to deal with the inherent difficulty of handling non-differentiable audio processors within learned-based systems.\n\nI believe the paper is of great interest for this conference, and thus my overall evaluation is Strong Accept.\n\nOne limitation that could be included in the discussion is that, due to the inherent nonlinearity of various audio effects, the computation of &#34;Oracle&#34; is ill-posed. This means that when having recordings with different content, A and B, applying an audio effects chain Fx with parameters Wx won\&#39;t necessarily yield the same music production style. This depends on the type of processing (or lack of) that A and B have. Fx_Wx(A) is not (always) equal to Fx_Wx(B) in terms of audio effect style similarity. Of course, the system seems to be performing well when even with this being omitted, but it could be included in the discussion to further strengthen the robustness and scientific contribution of this paper.\n\nFor further discussion, the paper could benefit from authors commenting on the effectiveness of AFx-Rep for style transfer of combinations of various audio effects, even though this metric was trained only with one effect at a time. What insights or intuition do the authors have about the reported performance of the system, as one would expect that a metric trained with one effect at a time could struggle to encode information relevant to various audio effects being applied to an audio signal?\n\nAlso, it is reported that the system struggles for parameter estimation for chorus. Is there any insight from the authors that somehow time-varying modulation effects are harder to model? The paper could also benefit from a brief clarification of this.\n\nFinally, more details about the listening test could have been reported. What type of multiple stimulus test was performed? What question was asked to the participants? What were the listening conditions? Are any p-values relevant to the given results?\n\nMinor comments about the paper:\n\n- Figure 1: Could &#34;sim&#34; be replaced with AFx-Rep? This could highlight AFx-Rep as one of the main contributions of this work.\n\n- Section 1, Line 85: Add a reference to DeepAFx-ST.\n\n- Section 1, Line 91: The subjective listening test should be included as a contribution only if there is novelty in the subjective listening test presented, which I believe is not the case. Thus, I suggest removing this from the list of contributions.\n\n- Section 3, Line 249: It is mentioned before that 20,000 segments were taken, but then it is said that random crops are applied as augmentations. It is not clear how these crops are being applied to the input and/or output. Is it always applied to both? What type of crops? Clarifying this could ease reproducibility of the paper.\n\n- Section 5.1: Although the authors mention that it is difficult to draw conclusions with the audio production Style metric alone, readers would see that since ST-ITO models were trained using AFx-Rep, these models will perform better when measured via AFx-Rep. It could be interesting to also see how this evaluation goes when using the metrics that DeepAFx systems were trained with.&#39;, &#39;review_3&#39;: &#39;This work presents a self-supervised embedding trained to specifically attend to production style. This is coupled with a generic control strategy based on derivative-free optimization methods. The use of the embedding generalizes the control process, allowing it potentially to control novel sets of effects.\n\nThe proposed system is described quite clearly, employing many diagrams, which help to understand the technical models and experiments. The training and experiment procedures are described in detail.\n\nA classification experiment proves that the system can identify production styles by the embedding, better than existing embedding approaches. Furthermore, attempts are made to mimic real production styles. This is no small task. My impression is that the parameter settings found in some of the real-world style transfer do not fit as well as they could. This, if explored, could lead to system improvements.\n\nI have a doubt is regarding the Oracle condition in the &#34;Real world style transfer&#34; task. For instance, now I see that the reason that the Oracle has non-zero error, is because the Oracle is still being compared to the reference embedding, which is a signal with entirely different content. I believe this may hide estimation errors which may be under the threshold of inherent differences (due to content difference) between the reference and Oracle conditions.\n\nHowever, wouldn\&#39;t it make sense to also compare the error (both in embedding space and parameter space) of the Oracle condition versus the algorithm conditions? In this way, every small deviation in parameter estimation would be visible (in either domain), potentially shedding more light on the performance of both the optimization algorithm, and the ability of the embedding to represent small changes in parameters.\n\nAs someone interested in optimization, I wonder about the landscape of the objective function itself. This could be shown with simple example with few parameters, plotting the cosine similarity directly. Furthermore, I wonder how capable the gradient-free method chosen (CMA-ES) is really able to solve the problem.\n\nIn future work, I think more could be done to deepen the evaluation. In my opinion, the optimization method could also have been better justified. All in all, I appreciate this paper for the clarity of its approach and descriptions, for a task as challenging as automatic style transfer.&#39;, &#39;meta_review&#39;: &#39;This paper proposes an inference-time optimization approach to estimating parameters of an audio effect chain based on audio effect similarity between the output and query for audio effect style transfer.\n\nThe paper proposes a novel approach, conducts thorough experiments, and presents the ideas and results clearly. However, reviewers do point out some limitations of this work. In particular, the (missing) invariance of the embedding to the content is the biggest limitation. As the paper tries to model the audio quality instead of the underlying audio content, this (missing) invariance is an important issue and should be discussed in the paper.\n\nOverall, all reviewers and the independent meta-review liked the idea and contributions of this work, and recommended &#34;strong accept&#34; or &#34;weak accept&#34;. One reviewer and the meta-review also recommended &#34;award quality&#34;, and the other reviewers did not object this recommendation.&#39;, &#39;author_changes&#39;: &#39;We would like to thank the reviewers for their detailed comments and suggestions. We have addressed these comments to the extent possible in the camera-ready submission. The changes include:\n\n1) Rephrasing the discussion on previous work in Sec. 1, specifically regarding the “context-dependent” nature of audio production raised by R2 and MR. This discussion aims to underline the limitations of previous works, both rule-based and machine learning systems, which treat audio production as a one-to-one mapping. This motivates style transfer systems, which adapt based on user input.\n\n2) Providing further details on the listening study as requested by R2. Participants were asked to rate each stimulus on a scale from 0 to 100, considering its similarity to a reference recording while ignoring differences in content and focusing on audio production.\n\n3) Clarifying the role of the Oracle, which all reviewers raised questions about. The Oracle simply takes the input recording and applies the same parameter configuration used to create the reference. Since the input recording may have a different “starting point” from the unprocessed reference, simply applying the same parameters may not result in an ideal style transfer. This was reflected in our evaluation, with listeners sometimes rating the Oracle lower than other methods.\n\n4) Addressing important points about the audio effect representation (AFx-Rep) raised by R2 and MR regarding its lack of invariance to content. This is true given our pretraining setup. We added details in Sec. 2 to clarify this potential limitation. However, we further motivated this choice by stating that our method is more scalable, as we can leverage any audio data, including already processed audio. This proves beneficial as it exposes our model to a wider range of effects beyond those we synthetically apply, aiding in more generalized style transfer.\n\n5) Resolving a number of typos and rephrasing some passages based on reviewer suggestions.\n&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>