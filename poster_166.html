

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Recep Oguz Araz (Universitat Pompeu Fabra)*"
               class="text-muted"
            >Recep Oguz Araz (Universitat Pompeu Fabra)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Xavier Serra (Universitat Pompeu Fabra )"
               class="text-muted"
            > Xavier Serra (Universitat Pompeu Fabra )</a
            >,
            
            <a href="papers.html?filter=authors&search= Dmitry Bogdanov (Universitat Pompeu Fabra)"
               class="text-muted"
            > Dmitry Bogdanov (Universitat Pompeu Fabra)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MSQ4M7" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p3-15-discogs-vi-a</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks"
                    class="text-secondary text-decoration-none"
            >MIR tasks</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; fingerprinting; MIR tasks -&gt; pattern matching and detection; Musical features and properties -&gt; representations of music"
                    class="text-secondary text-decoration-none"
            >Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; fingerprinting; MIR tasks -&gt; pattern matching and detection; Musical features and properties -&gt; representations of music</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1ZLoWu-jji5KbPxEsQtWLJIxxYGASZ919/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/11b84MPrhbqy3NfjtwJRNsJnNKlPLExfe/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1S6tj2j1ocTlGctPUCW7SIr6CuhdifpwX/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>This paper proposes a new dataset for the task of version identification. The dataset is based on the Discogs music database and is substantially larger than related datasets.</p>
<p>The reviewers all agree that the paper and the dataset are a valuable contribution to the community. The paper is well-written and the methodology is clearly described. The reviewers also mention a few minor issues that could further strengthen the paper. </p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>The paper presents a new dataset for the task of version identification that is significantly larger than existing datasets. The demand for datasets for this task is accurate due to the reasons presented (data volume), but mainly because of the difficulty in ensuring data quality (such as intra-click correctness) and the challenge in maintaining and disseminating the data (especially the audio associated with the tracks). Furthermore, obtaining the dataset from a source rich in metadata brought many advantages to the proposed set. Therefore, this paper is relevant.</p>
<p>However, I list some points that the text could be better addressed:</p>
<ul>
<li>While using "official videos" might help with the longevity of link availability, it limits the dataset to less varied versions. For example, the dataset is restricted to good recordings by professional musicians. The data will never include fan versions, such as those with specific instruments.</li>
<li>DISCOGS has information on song duration. Couldn't this information be used to filter songs contained in a playlist or videos with other elements besides the target music better, rather than cutting at 20 minutes duration?</li>
<li>The paper presenting Da-TACOS has a section devoted to analyzing the characteristics of a version ("What is a cover song" or something similar to this). This is a good idea that the paper introduced, as a way to explore the dataset without being limited to a straightforward exploratory data analysis. This article could include something along these lines to enrich its analysis, especially since it would be much richer than that conducted by the group that proposed Da-TACOS (since there are more tracks and more metadata). For instance, they could examine how much versions vary in genre.</li>
<li>The baseline model part seemed very disconnected from what the reader would expect to see. Instead of creating and evaluating a new architecture on different datasets, different architectures could be experimented on and applied to the proposed dataset. However, the section presented in the article resembles something a reader would expect in an article proposing a new architecture for the task as the main contribution.</li>
<li>Providing different feature sets is very relevant. However, the text does not clearly specify what these features will be (it only says "including" some), nor how they will be formatted and made available.</li>
</ul></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>The paper introduces a new dataset for version identification that features an unprecedented scale. Part of a dataset is provided with YouTube links, and audio features are available on request. The authors then briefly introduce a NN model to demonstrate a practical use of the dataset, and compare it to state-of-the-art as much as they can.</p>
<p>The paper is very well written and easy to follow.</p>
<p>Overall, the contribution of this paper is of major importance to the task of version identification. It has been showed numerous times in the literature that the high variability in the nature of music versions, the plurality of music genres and the range of clique sizes are key factors that influence the performance of VI models. Therefore, bringing a new, 1.8M versions with 330k cliques dataset is of major interest to the research community.</p>
<p>Moreover, the authors don't just release a massive metadata database, but explain in details the steps used to produce it. This is even more valuable for future research and datasets construction.</p>
<p>The baseline model is presented quite thoroughly. Authors do not elaborate much on the choice of modelling and on the set parameters, but this is understandable given the context of a dataset-focused paper. The results are presented in an extensive way, and authors honestly acknowledge the limitations of their interpretation, mainly due to the difficulty to cross compare the baseline databases.</p>
<p>I have very little complaints about the paper, that is quite clear as it is.
Here are a few minor remarks that could make it even better in my opinion:
- the end of section 2.1 lists a few version types that are present in the dataset: live versions, remixes, radio edits etc. I think authors should describe more these types, by providing an additional table or graph, as new types may be a valuable contribution, and will describe better the composition of the dataset.
- section 3 describes the process applied regarding YouTube content. It might be advisable for authors to remind here that audios are not disclosed as part of the database, and content derived from this copyrighted material, namely audio features, are only granted for non commercial, research purposes. I know it is stated elsewhere, but it might be worth mentioning it here.
- figure 3 is not easy to read, I suggest authors reorganize it. Maybe data can be represented for all 3 splits on a single, larger plot?
- many references are missing the conference name, references should then be carefully proofread and revamped</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This work introduces Discogs-VI-YT, a dataset for version identification (VI). The dataset is created by leveraging metadata from the Discogs music database and a search algorithm to programmatically identify a large corpus of music versions. The proposed dataset surpasses existing VI datasets in size and offers more comprehensive metadata. To demonstrate the efficacy of the proposed dataset, a baseline VI model is also trained.</p>
<p>The manuscript is well-written and easy to follow. The dataset creation process is explained in detail with reasonable design choices and well-recognized limitations. </p>
<p>The main weakness I found is that the discussion is limited on why the baseline trained with this large new dataset did not improve over other existing models when evaluated on the SHS100K-Test set (even for the CQTNet, which the proposed baseline model is based on). More discussion about potential causes can help readers gain more insights about the dataset, the task, or the existing models.</p>
<p>Other minor comments:
- Ln 187: duplicated Miles Davis
- Ln 454: missing “are”
- Ln 480: missing “use”
- Author names for reference [6] are missing
- It could be nice to place Tables and Figures at the top of the page. </p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We re-ran the data mining pipeline with the july dump, where the small amount of release youtube annotations are discarded. Using this new version of the dataset we re-ran the model training experiments a updated the corresponging table. We added a new table comparing the number of artists for selected datasets. All figures and tables are improved for visibility.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;166&#39;, &#39;session&#39;: &#39;3&#39;, &#39;position&#39;: &#39;16&#39;, &#39;forum&#39;: &#39;166&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1LBJWnWvXdGPgEicEIqG9NF6PkNRNM1cl/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Araz, Recep Oguz*&#39;, &#39; Serra, Xavier&#39;, &#39; Bogdanov, Dmitry&#39;], &#39;authors_and_affil&#39;: [&#39;Recep Oguz Araz (Universitat Pompeu Fabra)*&#39;, &#39; Xavier Serra (Universitat Pompeu Fabra )&#39;, &#39; Dmitry Bogdanov (Universitat Pompeu Fabra)&#39;], &#39;keywords&#39;: [&#39;MIR tasks&#39;, &#39;Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; fingerprinting; MIR tasks -&gt; pattern matching and detection; Musical features and properties -&gt; representations of music&#39;], &#39;abstract&#39;: &#39;Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.&#39;, &#39;TLDR&#39;: &#39;Current version identification (VI) datasets often lack suf- ficient size and musical diversity to train robust neural net- works (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped poten- tial of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions con- taining about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model com- plexities or data augmentations, which achieves competi- tive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the ex- tracted audio features, and a trained model, are all publicly available online.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/11b84MPrhbqy3NfjtwJRNsJnNKlPLExfe/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;3&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1ZLoWu-jji5KbPxEsQtWLJIxxYGASZ919/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1S6tj2j1ocTlGctPUCW7SIr6CuhdifpwX/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MSQ4M7&#39;, &#39;slack_channel&#39;: &#39;p3-15-discogs-vi-a&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;The paper presents a new dataset for the task of version identification that is significantly larger than existing datasets. The demand for datasets for this task is accurate due to the reasons presented (data volume), but mainly because of the difficulty in ensuring data quality (such as intra-click correctness) and the challenge in maintaining and disseminating the data (especially the audio associated with the tracks). Furthermore, obtaining the dataset from a source rich in metadata brought many advantages to the proposed set. Therefore, this paper is relevant.\n\nHowever, I list some points that the text could be better addressed:\n\n- While using &#34;official videos&#34; might help with the longevity of link availability, it limits the dataset to less varied versions. For example, the dataset is restricted to good recordings by professional musicians. The data will never include fan versions, such as those with specific instruments.\n- DISCOGS has information on song duration. Couldn\&#39;t this information be used to filter songs contained in a playlist or videos with other elements besides the target music better, rather than cutting at 20 minutes duration?\n- The paper presenting Da-TACOS has a section devoted to analyzing the characteristics of a version (&#34;What is a cover song&#34; or something similar to this). This is a good idea that the paper introduced, as a way to explore the dataset without being limited to a straightforward exploratory data analysis. This article could include something along these lines to enrich its analysis, especially since it would be much richer than that conducted by the group that proposed Da-TACOS (since there are more tracks and more metadata). For instance, they could examine how much versions vary in genre.\n- The baseline model part seemed very disconnected from what the reader would expect to see. Instead of creating and evaluating a new architecture on different datasets, different architectures could be experimented on and applied to the proposed dataset. However, the section presented in the article resembles something a reader would expect in an article proposing a new architecture for the task as the main contribution.\n- Providing different feature sets is very relevant. However, the text does not clearly specify what these features will be (it only says &#34;including&#34; some), nor how they will be formatted and made available.&#39;, &#39;review_2&#39;: &#34;The paper introduces a new dataset for version identification that features an unprecedented scale. Part of a dataset is provided with YouTube links, and audio features are available on request. The authors then briefly introduce a NN model to demonstrate a practical use of the dataset, and compare it to state-of-the-art as much as they can.\n\nThe paper is very well written and easy to follow.\n\nOverall, the contribution of this paper is of major importance to the task of version identification. It has been showed numerous times in the literature that the high variability in the nature of music versions, the plurality of music genres and the range of clique sizes are key factors that influence the performance of VI models. Therefore, bringing a new, 1.8M versions with 330k cliques dataset is of major interest to the research community.\n\nMoreover, the authors don&#39;t just release a massive metadata database, but explain in details the steps used to produce it. This is even more valuable for future research and datasets construction.\n\nThe baseline model is presented quite thoroughly. Authors do not elaborate much on the choice of modelling and on the set parameters, but this is understandable given the context of a dataset-focused paper. The results are presented in an extensive way, and authors honestly acknowledge the limitations of their interpretation, mainly due to the difficulty to cross compare the baseline databases.\n\nI have very little complaints about the paper, that is quite clear as it is.\nHere are a few minor remarks that could make it even better in my opinion:\n- the end of section 2.1 lists a few version types that are present in the dataset: live versions, remixes, radio edits etc. I think authors should describe more these types, by providing an additional table or graph, as new types may be a valuable contribution, and will describe better the composition of the dataset.\n- section 3 describes the process applied regarding YouTube content. It might be advisable for authors to remind here that audios are not disclosed as part of the database, and content derived from this copyrighted material, namely audio features, are only granted for non commercial, research purposes. I know it is stated elsewhere, but it might be worth mentioning it here.\n- figure 3 is not easy to read, I suggest authors reorganize it. Maybe data can be represented for all 3 splits on a single, larger plot?\n- many references are missing the conference name, references should then be carefully proofread and revamped&#34;, &#39;review_3&#39;: &#39;This work introduces Discogs-VI-YT, a dataset for version identification (VI). The dataset is created by leveraging metadata from the Discogs music database and a search algorithm to programmatically identify a large corpus of music versions. The proposed dataset surpasses existing VI datasets in size and offers more comprehensive metadata. To demonstrate the efficacy of the proposed dataset, a baseline VI model is also trained.\n\nThe manuscript is well-written and easy to follow. The dataset creation process is explained in detail with reasonable design choices and well-recognized limitations. \n\nThe main weakness I found is that the discussion is limited on why the baseline trained with this large new dataset did not improve over other existing models when evaluated on the SHS100K-Test set (even for the CQTNet, which the proposed baseline model is based on). More discussion about potential causes can help readers gain more insights about the dataset, the task, or the existing models.\n\nOther minor comments:\n- Ln 187: duplicated Miles Davis\n- Ln 454: missing “are”\n- Ln 480: missing “use”\n- Author names for reference [6] are missing\n- It could be nice to place Tables and Figures at the top of the page. &#39;, &#39;meta_review&#39;: &#39;This paper proposes a new dataset for the task of version identification. The dataset is based on the Discogs music database and is substantially larger than related datasets.\n\nThe reviewers all agree that the paper and the dataset are a valuable contribution to the community. The paper is well-written and the methodology is clearly described. The reviewers also mention a few minor issues that could further strengthen the paper. \n&#39;, &#39;author_changes&#39;: &#39;We re-ran the data mining pipeline with the july dump, where the small amount of release youtube annotations are discarded. Using this new version of the dataset we re-ran the model training experiments a updated the corresponging table. We added a new table comparing the number of artists for selected datasets. All figures and tables are improved for visibility.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>