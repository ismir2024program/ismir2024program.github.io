


<!DOCTYPE html>
<html lang="en">
<head>
    
    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024: Tutorials</title>
    
</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
        
<ul class="nav nav-pills justify-content-center">
  
  <li class="nav-item">
    <a
      id="calendar-html"
      class="nav-link text-muted"
      href="calendar.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Calendar
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="tutorials-html"
      class="nav-link text-muted"
      href="tutorials.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 10
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="day_2-html"
      class="nav-link text-muted"
      href="day_2.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 11
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="day_3-html"
      class="nav-link text-muted"
      href="day_3.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 12
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="day_4-html"
      class="nav-link text-muted"
      href="day_4.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 13
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="day_5-html"
      class="nav-link text-muted"
      href="day_5.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 14
    </a>
  </li>
  
  <li class="nav-item">
    <a
      id="day_6-html"
      class="nav-link text-muted"
      href="day_6.html"
      aria-controls="nav-home"
      aria-selected="true"
      >Nov 15
    </a>
  </li>
  
</ul>

    </div>
    <!-- Content -->
    <div class="content">
        
<div class="tab-content py-3 px-3 px-sm-0" id="nav-tabContent">
  <div id="day">
    <!-- Workshops -->
    <!-- <div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Tutorials</h2>
  </div>
</div> -->
    <div class="speakers">
      <div class="cards row">
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_1.html">
          <h3 class="card-title main-title">
            T1: Connecting Music Audio and Natural Language
          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim and Ke Chen

        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          Language serves as an efficient interface for communication between humans as well as between humans and machines. Through the integration of recent advancements in deep learning-based language models, the understanding, search, and creation of music is becoming capable of catering to user preferences with better diversity and control. This tutorial will start with an introduction to how machines understand natural language, alongside recent advancements in language models, and their application across various domains. We will then shift our focus to MIR tasks that incorporate these cutting-edge language models. The core of our discussion will be segmented into three pivotal themes: music understanding through audio annotation and beyond, text-to-music retrieval for music search, and text-to-music generation to craft novel sounds. In parallel, we aim to establish a solid foundation for the emergent field of music-language research, and encourage participation from new researchers by offering comprehensive access to 1) relevant datasets, 2) evaluation methods, and 3) coding best practices. 
        </div>
      </div>
    </div>
  </div>
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_2.html">
          <h3 class="card-title main-title">
            T2: Exploring 25 Years of Music Information Retrieval: Perspectives and Insights
          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Masataka Goto, Jin Ha Lee, and Meinard MÃ¼ller

        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          This tutorial reflects on the journey of Music Information Retrieval (MIR) over the last 25 years, offering insights from three distinct perspectives: research, community, and education. Drawing from the presenters' personal experiences and reflections, it provides a holistic view of MIR's evolution, covering historical milestones, community dynamics, and pedagogical insights. Through this approach, the tutorial aims to give attendees a nuanced understanding of MIRâs past, present, and future directions, fostering a deeper appreciation for the field and its interdisciplinary and educational aspects.

The tutorial is structured into three parts, each based on one of the aforementioned perspectives. The first part delves into the research journey of MIR. It covers the inception of query-by-humming and the emergence of MP3s, discusses the establishment of standard tasks such as beat tracking and genre classification, and highlights significant advancements, applications, and future challenges in the field. The second part explores the community aspect of ISMIR. It traces the growth of the society from a small symposium to a well-recognized international community, emphasizing core values such as interdisciplinary collaboration and diversity, and invites the audience to imagine the future of the ISMIR community together. Lastly, the third part discusses the role of music as an educational domain. It examines the broad implications of MIR research, the value of pursuing a PhD in MIR, and the significant educational resources available.

Each part invites audience interaction, aiming to provide attendees with a deeper appreciation of MIR's past achievements and insights into its potential future directions. This tutorial is not just a historical overview but also a platform for fostering a deeper understanding of the interplay between technology and music.

        </div>
      </div>
    </div>
  </div>
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_3.html">
          <h3 class="card-title main-title">
            From White Noise to Symphony: Diffusion Models for Music and Sound

          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Chieh-Hsin Lai, Koichi Saito, Bac Nguyen Cong, Yuki Mitsufuji, and Stefano Ermon

        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          This tutorial will cover the theory and practice of diffusion models for music and sound. We will explain the methodology, explore its history, and demonstrate music and sound-specific applications such as real-time generation and various other downstream tasks. By bridging the gap from computer vision techniques and models, we aim to spark further research interest and democratize access to diffusion models for the music and sound domains. 

The tutorial comprises four sections. The first provides an overview of deep generative models and delves into the fundamentals of diffusion models. The second section explores applications such as sound and music generation, as well as utilizing pre-trained models for music/sound editing and restoration. In the third section, a hands-on demonstration will focus on training diffusion models and applying pre-trained models for music/sound restoration. The final section outlines future research directions.

We anticipate that this tutorial, emphasizing both the foundational principles and practical implementation of diffusion models, will stimulate interest among the music and sound signal processing community. It aims to illuminate insights and applications concerning diffusion models, drawn from methodologies in computer vision.

        </div>
      </div>
    </div>
  </div>
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_4.html">
          <h3 class="card-title main-title">
            Humans at the Center of MIR: Human-subjects Research Best Practices
          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Claire Arthur, Nat Condit-Schultz, David R. W. Sears, John Ashley Burgoyne, and Josuha Albrecht

        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          In one form or another, most MIR research depends on the judgment of humans. Humans provide our ground-truth data, whether through explicit annotation or through observable behavior (e.g., listening histories); Humans also evaluate our results, whether in academic research reports or in the commercial marketplace. Will users like it? Will customers buy it? Does it sound good? These are all critical questions for MIR researchers which can only be answered by asking people. Unfortunately, measuring and interpreting the judgments and experiences of humans in a rigorous manner is difficult. Human responses can be fickle, changeable, and inconsistentâthey are, by definition, subjective. There are many factors that influence human responses, some of which can be controlled or accounted for in experimental design, and others which must be tolerated but ameliorated through statistical analysis. Fortunately, researchers in the field of behavioral psychology have amassed extensive expertise and institutional knowledge related to the practice and pedagogy of human-subject research, but MIR researchers receive little exposure to research methods involving human subjects. This tutorial, led by MIR researchers with training (and publications) in psychological research, aims to share these insights with the ISMIR community. The tutorial will introduce key concepts, terminology, and concerns in carrying out human-subject research, all in the context of MIR. Through the discussion of real and hypothetical human research, we will explore the nuances of experiment and survey design, stimuli creation, sampling, psychometric modeling, and statistical analysis. We will review common pitfalls and confounds in human research, and present guidelines for best practices in the field. We will also cover fundamental ethical and legal requirements of human research. Any and all ISMIR members are welcome and encouraged to attend: it is never too early, or too late, in oneâs research career to learn (or practice) these essential skills.

        </div>
      </div>
    </div>
  </div>
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_5.html">
          <h3 class="card-title main-title">
            Deep Learning 101 for Audio-based MIR
          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Geoffroy Peeters, Gabriel Meseguer Brocal, Alain Riou, and Stefan Lattner

        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          Audio-based MIR (MIR based on the processing of audio signals) covers a broad range of tasks, including analysis (pitch, chord, beats, tagging), similarity/cover identification, and processing/generation of samples or music fragments. A wide range of techniques can be employed for solving each of these tasks, spanning from conventional signal processing and machine learning algorithms to the whole zoo of deep learning techniques.

This tutorial aims to review the various elements of this deep learning zoo commonly applied in Audio-based MIR tasks. We review typical audio front-ends (such as waveform, Log-Mel-Spectrogram, HCQT, SincNet, LEAF, quantization using VQ-VAE, RVQ), as well as projections (including 1D-Conv, 2D-Conv, Dilated-Conv, TCN, WaveNet, RNN, Transformer, Conformer, U-Net, VAE), and examine the various training paradigms (such as supervised, self-supervised, metric-learning, adversarial, encoder-decoder, diffusion). Rather than providing an exhaustive list of all of these elements, we illustrate their use within a subset of (commonly studied) Audio-based MIR tasks such as multi-pitch/chord-estimation, cover-detection, auto-tagging, source separation, music-translation or music generation. This subset of Audio-based MIR tasks is designed to encompass a wide range of deep learning elements. For each tack we address a) the goal of the tasks, b) how it is evaluated, c) provide some popular datasets to train a system, and d) explain (using slides and pytorch code) how we can solve it using deep learning.

The objective is to provide a 101 lecture (introductory lecture) on deep learning techniques for Audio-based MIR. It does not aim at being exhaustive in terms of Audio-based MIR tasks nor on deep learning techniques but to provide an overview for newcomers to Audio-Based MIR on how to solve the most common tasks using deep learning. It will provide a portfolio of codes (Colab notebooks and Jupyter book) to help newcomers achieve the various Audio-based MIR Tasks.

        </div>
      </div>
    </div>
  </div>
  
  <div class="col-md-6 col-sm-12 p-3" style="box-sizing: border-box;">
    <div class="card" style="display: block; overflow: hidden; width: 100%;">
      <div
        class="card-header text-left"
        style="min-height: 200px; width: 100%;"
      >
        <a class="text-muted" href="tutorial_6.html">
          <h3 class="card-title main-title">
            Lyrics and Singing Voice Processing in Music Information Retrieval: Analysis, Alignment, Transcription and Applications
          </h3>
        </a>
        <div class="card-subtitle text-muted">
          Daniel Stoller, Emir Demirel, Kento Watanabe, and Brendan OâConnor
        </div>
        <div class="mt-3 mb-3 text-left card-subtitle font-italic" style="font-size: 0.95em">
          Singing, a universal human practice, intertwines with lyrics to form a core part of profound musical experiences, conveying emotions, narratives, and real-world connections. This tutorial explores the commonly used techniques and practices in lyrics and singing voice processing, which are vital in numerous music information retrieval tasks and applications.

Despite the importance of song lyrics in MIR and the industry, high-quality paired audio & transcript annotations are often scarce. In the first part of this tutorial, we'll delve into automatic lyrics transcription and alignment techniques, which significantly reduce the annotation cost and enable more performant solutions. Our tutorial provides insights into the current state-of-the-art methods for transcription and alignment, highlighting their capabilities and limitations while fostering further research into these systems.

Moreover, we present "lyrics information processing", which encompasses lyrics generation and leveraging lyrics to discern musically relevant aspects such as emotions, themes, and song structure. Understanding the rich information embedded in lyrics opens avenues for enhancing audio-based tasks by incorporating lyrics as supplementary input. 

Finally, we discuss singing voice conversion as one such task, which involves the conversion of acoustic features embedded in a vocal signal, often relating to timbre and pitch. We explore how lyric-based features can facilitate a model's inherent disentanglement between acoustic and linguistic content, which leads to more convincing conversions. This section closes with a brief discussion on the ethical concerns and responsibilities that should be considered in this area.

This tutorial caters especially to new researchers with an interest in lyrics and singing voice modeling, or those involved in improving lyrics alignment and transcription methodologies. It can also inspire researchers to leverage lyrics for improved performance on tasks like singing voice separation, music and singing voice generation, and cover song and emotion recognition.
        </div>
      </div>
    </div>
  </div>
  
</div>
    </div>
  </div>
</div>
<script>
  $('.nav-pills .nav-item .nav-link').each(function() {
console.log(window.location.pathname.split('/')[1].split('.')[0])
if (window.location.pathname.split('/')[1].replace('.', '-').includes($(this).attr('id')) ) {
  $(this).addClass('active');
}
})
</script> 

    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">Â© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>