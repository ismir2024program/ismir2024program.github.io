UID,Paper Title,Author Names,Abstract,reviewer,Correct paper?,Paper checklist ok?,Paper link,Video checklist ok?,Video link,Captions checklist ok?,Captions link,Poster checklist ok?,Poster link,Thumbnail checklist ok?,Thumbnail link,Optional Funfacts?,Funfacts link,All ok?,comments,channel_url,slack_channel
423,mshoxxDB - a Versioned Dataset for Electronic Music,Michael Taenzer (Fraunhofer IDMT / UPF)*,"Deep learning in Music Information Retrieval (MIR) relies on diverse datasets to cover various musical genres and instrumentation, ensuring robust model training and performance. The introduction of the mshoxxDB dataset aims to address the gap in low-resource data for electronic music, featuring 18 full-length songs and offering per-track MIDI files, multi-track files, mixture files, and extensive metadata for various MIR applications. This dataset spans multiple electronic music sub-genres and provides a resource for advancing research in tasks such as Automatic Music Transcription, Multi-Pitch Estimation, and Source Separation, fostering innovation in the field.",CN,1,1,https://drive.google.com/file/d/1qn5vZMrDevqve76l-M29qO0dl3ZULMor/view,1,https://drive.google.com/file/d/1H6N229-8QDOXCv9-aCw4AvZP-dlzktzA/view,1,https://drive.google.com/file/d/1QzviYy_DGWqDqqhJMcfmmhIHw39kCE-i/view,1,https://drive.google.com/file/d/1OdVtAaDdE45sEBsicWNEYSHT9_Y24w1m/view,1.0,https://drive.google.com/file/d/1gnrJrGScpLSwr5Poirx1Kf-xqNHSZS57/view,0,,1,,https://ismir2024.slack.com/archives/C0800QNARM0,lbd-423-mshoxxdb-a-versioned
424,MusicGen-Chord: Advancing Music Generation through Chord Progressions and Interactive Web-UI,Jongmin Jung (Sogang University)*; Andreas Jansson (Replicate); Dasaem Jeong (Sogang University),"MusicGen is a music generation language model (LM) that can be conditioned on textual descriptions and melodic features. We introduce MusicGen-Chord, which extends this capability by incorporating chord progression features. This model modifies one-hot encoded melody chroma vectors into multi-hot encoded chord chroma vectors, enabling the generation of music that reflects both chord progressions and textual descriptions. Furthermore, we developed MusicGen-Remixer, an application utilizing MusicGen-Chord to generate remixes of input music conditioned on textual descriptions. Both models are integrated into Replicate’s web-UI using cog, facilitating broad accessibility and user-friendly controllable interaction for creating and experiencing AI-generated music.",CN,1,1,https://drive.google.com/file/d/15WUU9cp9hgU8i3pDvVpqvGS5QVzOgxE5/view,1,https://drive.google.com/file/d/1XbSX99ikb7f2t5_ZTUuXqC7EZR0tTb-7/view,1,https://drive.google.com/file/d/1SQhbrdzGKLO4P0Q1PBXHNgsh3F1_MSsT/view,1,https://drive.google.com/file/d/1HM3YuOYhuiBOy_TUIm0TrBgVWm8jw2Pl/view,,https://drive.google.com/file/d/1Pe6aFXRzFYa2e5C-48c9-OPIm071BHKw/view,0,,1,,https://ismir2024.slack.com/archives/C07V86HKJA2,lbd-424-musicgen-chord-advancing
425,Optical Music Recognition for Jeongganbo Notation of Korean Court Music,DongMin Kim (Sogang University); Danbinaerin Han (KAIST); Dasaem Jeong (Sogang University)*; Jose J. Valero-Mas (University of Alicante),"The Jeongganbo notation, the first music notation system in East Asia capable of jointly expressing pitch and duration, has been extensively used in the Korean music tradition since its invention in the 15th century. 
 While the Optical Music Recognition (OMR) field has addressed the issue of automatically transcribing music sheets into digital formats in a number of music notations from the Western tradition, no previous research has considered Jeongganbo scores. 
 In this context, This work proposes a framework for the automatic recognition and transcription of Korean court music Jeongak (정악) in Jeongganbo notation utilizing synthetic data. 
 Our evaluations show performance rates close to 90% success, validating the proposed approach.",CN,1,1,https://drive.google.com/file/d/16RIc7-4zAKJtHXd27VZ13WP6yth1hCNn/view,1,https://drive.google.com/file/d/1ltMP6z1MIDCk-Kd4U5gL8yvIhUV01x7P/view,1,https://drive.google.com/file/d/1L2Mu1kAvv7sX_FXrtUj3R_cBfsgmA6ld/view,1,https://drive.google.com/file/d/1MqNrICoBJsbIdayRN5hRVO5PmeHdAqYL/view,1.0,https://drive.google.com/file/d/1zAoBb1zRsm2LatuXMCmIUKn9cGa6Oh_1/view,0,,1,"check generated captions, srt linking failed",https://ismir2024.slack.com/archives/C0800QNMXNE,lbd-425-optical-music-recognition
426,symusic: A swift and unified toolkit for symbolic music processing,"Yikai Liao (Beijing University of Posts and Telecommunications); Zhongqi Luo (Qiyin Technology Co., Ltd.)*; Yue Wang (China Conservatory of Music); Yujie Wu (殷渝杰);","This paper introduces symusic, a swift and unified toolkit for symbolic music processing. It provides high-level APIs for symbolic music manipulation, and an optimized IO engine which supports multiple file formats. Symusic addresses key challenges in symbolic music processing by offering exceptional speed, multi-format support, flexible time unit conversion, and comprehensive batch operations. With up to 1000 times faster parsing than existing libraries, symusic is poised to accelerate large-scale music analysis and generation. Code and pre-built package of symusic are available at Github and PyPI.",CN,1,1,https://drive.google.com/file/d/18xgV3eYD8CcxqqKpXWMRoMs8fo-r2mcW/view,1,https://drive.google.com/file/d/1IVbILS3hEESbeEedw0XiUMGYDJme85bW/view,1,https://drive.google.com/file/d/1EovSqHdKGEkxIh6tMYVs1cEX2qFeFlLt/view,1,https://drive.google.com/file/d/1jnQsgtl6Un9t0qwZem6Hf5pAhw9zNy_Z/view,1.0,https://drive.google.com/file/d/1iELDh8e1VFbQE_tBROq8LfwqK7zuuAE4/view,0,,1,,https://ismir2024.slack.com/archives/C07V86J1U4W,lbd-426-symusic-a-swift
427,Facing the Music: Tackling Singing Voice Separation in Cinematic Audio Source Separation,"Karn N Watcharasupat (Georgia Institute of Technology)*; Chih-Wei Wu (Netflix, Inc.); Iroro Orife (Netflix)","Cinematic audio source separation (CASS), as a standalone problem of extracting individual stems from their mixture, is a fairly new subtask of audio source separation. A typical setup of CASS is a three-stem problem, with the aim of separating the mixture into the dialogue (DX), music (MX), and effects (FX) stems. Given the creative nature of cinematic sound production, however, several edge cases exist; some sound sources do not fit neatly in any of these three stems, necessitating the use of additional auxiliary stems in production. One very common edge case is the singing voice in film audio, which may belong in either the DX or MX or neither, depending heavily on the cinematic context. In this work, we demonstrate a very straightforward extension of the dedicated-decoder Bandit and query-based single-decoder Banquet models to a four-stem problem, treating non-musical dialogue, instrumental music, singing voice, and effects as separate stems. Interestingly, the query-based Banquet model outperformed the dedicated-decoder Bandit model. We hypothesized that this is due to a better feature alignment at the bottleneck as enforced by the band-agnostic FiLM layer.",CN,1,1,https://drive.google.com/file/d/12CDsHhFfkc9ckL7Uq4Qv2JW9pld7Eqh9/view,1,https://drive.google.com/file/d/1ki1T4ihG9Shl8f9Ya-Kb4L3QRATtA_bs/view,1,https://drive.google.com/file/d/1JZ2eNaNUnlgq_l3YnEdLl3bXcayPzaQI/view,1,https://drive.google.com/file/d/1Y-p1V4gBxiCPZwdKnAsvnhSR6Y4VSqbW/view,1.0,https://drive.google.com/file/d/1Gww3RX35DWMO6YSSw3LxinhNS2QMnV9B/view,0,,1,,https://ismir2024.slack.com/archives/C07V86J8P7Y,lbd-427-facing-the-music
428,Groove Transfer VST for Latin American Rhythms,Anmol Mishra (Universitat Pompeu Fabra)*; Behzad Haki (Universitat Pompeu Fabra); Satyajeet Prabhu (Universitat Pompeu Fabra); Martín Rocamora (Universitat Pompeu Fabra),"Latin music has intricate rhythms with microtming and dy-
 namic variations. Modern deep learning based generative
 systems generate realistic sounding drum outputs. How-
 ever, most rhythm generation datasets focus have a focus
 on pop and rock music which leads to the generations be-
 ing biased towards those categories. In this work, we
 tend to specifically develop a system that can learn the
 groove in Brazilian Rhythms",CN,1,1,https://drive.google.com/file/d/1zP0OeCT9vkuGHi7maR1VfxKMtkTNY2oY/view,1,https://drive.google.com/file/d/1Epl7nPpqYfFe1MZiKvrGTF12zbC9NlqK/view,1,https://drive.google.com/file/d/1sVjoP5q1ctAJM_GOPWC4asY3ni1sDLm-/view,1,https://drive.google.com/file/d/1mLMuFrtFGootQueXxVRY8xpxoxCaLBXM/view,1.0,https://drive.google.com/file/d/1RoadvkgPW2IoLahl-XnrSwPxlbj70Z5I/view,0,,1,,https://ismir2024.slack.com/archives/C0800QPEY5Q,lbd-428-groove-transfer-vst
429,OPTIMIZING MUSIC CAPTIONING WITH REINFORCEMENT LEARNING AND RETRIEVAL-AUGMENTED METHODS,Haesun Joung (Seoul National University)*; Jinwoo Lee (Huawei Tech.); Kyogu Lee (Seoul National University),.,CN,1,1,https://drive.google.com/file/d/1ixP6C6n0h7rPab2XP99RvmkyF67UPbFp/view,1,https://drive.google.com/file/d/1K7cLr1Cc5HM3DIRI9AbhxZRnRL4AzcLH/view,1,https://drive.google.com/file/d/1k8liHReZEp2TazjekSGK8LlzQJxR8T2G/view,1,https://drive.google.com/file/d/1U8lidoFbUGUg-QGN1T7740YLN0tAmbmG/view,1.0,https://drive.google.com/file/d/1RnI1gVRomnHPQ54YWxmQucUMTqE6_D_3/view,0,,1,,https://ismir2024.slack.com/archives/C07V86JR8KY,lbd-429-optimizing-music-captioning
430,Conditional piano music generation by flow matching for performance style transfer,Ahyeon Choi (Seoul National University)*; Dohoon Lee (Seoul National University); Kyogu Lee (Seoul National University),"This study explores the generation of classical piano performances conditioned on individual performers' expressive styles. 
 The interpretative diversity in classical music naturally results in comparisons and preferences for different interpretations of the same piece, fostering a desire to explore performances in varied styles.
 However, most research in music style transfer has primarily focused on timbre and compositional changes, with limited success in transferring performance style due to the difficulty of separating style from music content.
 To address this, we propose a novel approach that utilizes easily obtainable audio data and employs a flow matching model to generate high-quality, style-conditioned audio performances.
 We created datasets from pianists Seong-Jin Cho and Lang Lang to demonstrate our method, successfully generating performances in their distinct styles. 
 This work expands the possibilities for performance style transfer and can potentially be applied to other performers' styles with additional audio data.",CW,1,1,https://drive.google.com/file/d/15hR_XNq47IFlH4ihot9cAXpDRIIGyQPz/view?usp=drive_link,1,https://drive.google.com/file/d/1jv0guf8Arbni5tonywdaF7nOae6ZYHsf/view?usp=drive_link,1,https://drive.google.com/file/d/1KkAQaAR5wNQuUxNWnsgKhnQCSTF7gNZn/view?usp=drive_link,1,https://drive.google.com/file/d/1UdTdLxhBDiHTwjd5eNFGJFhze8ORzQqZ/view?usp=drive_link,1.0,https://drive.google.com/file/d/1LUKFgN7Hna-tSPkCuN6CAMq1rTrdTYmD/view?usp=drive_link,0,,1,video is a bit small (720x332),https://ismir2024.slack.com/archives/C0803A923N0,lbd-430-conditional-piano-music
431,A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument,Kyungsu Kim (Seoul National University)*; Junghyun Koo (Sony AI); Sungho Lee (Seoul National University); Haesun Joung (Seoul National University); Kyogu Lee (Seoul National University),"TokenSynth is a neural synthesizer that uses neural audio codecs and transformers to generate single-instrument musical audio from MIDI information and CLAP embeddings. The model can perform instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without fine-tuning. This enables various creative sound design applications and intuitive timbre control. The timbral similarity to target audio/text, and synthesis accuracy were evaluated using objective measures.",CW,1,1,https://drive.google.com/file/d/1Cv7fa1PVb5pOJPwVbnixFYCWYuVYCQTM/view?usp=drive_link,1,https://drive.google.com/file/d/1UkRqiH-3DRoBIEqInGM77hJTpN-asMWD/view?usp=drive_link,1,https://drive.google.com/file/d/1MBvNvRu5x471Be1QiOycnLmyGlT_Pyus/view?usp=drive_link,1,https://drive.google.com/file/d/1RymfZkkYjJGgNN_T_QTw56c6ZDrMGD4M/view?usp=drive_link,1.0,https://drive.google.com/file/d/1-2yOJEWBCwqmn5bs8bErBStvCPdEtD7f/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V08WRV71,lbd-431-a-token-based
432,SOURCE-LEVEL PITCH AND TIMBRE EDITING FOR MIXTURES OF TONES USING DISENTANGLED REPRESENTATIONS,Yin-Jyun Luo (Queen Mary University of London)*; Kin Wai Cheuk (Sony AI); Woosung Choi (Sony AI); Toshimitsu Uesaka (Sony Group Corporation); Keisuke Toyama (Sony Group Corporation); Wei-Hsiang Liao (Sony Group Corporation); Simon Dixon (Queen Mary University of London); Yuki Mitsufuji (Sony AI),"We propose a model to learn latent representations of pitch and timbre of each individual source of instrument tones from a mixture of instruments. We employ variational autoencoders to train the model using a query-based inference network. Given a mixture, the model allows for precise source-level attribute editing, e.g., instrument or pitch replacement, by manipulating the pitch and timbre latents. On the synthetic audio clips of chords compiled using the JSB Chorales dataset, our quantitative evaluation protocol shows empirical success of the model on both pitch-timbre disentanglement of individual sources and source-level attribute manipulation of mixtures.",CW,1,1,https://drive.google.com/file/d/1yEb_vANrctDvvcxrEHZr1Vichy9ZExAe/view?usp=drive_link,https://drive.google.com/file/d/1yEb_vANrctDvvcxrEHZr1Vichy9ZExAe/view?usp=drive_link,https://drive.google.com/file/d/1UmGP_4m4bNYI0DakfnTUQQS1ovPS9rUW/view?usp=drive_link,1,https://drive.google.com/file/d/1iIVvILGtxCPlVA3tX_m3p_vkoPDXieue/view?usp=drive_link,1,https://drive.google.com/file/d/1NFbMAd-CNEpF2QWM_X7B-vIUUHgo79tD/view?usp=drive_link,1.0,https://drive.google.com/file/d/18k8jiWdqpKD2v-Yx-3ykmA0sVuYn0HJJ/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800N3BYDR,lbd-432-source-level-pitch
433,LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models,Haven Kim (University of California San Diego)*; Kahyun Choi (UIUC),"This paper addresses the unique challenge of conducting research in lyric studies, where direct use of lyrics is often restricted due to copyright concerns. Unlike typical data, internet-sourced lyrics are frequently protected under copyright law, necessitating alternative approaches. Our study introduces a novel method for generating copyright-free lyrics from publicly available Bag-of-Words (BoW) dataset, which contain the vocabulary of lyrics but not the lyrics themselves. Utilizing metadata associated with BoW datasets and large language models, we successfully reconstructed lyrics. We have compiled and made available a dataset of reconstructed lyrics, LyCon, aligned with metadata from renowned sources including the Million Song Dataset, Deezer Mood Detection Dataset, and AllMusic Genre Dataset, available for public access. We believe that the integration of metadata such as mood annotations or genres enables a variety of academic experiments on lyrics, such as conditional lyric generation.",CW,1,1,https://drive.google.com/file/d/1AhUEK3bAvTwwN8LGrBhmMnovMTqsuQKm/view?usp=drive_link,1,https://drive.google.com/file/d/1A4QGz7PHdte9265VIxjLug2tK0KUKaJ0/view?usp=drive_link,1,https://drive.google.com/file/d/1Z17tupKApww0JceO7kD7cU4DBtYTrR65/view?usp=drive_link,1,https://drive.google.com/file/d/1xg3QotW6kzcHgmxPsDZt9Fv0iqGmt1Ij/view?usp=drive_link,1.0,https://drive.google.com/file/d/1nfD-oQny2BuqpwhGuRhpqqhkfdBfpU0f/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800N3HSJF,lbd-433-lycon-lyrics-reconstruction
434,PyNeuralFx: A Python Package for Neural Audio Effect Modeling,Yen-Tung Yeh (National Taiwan University)*; Wen-Yi Hsiao (Indepedent Researcher); Yi-Hsuan Yang (National Taiwan University),"We present PyNeuralFx, an open-source Python toolkit designed for research on neural audio effect modeling. The toolkit provides an intuitive framework and offers a comprehensive suite of features, including standardized implementation of well-established model architectures, loss functions, and easy-to-use visualization tools. As such, it helps promote reproducibility for research on neural audio effect modeling, and enable in-depth performance comparison of different models, offering insight into the behavior and operational characteristics of models through DSP methodology. The toolkit can be found at https://github.com/ytsrt66589/pyneuralfx.",CW,1,1,https://drive.google.com/file/d/1nk38b32N19YwwOZkeY4IS8yCqRsXyh1k/view?usp=drive_link,1,https://drive.google.com/file/d/1SbEcGUZlZZKPvx_APQWbmM1Qy2ZMxwAU/view?usp=drive_link,1,https://drive.google.com/file/d/16O3jLLL3jRE4N3N0F7h-cT3tOl5O3qgB/view?usp=drive_link,1,https://drive.google.com/file/d/18pWA2sglF2zWv89DeQUgvxdHadJDyWrr/view?usp=drive_link,1.0,https://drive.google.com/file/d/1e23oosrtxAhFXq6n93-Ik-drlAKOi0_N/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0803A9NM0C,lbd-434-pyneuralfx-a-python
435,Matchmaker: A Python library for Real-time Music Alignment,Jiyun Park (KAIST)*; Carlos Eduardo Cancino-Chacón (Johannes Kepler University Linz); Taegyun Kwon (KAIST); Juhan Nam (KAIST),"Music alignment is a fundamental MIR task, and real-time music alignment is a necessary component of many interactive applications (e.g., automatic accompaniment systems, automatic page turning). This paper introduces Matchmaker, an open source Python library for real-time music alignment. Unlike offline alignment methods, for which state-of-the-art implementations are publicly available, real-time (online) methods have no standard implementation, forcing researchers and developers to build them from scratch for their projects. We aim to provide efficient reference implementations of score followers for use in real-time applications which can be easily integrated into existing projects. We also aim to provide guidelines to help researchers and developers select an appropriate configuration for their applications.",CW,1,1,https://drive.google.com/file/d/1ybVrakAxUVrWXU39_jqdAvJFjudSE-P9/view?usp=drive_link,1,https://drive.google.com/file/d/1HZeRrvwPUcCx1b6KnZJoaG3KsHRT20Yp/view?usp=drive_link,1,https://drive.google.com/file/d/10hxaBtHErsfgwt6sV2tHdMXj0P15LCdi/view?usp=drive_link,1,https://drive.google.com/file/d/1e8cXQohKwHKF4NK9Tzr2g791-r7V4VWu/view?usp=drive_link,1.0,https://drive.google.com/file/d/1fZhhhUuUVKOaMyWag7CWHJUNEuIVPO2J/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V86KU05C,lbd-435-matchmaker-a-python
437,Audio Atlas: Visualizing and Exploring Audio Datasets,Luca A Lanzendoerfer (ETH Zurich)*; Florian Grötschla (ETH Zürich); Uzeyir Valizada (ETH Zurich); Roger Wattenhofer (ETH Zurich),"We introduce Audio Atlas, an interactive web application for visualizing audio data using text-audio embeddings. Audio Atlas is designed to facilitate the exploration and analysis of audio datasets using a contrastive embedding model and a vector database for efficient data management and semantic search. The system maps audio embeddings into a two-dimensional space and leverages DeepScatter for dynamic visualization. Designed for extensibility, Audio Atlas allows easy integration of new datasets, enabling users to better understand their audio data and identify both patterns and outliers. We open-source the codebase of Audio Atlas, and provide an initial implementation containing various audio and music datasets.",CN,1,1,https://drive.google.com/file/d/18csA-5uZ9qekZdlBBsAEfTZmwEXJ5W3X/view,1,https://drive.google.com/file/d/1fH3s3TRPbFON3snuP_lvi60HGUB7gqF_/view,1,https://drive.google.com/file/d/1gBSAu5dROwMI3b_ZEFDkxb8WZjJuv-hh/view,1,https://drive.google.com/file/d/1wqZ2MXTwjgEaLMxnfLclLMIQM8Jc04tJ/view,1.0,https://drive.google.com/file/d/1Yehu7hoJGlnRm2a1_X7l8tHUno7057xP/view,0,,1,,https://ismir2024.slack.com/archives/C080DFG1CJV,lbd-437-audio-atlas-visualizing
438,A New Dataset for Tag- and Text-based Controllable Symbolic Music Generation,Weihan Xu (Duke University)*; Julian McAuley (UCSD); Taylor Berg-Kirkpatrick (UCSD); Shlomo Dubnov (UC San Diego); Hao-Wen Dong (University of Michigan),"Recent years have seen many audio-domain text-to-music generation models that rely on large amounts of text-audio pairs for training. However, symbolic-domain controllable music generation has lagged behind due to the lack of a large-scale symbolic music dataset with extensive metadata and captions. In this paper, we present MetaScore, a new dataset consisting of 963K musical scores paired with rich metadata collected from an online music forum and generated psudo captions. With MetaScore, we explore tag- and text-based controllable symbolic music generation. 
 Both subjective test and objective test showcase the potential of our dataset in tag- and text-conditioned music generation.",CN,1,1,https://drive.google.com/file/d/1uX4RGx1ZraqsDvKzA7lXxXWVupe0YRpf/view,1,https://drive.google.com/file/d/167qlX0Wf2Tpbn23PjEKUKuMiT4w6qoF2/view,1,https://drive.google.com/file/d/1_ERjsWKNjTPWPPgxWCo2WNhQVi7sVcO_/view,1,https://drive.google.com/file/d/1n6c5kroCTYf0_lBUI8qthxPcRQyL2buQ/view,1.0,https://drive.google.com/file/d/1PWdusVKhFK7A3oUXnQXTTyCs1zLucvvY/view,0,,1,,https://ismir2024.slack.com/archives/C07VBV60C6R,lbd-438-a-new-dataset
439,ENHANCED FORMULATION OF THE LATENT ORDER LOGISTIC REGRESSION (LOLOG) MODEL FOR ANALYSIS OF AUSTRALIAN MUSICIAN NETWORKS,"Lekshmy Hema Nair (Western Sydney University)*; Simon Chambers (Western Sydney University); Roger T. Dean (The MARCS Institute for Brain, Behaviour and Development, Western Sydney University)","Network analysis, especially through models like Exponential Random Graph Models (ERGMs), is crucial in understanding how musical styles, trends, and cultural influences diffuse through networks of musicians. However, ERGMs often face challenges like degeneracy, particularly when analyzing higher-order patterns within these networks. The Latent Order Logistic Regression (LOLOG) model provides an alternative by modeling connections as they form sequentially, which helps avoid degeneracy. To further enhance this model, we propose incorporating triadic independency, wherein we can capture more complex relationships in musical collaborations, offering deeper insights into how cultural information spreads and evolves within these networks. This enhanced model is being tested on networks of Australian musicians to assess its effectiveness in tracing collaboration patterns and enhancing music information retrieval, while also reflecting the cultural dynamics within these communities.",CW,1,1,https://drive.google.com/file/d/1RN5tPORDe88QNM7vRKc3YuCbI2Dpew9i/view?usp=drive_link,1,https://drive.google.com/file/d/1pD8q3R9R3qSgVYdiJ4pfo6L1j3NQB3dN/view?usp=drive_link,1,https://drive.google.com/file/d/1iL05e6eDIjzepsxMKd7yHLMGL0khieFQ/view?usp=drive_link,1,https://drive.google.com/file/d/1XqqvSfiiBUVxApshhjiK9qCgcWtXeDaG/view?usp=drive_link,1.0,https://drive.google.com/file/d/15qLUkAvFxaHaL8tQzzdSTCjrJtFJTJQM/view?usp=drive_link,0,,1,"burn-in sub, thumbnail too big",https://ismir2024.slack.com/archives/C07V86LEYQN,lbd-439-enhanced-formulation-of
440,Wavespace: A Highly Explorable Wavetable Generator,Hazounne Lee (Seoul National University)*; Kihong Kim (Kyungpook National University); Sungho Lee (Seoul National University); Felix You (The University of Texas at Austin); Kyogu Lee (Seoul National University),"Wavetable synthesis generates quasi-periodic waveforms of musical tones by interpolating a list of waveforms called wavetable. Studies in wavetable generation with invertible architecture have arisen recently. However, it is still challenging to generate wavetables with detailed controls within the latent representation. In response, we present Wavespace, a framework for wavetable generation that empowers users with enhanced parameter controls. Our model allows users to apply timbral conditions to the output wavetables. Users handle a latent space factorized by each timbral group called style. Then they adjust timbral details with descriptors. This way, users can create wavetables by manipulating timbral parameters step by step. Our framework is efficient enough for practical use; we prototyped an oscillator plug-in for real-time integration of Wavespace within digital audio workspaces (DAWs).",CW,1,1,https://drive.google.com/file/d/1gz4kTA0tOQ0-u1h_-QjYmupMD0xgnPF6/view?usp=drive_link,1,https://drive.google.com/file/d/1TyBAf6ijNY3maKtWrbQEyutt3pLq17lw/view?usp=drive_link,1,https://drive.google.com/file/d/13btDB38j8PBoxpqItJFkM_AdrmjuA_dw/view?usp=drive_link,1,https://drive.google.com/file/d/1ObP2xxkMtxxUN3a29u20-3g7gyzjRLTb/view?usp=drive_link,1.0,https://drive.google.com/file/d/1MM0Yjo3_CsxD8OmH6Ma20bhrAzvA5TXR/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0803AAM38C,lbd-440-wavespace-a-highly
441,A DBN-Based Regularization Approach for Training Postprocessing-free Joint Beat and Downbeat Estimator,Yiming Wu (AlphaTheta Corporation)*; Yuya Yamamoto (AlphaTheta Corporation); Shunya Ishikawa (The University of Electro-Communications),"In a general Deep Neural Network (DNN)-based beat and downbeat tracking pipeline, a post-processing stage is required to refine the beat/downbeat posteriors estimated by the DNN. 
 A widely used post-processing method is to infer the beat/downbeat sequence that maximizes the likelihood of a probabilistic model such as the Dynamic Bayesian Network (DBN). In this work, we aim to train a DNN that can directly estimate consistent beat/downbeat posteriors without the need for post-processing. We adopt regularization approach that minimizes the difference between the DNN and the DBN. We experimentally show that the DNN trained with the regularization loss can estimate beat and downbeat posteriors with higher temporal consistency, reducing the need for post-processing.",CW,1,1,https://drive.google.com/file/d/1lTDk7j7C0RPg2UCyl0ZtkxpG0PmSxBfi/view?usp=drive_link,1,https://drive.google.com/file/d/1bJj9FCfBjzPYWFVbIYKRtD5xdK_k5rQR/view?usp=drive_link,1,https://drive.google.com/file/d/1VcZyIke10vUWMI9KlULp1KMhY-4SlKIg/view?usp=drive_link,1,https://drive.google.com/file/d/15-YNSyWxk2m3qiZxWV91wELdLZZRlEO8/view?usp=drive_link,1.0,https://drive.google.com/file/d/1ZxEPUKHnp4WSh43M3cSGHDHtMAOMlW_Y/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V86LS82J,lbd-441-a-dbn-based
442,Real-time Future Rhythm Visualizer,Masatoshi Hamanaka (RIKEN)*,"In breaking, the timing of the technique and rhythm needs to be matched, but the audience has trouble judging whether or not the timing is correct. Therefore, We developed a system that shows the future rhythm of the sound source the DJ is playing.",CW,1,1,https://drive.google.com/file/d/1tLiZAiwiDltzf79FQn773YXBpcdM_jbk/view?usp=drive_link,1,https://drive.google.com/file/d/1m7kc1GPbT1OxaFqME44NCXXk7q5TEAbp/view?usp=drive_link,0,,1,https://drive.google.com/file/d/1OV1KyZhsuCnAQc5-fHr337oFX-6by1yq/view?usp=drive_link,1.0,https://drive.google.com/file/d/19p4CzacxDc3TTqySqba9jlamRmb4HVfa/view?usp=drive_link,0,,1,"srt is missing, but they don't really need it honestly",https://ismir2024.slack.com/archives/C07V08YMWUX,lbd-442-real-time-future
443,Piano Concerto Accompaniment Creation,Yigitcan Özer (International Audio Laboratories Erlangen); Simon J Schwär (International Audio Laboratories Erlangen)*; Meinard Müller (International Audio Laboratories Erlangen),"Creating orchestral accompaniments for solo piano performances of concertos is challenging, especially when aiming for a system that adapts to the pianist's tempo and dynamics. In this work, we present a semi-automatic offline approach using source separation, alignment, and time-scale modification techniques to generate orchestral accompaniments from public-domain recordings. Our approach separates the orchestral part, aligns it with a new solo-piano performance played by the user, and adjusts the tempo and dynamics to match the user's interpretation. While some processes are automated, significant manual adjustments are needed for temporal alignment and sound production. Our results demonstrate the feasibility of this method while highlighting the need for further automation. We have made the multi-track recordings of the created mixes publicly available for research purposes at https://www.audiolabs-erlangen.de/resources/MIR/PCD_AudioLabs.",CN,1,1,https://drive.google.com/file/d/1ZwojdQFFjnu88jKaXpumWKYUjvqXgNY2/view?usp=drive_link,1,https://drive.google.com/file/d/153jkPslX3wxwgCdW0vBIYxD-h8lB28gH/view?usp=drive_link,1,https://drive.google.com/file/d/14-Ry7k2IXDEu3Z-ghyiofb_fnNhPLi8X/view?usp=drive_link,1,https://drive.google.com/file/d/1GJe2-FbFHhryNETxKqR4xLOsMZDRhLgl/view?usp=drive_link,1.0,https://drive.google.com/file/d/1XdFDmkL9miTqnR0QvJIzxqQNEg24iTBu/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800QS37TL,lbd-443-piano-concerto-accompaniment
445,VizMuc - Vizualization of Music Corpora,Filip Trplan (University of Ljubljana); Klara Žnideršič (University of Ljubljana); Vid Klopčič (University of Ljubljana); Matevž Pesek (University of Ljubljana)*; Leon Stefanija (University of Ljubljana); Matija Marolt (University of Ljubljana),"To meet the challenge of analyzing and visualizing large music corpora, we have developed a web platform that simplifies the path from raw data processing to data visualization, search and comparison. Our platform can process both symbolic (MusicXML) and audio data, allowing users to easily access and compare different corpora. It provides a web-based interface for intuitive browsing and filtering as well as comparison of selected features of music pieces. It simplifies the exploration and comparison of growing digital music collections and speeds up the analysis process for users with different levels of knowledge.",CW,1,1,https://drive.google.com/file/d/1dpq5bAHwOyPMxHzvZkCh7rk9bdllVvBz/view?usp=drive_link,1,https://drive.google.com/file/d/1YiB63tqyE8m1KEXL_3obm7dHkmEfuMJ5/view?usp=drive_link,1,https://drive.google.com/file/d/1MrDs385cYAZYAQ3QeeBXQhltdrQ0_iN5/view?usp=drive_link,1,https://drive.google.com/file/d/1GcFIrSwWgAUso0W3O_VPXZUCLa8Whevb/view?usp=drive_link,1.0,https://drive.google.com/file/d/1eleBRTT6MGKQxlSi1LSVTYllo39DUp9g/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800QS6E7L,lbd-445-vizmuc-vizualization-of
446,ITO-Master: Inference-Time Optimization for Music Mastering Style Transfer,Junghyun Koo (Sony AI)*; Marco A Martinez Ramirez (Sony AI); Wei-Hsiang Liao (Sony Group Corporation); Giorgio Fabbro (Sony); Michele Mancusi (Sony Europe); Yuki Mitsufuji (Sony AI),"Music mastering style transfer involves applying the mastering-related audio features of a reference track to another, simulating the professional mastering process that enhances overall sound quality. In this paper, we propose the ITO-Master framework, which introduces Inference Time Optimization (ITO) on reference embeddings to improve the mastering style transfer process. Our approach achieves effective automatic mastering and gives users flexibility, enabling them to adapt the system to their preferences by adjusting the reference song or specific audio effects traits. We explore both black-box and differentiable methods, demonstrating that ITO improves performance on key metrics. The framework provides flexible, user-driven mastering style transfer with an interactive demo available on our demo page: https://tinyurl.com/ITO-Master.",CW,1,1,https://drive.google.com/file/d/1RUTud2s8-B_GYP7eIDrT4xIhJNGMnq7t/view?usp=drive_link,1,https://drive.google.com/file/d/1V69wnQv26BbuR55EOPaOAKoe-d_Fie4v/view?usp=drive_link,0,,1,https://drive.google.com/file/d/1gQFaZs6iMOwbGkBJO2qGeawawMpcYhGf/view?usp=drive_link,1.0,https://drive.google.com/file/d/1Qy66l1-QjzMdRcrqKF7dm1dCLHjNdeeO/view?usp=drive_link,0,,1,no speech. Srt is not needed; thumbnail is a bit small...,https://ismir2024.slack.com/archives/C0800QS9T0S,lbd-446-ito-master-inference
447,Demo of Zero-Shot Guitar Amplifier Modelling: Enhancing Modeling with Hyper Neural Networks,Yu-Hua Chen (NTU)*; Yuan-Chiao Cheng (Positive Grid); Yen-Tung Yeh (National Taiwan University); Jui-Te Wu (Positive Grid); Yu-Hsiang Ho (Positive Grid ); Jyh-Shing Roger Jang (National Taiwan University); Yi-Hsuan Yang (National Taiwan University),"Electric guitar tone modeling typically focuses on the non-linear transformation from clean to amplifier-rendered audio. Traditional methods rely on one-to-one mappings, incorporating device parameters into neural models to replicate specific amplifiers. However, these methods are limited by the need for specific training data. 
In this paper, we adapt a model based on the previous work, which leverages a tone embedding encoder and a feature wise linear modulation (FiLM) condition method. In this work, we altered conditioning method using a hypernetwork-based gated convolutional network (GCN) to generate audio that blends clean input with the tone characteristics of reference audio.
By extending the training data to cover a wider variety of amplifier tones, our model is able to capture a broader range of tones. Additionally, we developed a real-time plugin to demonstrate the system's practical application, allowing users to experience its performance interactively. Our results indicate that the proposed system achieves superior tone modeling versatility compared to traditional methods.",CW,1,1,https://drive.google.com/file/d/1UoOT1ZqZOt2FA0OxUCvvepMDxJlYaT-o/view?usp=drive_link,1,https://drive.google.com/file/d/1C5sQBWdUo5jVKVexYBrQI6kQ7JhyXzCV/view?usp=drive_link,1,https://drive.google.com/file/d/1iM7a3zSUXUu3aNzUM5xHNAKzJ44zYT7B/view?usp=drive_link,1,https://drive.google.com/file/d/1CSrn7gzQziI5JQnY-NQmleklFtJ4qOcY/view?usp=drive_link,1.0,https://drive.google.com/file/d/1cb2SamtoSsvWnwFry7cum2l2Fr3f5GGa/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C080DFHF56D,lbd-447-demo-of-zero
448,HI-AUDIO ONLINE PLATFORM: OPPORTUNITIES AND CHALLENGES OF COLLECTING VARIED MUSIC DATA ON THE WEB,"Jose Gil Panal (LTCI, Telecom Paris, Institut polytechnique de Paris); Aurelien David (LTCI, Telecom Paris, Institut polytechnique de Paris); Gaël Richard (LTCI, Telecom Paris, Institut polytechnique de Paris)*","We present in this paper the extended online HI-AUDIO platform which relies on a distributed and iterative music recording paradigm to asynchronously record musicians localised at different remote individual sites. The major goal of this platform is to become a key enabling tool for building a large, varied, multi-genre, multi-track, multi-instrument music dataset, to be ultimately publicly distributed for MIR research purposes. We describe in this paper the main characteristics of the web platform and discuss some of the major challenges for collecting music data on the web. The platform will be demonstrated on site with local and distant access and illustrate its merits for recording collaborative compositions.",CW,1,1,https://drive.google.com/file/d/1idha7KsakrTBNz5ujLjZtgri_L7UdZub/view?usp=drive_link,1,https://drive.google.com/file/d/186178KvHVwWoN8hNOgw-xqanh4qNpwYu/view?usp=drive_link,1,https://drive.google.com/file/d/1wrlA0yf7nTH6uXsVyyKmSjSiVZwuV1Pb/view?usp=drive_link,1,https://drive.google.com/file/d/1wfrsiGCc2GL8YhnV647Yysr-bdBCDEAM/view?usp=drive_link,1.0,https://drive.google.com/file/d/1OwTyUway8HfYaUmkkMA7qVKL4ZYp0E5r/view?usp=drive_link,0,,1,audio clipped... well...,https://ismir2024.slack.com/archives/C0803ABS8MA,lbd-448-hi-audio-online
449,Multimodal Structured Extraction for Self-Querying Music Video Retrieval and Playlist Generation,Kevin Dela Rosa (Aviary Labs)*,"In this study we introduce early results for a music video structured extraction framework designed to extract key metadata and descriptions such as genre, mood, video style, and summaries of general music, lyrical and visual narrative content. Leveraging video language models (VLM) and zero-shot prompting techniques, the system supports three key applications: entity discovery and browsing, multimodal self-querying retrieval, and playlist generation. The multimodal self-querying retrieval setup intelligently combines structured metadata filtering (e.g., video style, musical genre, emotion, visual elements) with lexical and semantic search, allowing users to query music videos using multiple facets. Additionally, the structured extraction powers entity discovery, enabling exploration of videos based on extracted metadata across the dataset. We provide qualitative examples of structured information extraction over an initial dataset of over 60K music videos to showcase the potential for search and video playlist generation.",CW,1,1,https://drive.google.com/file/d/194QEtNm3ofqlMoCuUi04J0-hIyLoplCF/view?usp=drive_link,1,https://drive.google.com/file/d/1MLZDLLKbdnyYq_tIeThbkvl9qpol_xsy/view?usp=drive_link,1,https://drive.google.com/file/d/1YVxaAxRok3MSy-VGQcNsvDqGN0K29G-J/view?usp=drive_link,1,https://drive.google.com/file/d/1mzQGUX5HIiltCT2Rd_vzIcmap-u9eYzj/view?usp=drive_link,1.0,https://drive.google.com/file/d/1Zc1zvCGcUC5z-Nx5lr37L3FAKQT8tuzp/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800QT0JF4,lbd-449-multimodal-structured-extraction
450,PYAMPACT: A SCORE-AUDIO ALIGNMENT TOOLKIT FOR PERFORMANCE DATA ESTIMATION AND MULTI-MODAL PROCESSING,Johanna Devaney (Brooklyn College)*; Daniel McKemie (Brooklyn College); Alexander Morgan (Independent),"pyAMPACT (Python-based Automatic Music Performance
Analysis and Comparison Toolkit) links symbolic and audio music representations to facilitate score-informed estimation of performance data in audio as well as general linking of symbolic and audio music representations with a variety of annotations. pyAMPACT can read a range of symbolic formats and can output note-linked audio descriptors/performance data into MEI and Humdrum kern files. The audio analysis uses score alignment to calculate time-frequency regions of importance for each note in the symbolic representation from which to estimate a range of
parameters. These include tuning-, dynamics-, and timbre-
related performance descriptors, while timing-related information is available from the score alignment. Beyond performance data estimation, pyAMPACT also facilitates multi-modal investigations through its robust infrastructure for linking symbolic representations and annotations to audio.",CN,1,1,https://drive.google.com/file/d/10pFTy9Pu61Gab3xg-1FqRD4oma53hlQ-/view?usp=drive_link,1,https://drive.google.com/file/d/1IBY3-Lzoe066pSEd37zglDqYHlIaEx3G/view,1,https://drive.google.com/file/d/11Vun91YKjgrJVEIIuWxf3acFwV_ZdP3Q/view,1,https://drive.google.com/file/d/1iWAlzUEGFGU-knGo-lgnHQY6JfFYOS07/view,1.0,https://drive.google.com/file/d/1_vAoC8awQvaFfrzAzGdwxOHrNsjDg8nw/view,0,,1,,https://ismir2024.slack.com/archives/C080PJVCK2L,lbd-450-pyampact-a-score
452,Chord Naming for Analysis,Mayank Sanganeria (Independent); Christopher G Leeper (Sharp15 Studios)*,"ChordNamer is a web app tool that takes in a MusicXML file, and uses Music21 to generate chord symbols for it.  More specifically, it implements logic to utilize the chordify() and chordSymbolFigureFromChord() to provide one chord symbol per measure, like in a jazz or pop lead sheet. In our demonstration, we apply the tool to songs from soundtracks for the games ""I Am Setsuna"" and ""The Legend of Zelda: Ocarina of Time""
",CN,1,1,https://drive.google.com/file/d/1WFv8_L8h1W6YQSbKieQ5l_GCxDo-2ACG/view?usp=drive_link,1,https://drive.google.com/file/d/140_eNWD_E-NBFKsaDjGavf6MpUBPPsOx/view,1,https://drive.google.com/file/d/1Wrhdb8fZwSmP8jihDAiAaAQm7ETeRax3/view,1,https://drive.google.com/file/d/1mYkIMR-y3qLIna69ZLJ4FnpsPJotVUPW/view,1.0,https://drive.google.com/file/d/1fa4FHFjvzFcU1Oywa-xHJCcOioqh7HJa/view,0,,1,,https://ismir2024.slack.com/archives/C07V0906279,lbd-452-chord-naming-for
453,Symbotunes: unified hub for symbolic music generative models,Paweł Skierś (Warsaw University of Technology); Maksymilian Łazarski (Warsaw University of Technology); Michał Kopeć (Warsaw University of Technology); Mateusz Modrzejewski (Warsaw University of Technology)*,"Implementations of popular symbolic music generative models often differ significantly in terms of the libraries utilized and overall project structure. Therefore, directly comparing the methods or becoming acquainted with them may present challenges. To mitigate this issue we introduce Symbotunes, an open-source unified hub for symbolic music generative models. Symbotunes contains several modern Python implementations of well-known methods for symbolic music generation, as well as a unified pipeline for generating and training.",CW,1,1,https://drive.google.com/file/d/1MAavFNWW3oYlounnmPrEgy56TFl-koCj/view?usp=drive_link,1,https://drive.google.com/file/d/1YQOYEFMpmjT3LPEVNpiFgO1w7Qg6gENo/view?usp=drive_link,1,https://drive.google.com/file/d/1_duBMD9VGU8IsvFmCSDBnut5lSjeK7nh/view?usp=drive_link,1,https://drive.google.com/file/d/1ZpXf7NOY0pdc0PDUqtf3Ws4QboWM0ZA0/view?usp=drive_link,1.0,https://drive.google.com/file/d/1vF9-95v3phPWjv47NMInCmbp_hTopNi7/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800N6K1PV,lbd-453-symbotunes-unified-hub
454,Demonstrating OpenMU-LightBench: A benchmark suite for music understanding,Mengjie Zhao (Sony Group Corporation)*; Zhi Zhong (Sony Group Corporation); Zhuoyuan Mao (Sony Group Corporation); Shiqi Yang (Sony); Wei-Hsiang Liao (Sony Group Corporation); Shusuke Takahashi (Sony Group Corporation); Hiromi Wakaki (Sony Group Corporation); Yuki Mitsufuji (Sony AI),"We present OpenMU-LightBench, a large-scale benchmark for training and evaluating music understanding models based on large language models
(LLMs). OpenMU-LightBench consists of approximately one million data examples of two music understanding subtasks: music captioning and music reasoning. We provide details on the construction process of
OpenMU-LightBench, including metadata collection and conversion. Next, we showcase data generated by prompting GPT-3.5. We release OpenMU-LightBench, and hope that its rich annotations can facilitate future research and development of building music understanding models based on LLMs.",CW,1,1,https://drive.google.com/file/d/1SFiX0ASMndnWmQktNtK9Fhf2Yj6j7aUg/view?usp=drive_link,1,https://drive.google.com/file/d/1oigG7u-34Zqd71Um06jqvD9OATnaInAX/view?usp=drive_link,1,https://drive.google.com/file/d/1BAEMy7lg9gCE97WGCRhPPbgBdNII9pFO/view?usp=drive_link,1,https://drive.google.com/file/d/1K50eaXsyh8PN4Jjc2Oe_CVYLgW_cfsB0/view?usp=drive_link,1.0,https://drive.google.com/file/d/1rq31noKsRogSIbfku2W8ggcK__K-Tgcz/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0803ACSLUU,lbd-454-demonstrating-openmu-lightbench
455,MidiTok Visualizer: a tool for visualization and analysis of tokenized MIDI symbolic music,Michał Wiszenko (Warsaw University of Technology); Kacper Stefański (Warsaw University of Technology); Piotr Malesa (Warsaw University of Technology); Łukasz Pokorzyński (Warsaw University of Technology);  Mateusz Modrzejewski (Warsaw University of Technology)*,"Symbolic music research plays a crucial role in music-related machine learning, but MIDI data can be complex for those without musical expertise. To address this issue, we present MidiTok Visualizer, a web application designed to facilitate the exploration and visualization of various MIDI tokenization methods from the \texttt{MidiTok} Python package. MidiTok Visualizer offers numerous customizable parameters, enabling users to upload MIDI files to visualize tokenized data alongside an interactive piano roll.",CW,1,1,https://drive.google.com/file/d/1a8H-voeuctFeffHki1f8KqZ8ZWIrwRSg/view?usp=drive_link,1,https://drive.google.com/file/d/1K_flJLTMyx8_npWijzPzh63Pwr1pisLi/view?usp=drive_link,1,https://drive.google.com/file/d/1e__RzEJES4VL4fbqpbjcTY0DxrRw-kXd/view?usp=drive_link,1,https://drive.google.com/file/d/1E5FzExQ8sZoKkKsHbcPXju33w806oO3b/view?usp=drive_link,1.0,https://drive.google.com/file/d/1H3YIR3uhZ1IeRoVygeq6_73dgh6zFku-/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0803AD0JRJ,lbd-455-miditok-visualizer-a
456,REVAMP: VISUALISATION AND ANALYSIS IN THE DIGITAL AUDIO WORKSTATION,Chris Cannam (QMUL); George Fazekas (QMUL)* ,"We adapt the Sonic Visualiser music audio analysis software to create a set of analysis and visualisation plugins that can be used in a digital audio workstation (DAW). These proof-of-concept plugins, written to the industry standard Audio Random Access (ARA) plugin format, explore a number of dedicated purposes: pitch analysis, comparative spectral viewing, precise frequency inspection, chord extraction, and track segmentation.
Here we describe our work so far, report on our findings, and consider the future work necessary to produce plugins that are truly useful in a DAW context. In our demo, we will show and discuss examples of our proof-of-concept pitch visualisation and chord plugins used interactively within sessions in some popular DAWs.",CW,1,1,https://drive.google.com/file/d/1D0uujMpAL5onFvmnlJ583cSeGyzfy34f/view?usp=drive_link,1,https://drive.google.com/file/d/1ZZoNlo5U2die_uN1JQJ9iU81Y3zSm58Y/view?usp=drive_link,1,https://drive.google.com/file/d/1jAmA8u3eAzmjsVl5eaeTRh8mNSZcaExN/view?usp=drive_link,1,https://drive.google.com/file/d/1xQmTFQ76dArMKv_Y__azkU9IJ_NM3ZsI/view?usp=drive_link,1.0,https://drive.google.com/file/d/1KK4BjCVsPiIKFBiQfGDDirUpLeDyWSC9/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V86P5WES,lbd-456-revamp-visualisation-and
457,The Surprising Effect of Song-Level Demixing for Music Foundation Model Pretraining,Junyan Jiang (New York University Shanghai)*; Akira Maezawa (Yamaha Corporation); Gus Xia (New York University Shanghai),"Music foundation models have been playing a more and more important role in many downstream tasks for music understanding. Previous foundation models typically adopt an auto-regressive language modeling or masked language modeling as a training objective, which yields limited performance on some downstream tasks like source separation. In this extended abstract, we propose a new training target via self-supervised demixing of two randomly mixed music pieces. We show that with this only training target, the model learns strong general-propose representation, and also shows good performance on source separation tasks when Parameter-Efficient Fine-Tuning (PEFT) is applied.",CW,1,1,https://drive.google.com/file/d/1ssdDI9BLF-G_khxk6SLRALIUbdDC7sYc/view?usp=drive_link,1,https://drive.google.com/file/d/1a6OXzhavNHiFLCsYORQkJdOCfhdC-om3/view?usp=drive_link,1,https://drive.google.com/file/d/1HDRbUzDSOQUT1qD_KL7pxEmjO2BC3KWQ/view?usp=drive_link,1,https://drive.google.com/file/d/1xE39FloGCgytpNn6U4z5voAeVB5UN_2N/view?usp=drive_link,1.0,https://drive.google.com/file/d/18UUN9edj4LTPTmwOqwwfuUwww8mGe6P5/view?usp=drive_link,0,,1,The license is incorrect; email sent; fixed,https://ismir2024.slack.com/archives/C07VBV8U64D,lbd-457-the-surprising-effect
458,Enhancement of Speech and Language Models through unsupervised Learning with Music Datasets,Eviatar Bas (Independent)*; Iran R Roman (Queen Mary University of London),"Music processing is known to emerge spontaneously in early development, yet the impact of music exposure on brain development and cognitive abilities, such as language acquisition, remains unclear. In this study, we investigated this effect on artificial neural networks by training identical autoencoders to recreate speech excerpts using datasets with varying proportions of music. We then assessed the models' performance by using transfer learning to apply the encoder to a language classification task. Our findings indicate that incorporating a small share of music into the pre-training data improved the ability of the model trained on English data to classify other languages, including Japanese and Korean, suggesting a potential benefit of music exposure in enhancing model generalization to linguistically distant languages.",CW,1,1,https://drive.google.com/file/d/1V7VcCkFGxlRtQcZRTYed2_xK45Kou5E8/view?usp=drive_link,1,https://drive.google.com/file/d/10ppi18SZsalJmssLfgMW1IHDqomRfvUc/view?usp=drive_link,1,https://drive.google.com/file/d/152zOHZdGH96G7K69tyu5jIo1IbjSSHCp/view?usp=drive_link,1,https://drive.google.com/file/d/1Q2JdmcYyLOGehvGuJCTCocfheOlqlX1h/view?usp=drive_link,1.0,https://drive.google.com/file/d/1jsgEy0U8iMw1ynPkFr4qZrJfVP4WNNQB/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VBV94VHT,lbd-458-enhancement-of-speech
460,Understanding Human Perception of Music Plagiarism Through a Computational Approach,"Daeun Hwang (University of California, Santa Cruz)*; Hyeonbin Hwang (KAIST)",to be updated,CW,1,1,https://drive.google.com/file/d/1YNcmWeqOsBn672sGYWJxshdbZ9zgn2zw/view?usp=drive_link,1,https://drive.google.com/file/d/1XTtbQIFqMPKeLM4LtjW_3OZ1qFouNb7X/view?usp=drive_link,1,https://drive.google.com/file/d/18jwjT3hg5VHNDaltrwDi6x-WkyZSBaEE/view?usp=drive_link,1,https://drive.google.com/file/d/11c9i4q2_xvf9lyo6eSQIok_Ks_Ekdhei/view?usp=drive_link,1.0,https://drive.google.com/file/d/1pZEC2ioMytna-wW1tGWOpm-dzWTKIFyC/view?usp=drive_link,1,https://docs.google.com/document/d/1NMT95Nz1BYoFurQ3zx7Hv_EyiRyjVpGO/edit?usp=drive_link&ouid=108424826030549393269&rtpof=true&sd=true,1,,https://ismir2024.slack.com/archives/C07V86PMVLN,lbd-460-understanding-human-perception
461,A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games,"Daeun Hwang (University of California, Santa Cruz)*; Xuyuan Cai (University of California, Santa Cruz); Edward Melcer (University of California, Santa Cruz); Elin Carstensdottir (University of California, Santa Cruz)","In the field of ludomusicology that overlaps the interests of sound and games, video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the perceived genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several perceived game genres. Therefore, we built an interactive system that extracts musical features from VGM on different perceived genres of video games, and then hypothesized how different musical features are correlated to the perceptions of each genre. This observed correlation may be used to further suggest such features are relevant to the expected elements associated with the sub-genre.",CW,1,1,https://drive.google.com/file/d/15vbIagVRIATHY8igKpiVrNFiKuoIe2x_/view?usp=drive_link,1,https://drive.google.com/file/d/1zRP5eo1kVD__uw4GEzRKwf_z_MqqpQvQ/view?usp=drive_link,1,https://drive.google.com/file/d/1JSXapMEZpi4gO_pJSHUdcIjDZzIzDIj2/view?usp=drive_link,1,https://drive.google.com/file/d/1ARKkua-MEyryG51o7jTKZ1V82jhe2P1J/view?usp=drive_link,1.0,https://drive.google.com/file/d/1RCXCE05qsu7a20L13GoN8GeqnutjAyRV/view?usp=drive_link,1,https://docs.google.com/document/d/1-hBPQ37UutItPumU5LiM6s8nSExnHVTv/edit?usp=drive_link&ouid=108424826030549393269&rtpof=true&sd=true,1,,https://ismir2024.slack.com/archives/C07V091EM5M,lbd-461-a-music-information
462,Analysis of the Originality of Gen-AI Song Audio,Rajesh Fotedar (University of Miami); Tom Collins (University of Miami)*,"We are investigating whether an AI-based generative music (Gen-AI) system returns original song audio in response to the same text prompts.  To determine a similarity threshold for our analysis, we compare audio from human-composed song pairs that were previously involved in actual copyright disputes. Participants are tasked with sending the same 8 text prompts, specifically designed for our investigation, to the Gen-AI system.  By comparing all returned Gen-AI songs to each other, a maximum 8-beat correlation value is determined for each unique song pairing. Gen-AI songs pairs are deemed too similar -- or unoriginal relative to each other -- if their maximum 8-beat correlation value exceeds the similarity threshold. We observe that 54% of Gen-AI song pairs fall into this category. Prompts that exhibit the highest probability for exceeding the threshold include mentions of a specific musical genre or song title.  Our preliminary results suggest that the Gen-AI system struggles to generate original material when different users send the same prompts.",CW,1,1,https://drive.google.com/file/d/1Eful_uryN_BilTGZ9uQ2r5szQViySPty/view?usp=drive_link,1,https://drive.google.com/file/d/1wfoif1YM1aFNcTExKQJvrkR4EYKDh9Nn/view?usp=drive_link,1,https://drive.google.com/file/d/1a6365oCsu4XWsEp6LeQr0S995PWK1lTn/view?usp=drive_link,1,https://drive.google.com/file/d/17DhrBuXdIGXNfZAkrtwHPDOWmK-JNeMA/view?usp=drive_link,1.0,https://drive.google.com/file/d/1fMvIDsr93JBEtzbmLznSCNF00uwDu-mU/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800N7TPEF,lbd-462-analysis-of-the
463,S3: A Symbolic Music Dataset for Computational Music Analysis of Symphonies,Zih-Syuan Lin (Academia Sinica)*; Yu-Chia Kuo (McGill University ); TZU-YUN Hung (National Taiwan Normal University); Wei-Yang Lin (National Taiwan University); YA-HSUAN CHU (National Yang-Ming Chiao-Tung University); Ting-Kang Wang (Academia Sinica); Jing-Heng Huang (Academia Sinica); Chien Chang (Academia Sinica); Christofer Julio (University of Malaya); Gloria Hsieh (Academia Sinica); Li Su (Academia Sinica),"The scarcity of symbolic music datasets has long been a challenge in the field of music information retrieval. Many studies have emphasized the need for high-quality, manually annotated datasets that include multifaceted labels, or focus on underrepresented periods like the Romantic period. In this paper, we present the S3, Symbolic Symphony Set, a comprehensive collection featuring four symphonies, totalling 16 movements, by Mozart, Beethoven, Dvorak, and Tchaikovsky. This dataset includes XML files and detailed annotations in the CSV format for notes and musical structure on both horizontal and vertical aspects, which are commonly known as form-related and texture-related information.  The note annotations are semi-automatically generated. Form-related information includes form analysis, cadence, and harmony, while orchestral texture include the role (melody, rhythm, harmony, or mixed) for each instrument. All annotations have been converted into CSV format to facilitate further analysis and modeling. Additionally, manually annotated PDF files are included in the dataset for reference. Our dataset is available on https://github.com/iis-mctl/mctl-symphony-dataset.",CW,1,1,https://drive.google.com/file/d/1Db_qa7JN2l8t0keedrqdwXSdg2Uxq7g1/view?usp=drive_link,1,https://drive.google.com/file/d/1hnYjKxQnpB_tOGGNJcbzQPmeQiFZSv6v/view?usp=drive_link,1,https://drive.google.com/file/d/1opPFp95MxHg56WHsL6wAvLPPO9kaZF57/view?usp=drive_link,1,https://drive.google.com/file/d/1Jyp192nFAtqyRhfow6zd8wPG_vIu5Nrk/view?usp=drive_link,1.0,https://drive.google.com/file/d/1B4xLsvr5X8MWPFHkpuJmd0buol06ljNX/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800QV1VJ6,lbd-463-s3-a-symbolic
464,Localify.org: Contextualizing Long-Tail Music for Local Artist Discovery,Paul Gagliano (Ithaca College)*; Griffin Homan (Ithaca College); Cassandra Raleault (Ithaca College); Ruth Ayambem (Ithaca College); Bridget Burns (Ithaca College); Douglas Turnbull (Ithaca College),"Localify.org is a personalized music recommendation service that helps users discover local artists and find live music events at small and medium-sized venues. Most local artists are relatively unknown and tend to reside in the long-tail of the artist popularity distribution. In this demo paper, we describe how we have designed Localify to contextualize long-tail artists for local music discovery. This focus on contextualization is reflected in our UI/UX design, our focus on explainable recommendations, our playlist generation algorithm, and our personalized weekly email digest.",CW,1,1,https://drive.google.com/file/d/1dpvgQQJHIarORK4Z3kAEkiaCfAGVEiyO/view?usp=drive_link,1,https://drive.google.com/file/d/1iZPuRfbNgesL2iSrgISDLV0eQI93DTQM/view?usp=drive_link,1,https://drive.google.com/file/d/1M9fX9P9Fxt-Fz-C7qF-RflwGxNpmbwnd/view?usp=drive_link,1,https://drive.google.com/file/d/1ODY4MPJ1bBTtGl1xTYrzvjgNQsFlBCsx/view?usp=drive_link,1.0,https://drive.google.com/file/d/1r0nty5ZvLYNqO7WDqrmRroVcZH8JClmY/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V0920CFR,lbd-464-localify-org-contextualizing
465,How does the teacher rate? Observations from the NeuroPiano dataset,"Huan Zhang (Queen Mary University of London)*; Vincent K.M. Cheung (Sony Computer Science Laboratories, Inc.); Hayato Nishioka (Sony Computer Science Laboratories, Inc); Simon Dixon (Queen Mary University of London); Shinichi Furuya (Sony Computer Science Laboratories Inc.)","This paper provides a detailed analysis of the NeuroPiano dataset, which comprise 104 audio recordings of student piano performances accompanied with 2255 textual feedback and ratings given by professional pianists. We offer a statistical overview of the dataset, focusing on the standardization of annotations and inter-annotator agreement across 12 evaluative questions concerning performance quality. We also explore the predictive relationship between audio features and teacher ratings via machine learning, as well as annotations provided for text analysis of the responses.",CW,1,1,https://drive.google.com/file/d/1vqMzYkoLO1suJOr3vvshbzdI1svZKCDH/view?usp=drive_link,1,https://drive.google.com/file/d/1xV7RhPeOvmltCyuIW4-dSDW1anBbSxVz/view?usp=drive_link,1,https://drive.google.com/file/d/1BTnf1AkPLw2xo2XDeRLCqQhvsl5ZJkqC/view?usp=drive_link,1,https://drive.google.com/file/d/1VLdmFn5Lydx5a6W7jzytYhpdGtNlQwrf/view?usp=drive_link,1.0,https://drive.google.com/file/d/1qATodsEYmyoBNvgmA99jx5QV-tT3azhu/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VBVA6T9T,lbd-465-how-does-the
466,Mamba-Based Model for Automatic Chord Recognition,"Chunyu N Yuan (the Graduate Center, CUNY); Johanna Devaney (Brooklyn College)*","In this work, we propose a new efficient solution, which is a Mamba-based model named BMACE (Bidirectional Mamba-based network, for Automatic Chord Estimation), which utilizes selective structured state-space models in a bidirectional Mamba layer to effectively model temporal dependencies. Our model achieves high prediction performance comparable to state-of-the-art models, with the advantage of requiring  fewer parameters and lower computational resources, while still captureing and leveraging long-term dependencies.",CW,1,1,https://drive.google.com/file/d/1eJ0EHsIDWnrjj1q_CNC5PgAP7WhKsBTn/view?usp=drive_link,1,https://drive.google.com/file/d/1-TCrsB8VqwYmIGKW_aRRIm9qKaxhWt2A/view?usp=drive_link,1,https://drive.google.com/file/d/1cZhOP2gJwSt-yX0rf8soOYAlWlU77LZt/view?usp=drive_link,1,https://drive.google.com/file/d/13CexEzJFDsYiszlAoLrgpy_atjD4t7uS/view?usp=drive_link,1.0,https://drive.google.com/file/d/1671CQfOyPXNme5hEJYX6lqWlujNmk69o/view?usp=drive_link,0,,1,The license is incorrect; email sent; fixed,https://ismir2024.slack.com/archives/C080PJXRHH6,lbd-466-mamba-based-model
467,Computationally Validating Synchronisation Between Musical Phrase Arcs and Autonomic Variables,Natalia Cotic (King's College London)*; Vanessa Pope (King's College London); Mateusz Solinski ( King's College London); Pier Lambiase (University College London); Elaine Chew (King's College London),"Previous research suggests that musical phrase structures may synchronise listeners' autonomic variables. We use a computational method to automatically identify probabilistic phrase arc boundaries from music audio and compare them to physiological envelopes (respiration and RR intervals). Automating the evaluation of synchronisation of autonomic responses to musical phrases enables the empirical evaluation of music's physiology-modulating power. Participants' respiration and RR intervals were recorded while listening to versions of Prokofiev's Gavotte Op.12 No.2. A novel Bayesian dynamic programming algorithm is used to derive phrase boundary credence profiles from the loudness. 
Increased curve similarity is observed between loudness phrase boundary credences and listeners' physiological signal envelopes, with the degree of response affected by track version. Loudness credence and RR interval entrainment is statistically significant for the original version.
We developed a fully automated system for evaluating musical phrase arc-autonomic variable entrainment. Initial findings suggest that phrase structures can affect physiological signals.",CW,1,1,https://drive.google.com/file/d/1dwIeqaV7BrRTeF7mKdY_1BUFZNdnEy8h/view?usp=drive_link,1,https://drive.google.com/file/d/1dJvvWy7IzhC-avsqKpZEBYUgL0zzn6l3/view?usp=drive_link,1,https://drive.google.com/file/d/1tNrDiAkXuOymRPjcpJajCBLqW7vjZe-k/view?usp=drive_link,1,https://drive.google.com/file/d/1souMYQ0lFhLDNL3EoGRyhV9ri3Jj8D2a/view?usp=drive_link,1.0,https://drive.google.com/file/d/1WAZoI6noxrHkNryWwOqoiHY20aTus50E/view?usp=drive_link,0,,1,volume a bit too low,https://ismir2024.slack.com/archives/C07V092G7HD,lbd-467-computationally-validating-synchronisation
468,Text2EQ: Human-in-the-Loop Co-Creation Interface for EQ,Annie Chu (Northwestern University)*; Hugo Flores García (Northwestern University); Patrick O'Reilly (Northwestern University); Bryan Pardo (Northwestern University),"We introduce Text2EQ, a human-in-the-loop semantic audio production interface that bridges the gap between intuitive language descriptors and equalization (EQ) parameters. Text2EQ enables users to describe their desired audio sound in natural language and maps these descriptors to EQ settings. The system offers initial suggestions and supports iterative refinement, allowing users to adjust parameters through direct manipulation of the EQ parameters or additional natural language inputs. We aim to contribute to the current dialogue on the role and incorporation of intelligent audio tools into pre-existing workflows, highlighting the importance of balancing usability with creativity.",CW,1,1,https://drive.google.com/file/d/1rMMPvI2WEi-ppWNIutq-b9Kyr5Kq23bW/view?usp=drive_link,1,https://drive.google.com/file/d/1I6D4497SwQjOOJF7Fn8d69Z_zUgpLpWG/view?usp=drive_link,1,https://drive.google.com/file/d/1AkrNlQ2F0-pYjWA3VcOKH618SNCW7xyc/view?usp=drive_link,1,https://drive.google.com/file/d/172CwsVEgKlwsrzahY42wnqftdkwmeVVB/view?usp=drive_link,1.0,https://drive.google.com/file/d/1ZprFlpcBYfiIJxuGAfQEdtPGjGRITOZ_/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V86R47EJ,lbd-468-text2eq-human-in
469,Language Models for Music Medicine Generation,"Emmanouil Nikolakakis (University of California, Santa Cruz); Joann Ching (Johannes Kepler University); Emmanouil Karystinaios (Johannes Kepler University)*; Gabrielle Sipin (Liberty Healthcare Corporation); Gerhard Widmer (Johannes Kepler University); Razvan V Marinescu (UC Santa Cruz)","Music therapy has been shown in recent years to provide multiple health benefits related to emotional wellness. In turn, maintaining a healthy emotional state has proven to be effective for patients undergoing treatment, such as Parkinson's patients or patients suffering from stress and anxiety. We propose fine-tuning MusicGen, a music-generating transformer model, to create short musical clips that assist patients in transitioning from negative to desired emotional states. Using low-rank decomposition fine-tuning on the MTG-Jamendo Dataset with emotion tags, we generate 30-second clips that adhere to the iso principle, guiding patients through intermediate states in the valence-arousal circumplex. The generated music is evaluated using a music emotion recognition model to ensure alignment with intended emotions. By concatenating these clips, we produce a 15-minute ""music medicine"" resembling a music therapy session. Our approach is the first model to leverage Language Models to generate music medicine. Ultimately, the output will be used as a temporary relief between music therapy sessions with a licensed therapist.",CW,1,1,https://drive.google.com/file/d/1GRDZR4FN76DIxh7o3TMmuRc9WTqweGC2/view?usp=drive_link,1,https://drive.google.com/file/d/1TAmuQygIuCJ4XD8c0zLF4tItc7piOerK/view?usp=drive_link,1,https://drive.google.com/file/d/1jSeJw6HAJ4dfgGOc0yYTSzLucY2o9fMx/view?usp=drive_link,1,https://drive.google.com/file/d/1NZxOaLZHHSj3scfqSDICu5RtReVKOj5w/view?usp=drive_link,1.0,https://drive.google.com/file/d/13kO6UMYiNHfwW3p1DlP4_Jc1nfAsGsQF/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800N9CXMZ,lbd-469-language-models-for
470,GENERATIVE SINGING STYLE TRANSFER ACROSS GENRES,Saanvi Bhargava (The Harker School); Ethan Chu (Monta Vista High School); Matthew Lee (Riverdale Country School); Chuyang Chen (New York University); Kelvin Walls (New York University); Bea Steers (New York University); Iran R Roman (Stanford University)*,"Voice style transfer has focused on transforming speech between speakers, leaving singing style, independent of speaker, unexplored. We introduce SingStyleTransfer, a VAE-GAN to perform singing style transfer across genres. The model is evaluated on the SingStyle111 dataset for its ability to carry out genre-to-genre transformations.",CW,1,1,https://drive.google.com/file/d/1THjbdY8HtVlzUlrNyEKUHDZYprTXdRQ6/view?usp=drive_link,1,https://drive.google.com/file/d/1f_zGJGI5-xQTu910bUgxYsSsz5yvjhbO/view?usp=drive_link,1,https://drive.google.com/file/d/1019_0ntA8I1bih_FJcrt6tMMZoWME2F-/view?usp=drive_link,1,https://drive.google.com/file/d/1zHm-V590lTzTNgpSu0v5h_Wc70uCgNpX/view?usp=drive_link,1.0,https://drive.google.com/file/d/1uX894piD0WY77QCCgYlB1K--c12livzh/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VBVBBTKP,lbd-470-generative-singing-style
471,Using feature-based composer classification to test musicological evidence for Josquin attribution,Cory McKay (Marianopolis College)*; Julie Cumming (McGill University),"The musicologists Jesse Rodin and Joshua Rifkin have presented an important taxonomy of works that could potentially be attributed to the Renaissance composer Josquin des Prez. It is broken into five categories indicating how strong the evidence is that Josquin was the composer of the given pieces, based primarily on historical evidence from sources and biography. We apply statistical analysis and machine learning to features extracted from the scores of all this music using our jSymbolic software to see how well a purely content-based approach supports (or contradicts) this taxonomy. We also identify particularly statistically characteristic features of Josquin’s style.",CN,1,1,https://drive.google.com/file/d/1u-e0rrWZGvXZhGs4tAdzsWdSeMvD5BgQ/view,1,https://drive.google.com/file/d/13ugDIWrnPS2x7wscij_xplro8N0PGR3E/view,1,https://drive.google.com/file/d/1vi65NhYCbQEcwsPC5VaFfvK0oUDlZBHw/view,1,https://drive.google.com/file/d/1wikIq3_o4sFfM-a6OQvEhU4PrAFZ3G7t/view,1.0,https://drive.google.com/file/d/1_azh7crHWhymtXgNrvBsfDegQWWlrmC6/view,0,,1,,https://ismir2024.slack.com/archives/C0803AG0YTW,lbd-471-using-feature-based
472,Real-time Flutist Gesture Cue Detection System for Auto-Accompaniment,Jaeran Choi (KAIST)*; Taegyun Kwon (KAIST); Joonhyung Bae (KAIST); Jiyun Park (KAIST); Yonghyun Kim (Georgia Institute of Technology); Juhan Nam (KAIST),"Visual cues are essential for synchronization in ensemble performances, especially at the start of a piece or during fermatas with sudden tempo changes, where gestures facilitate coordination. However, in human-automated piano ensembles, traditional gesture cues are inapplicable due to the absence of a pianist. To address this, we propose a real-time gesture cue detection and visual feedback system which is structured into two main components: (1) the development of an accompaniment system that detects flutists’ gesture cues and predicts onset timings for initiating playback, and (2) the provision of real-time visual feedback regarding cue detection status, enabling performers to synchronize with the system effectively.
The bidirectional gesture cue detection and visual feedback system was empirically validated in live performances with professional musicians, demonstrating its capability to maintain a natural musical flow. This approach surpasses the limitations of previous audio-based accompaniment systems by incorporating gesture cues at critical moments, such as the start of pieces and fermata passages, enabling optimized reactive accompaniment.",CN,1,1,https://drive.google.com/file/d/1eYnEudUIYrghUMcOjKJmDha--WMH1qfS/view,1,https://drive.google.com/file/d/1MTqufFVhaVgTu6PbzpUgvv0LKjRtRJxj/view,1,https://drive.google.com/file/d/1z2FZHAPd9zxcKuy6JH2pfY26T8OqXegJ/view,1,https://drive.google.com/file/d/1tyQ9lb8Ea0WDqZ7bXouSRSIdr1NJx7Vv/view,1.0,https://drive.google.com/file/d/1XyeLNLwjTdY7A7x8jSqV72G-LCmiC5Ri/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V86S74DU,lbd-472-real-time-flutist
473,Exploring Tokenization Methods for Multitrack Sheet Music Generation,Yashan Wang (Central Conservatory of Music)*; Shangda Wu (Central Conservatory of Music); Xingjian Du (University of Rochester); Maosong Sun (Tsinghua University),"This study explores the tokenization of multitrack sheet music in ABC notation, introducing two methods—bar-stream and line-stream patching. We compare these methods against existing techniques, including bar patching, byte patching, and Byte Pair Encoding (BPE). In terms of both computational efficiency and the musicality of the generated compositions, experimental results show that bar-stream patching performs best overall compared to the others, which makes it a promising tokenization strategy for sheet music generation.",CN,1,1,https://drive.google.com/file/d/1dfdGvxb0cuGxwE4FmMoBym8OgV_s1B8f/view,1,https://drive.google.com/file/d/1eaAlFXR0D7Cz9ES3Hnlyye1EAt0w07R9/view,1,https://drive.google.com/file/d/1lPIghofshsCzAhErcl5F0HF5PeOFg_Fc/view,1,https://drive.google.com/file/d/1QLEkRQAj5picVK3lvoSUZh_5FEvnRuAm/view,1.0,https://drive.google.com/file/d/1JL6PNyvdUW6QDVfn8zoEp5meg8gtDb8B/view,0,,1,,https://ismir2024.slack.com/archives/C07V0943DP1,lbd-473-exploring-tokenization-methods
474,Masked Token Modeling for Zero-Shot Anything-to-Drums Conversion,Patrick O'Reilly (Northwestern University)*; Hugo Flores García (Northwestern University); Prem Seetharaman (Adobe); Bryan Pardo (Northwestern University),"Musicians often represent drum beats through sound gestures such as vocal imitation and finger tapping. While these gestures can convey rich rhythmic information, realizing them as fully-produced drum beats requires time and skill. We propose a system for mapping arbitrary percussive sound gestures to high-fidelity drum recordings. Our system, dubbed TRIA (The Rhythm In Anything), takes as input two audio prompts -- one specifying the desired drum timbre, and one specifying the desired rhythm -- and generates audio satisfying both prompts (i.e. playing the desired rhythm with the desired timbre). TRIA can synthesize realistic drum audio given rhythm prompts from a variety of non-drum sound sources (e.g. beatboxing, environmental sound) in a zero-shot manner, enabling novel creative interactions.",CN,1,1,https://drive.google.com/file/d/1Busk6oAGOBNvNFVeuNpY6HTeqExmNWAy/view,1,https://drive.google.com/file/d/1SETjengbSOtBNuaMQymE8OTBHRJJjTi9/view,1,https://drive.google.com/file/d/1yFHxHingLsY-uXt-angN_hskoY7q35KK/view,1,https://drive.google.com/file/d/1EEGXNHAhRbKgDUYvyP2kdp1rOY5j8vv6/view,0.0,https://drive.google.com/file/d/10ieZILwQcTSzn_ABlnwRHvJBFVbIZL0S/view,0,,1,,https://ismir2024.slack.com/archives/C0800NAK283,lbd-474-masked-token-modeling
475,Diff-MST^C: A Mixing Style Transfer Prototype for Cubase,Soumya Sai Vanka (QMUL)*; Lennart Hannink (Steinberg Media Technologies GmbH); Jean-Baptiste Rolland (Steinberg Media Technologies GmbH); George Fazekas (QMUL),"In our demo, participants are invited to explore the Diff-MST^C prototype, which integrates the Diff-MST model into Steinberg's digital audio workstation (DAW), Cubase. Diff-MST, a deep learning model for mixing style transfer, forecasts mixing console parameters for tracks using a reference song. The system processes up to 20 raw tracks along with a reference song to predict mixing console parameters that can be used to create an initial mix. Users have the option to manually adjust these parameters further for greater control. In contrast to earlier deep learning systems that are limited to research ideas, Diff-MST^C is a first-of-its-kind prototype integrated into a DAW. This integration facilitates mixing decisions on multitracks and lets users input context through a reference song, followed by fine-tuning of audio effects in a traditional manner.",CN,1,1,https://drive.google.com/file/d/1D-4dYs3JBa0ub8vj-ptpnuERgxsOeleX/view,1,https://drive.google.com/file/d/1Hyyhp1UBuHb6sn6odMuOabMI867uUSe3/view,1,https://drive.google.com/file/d/1WZsFYnUNxLTD8ccI0R5tK0sSavFX77gM/view,1,https://drive.google.com/file/d/11Gv33iSjlaIqbO420W2sLe3srFgf-VB5/view,1.0,https://drive.google.com/file/d/1k4HlKi0QLONuqwy-3jSGDXi-S_UN9J6c/view,0,,1,,https://ismir2024.slack.com/archives/C080PJZU8V6,lbd-475-diff-mst-c
477,SONG REVIEW GENERATION USING ACOUSTIC INFORMATION AND LYRICS,Keita Kawachi (Nagoya Institute of Technology)*; Shinji Sako (Nagoya Institute of Technology),"In recent years, the spread of music streaming services has
 greatly increased access to music, but it seems that oppor
tunities to deeply appreciate music have decreased. There
fore, we believe that verbalizing and explaining music is
 one way to help listeners understand music and enhance
 satisfaction in the music experience. Conventional meth
ods for verbalization of music has focused on superficially
 explaining the song using acoustic information. In this
 study, we attempted to verbalize the impression and atmo
sphere of the song by generating review text that takes into
 account lyrics in addition to acoustic information.",CN,1,1,https://drive.google.com/file/d/1j4E6UR8C-bt8jdtX9-T2c5dshAP9bprJ/view,1,https://drive.google.com/file/d/1o4uMoyOobzhYG3spIIv4GuW0z-qnftro/view,1,https://drive.google.com/file/d/1qOnboqhMZHowEjz98eAk998xitJT43wA/view,1,https://drive.google.com/file/d/1ITcD5rQnN6ao9p427kvkMCxbJTVZ7l9f/view,1.0,https://drive.google.com/file/d/1sADKYk5BI1gIvkTGoNLTMML3kv2QEz6u/view,0,,1,,https://ismir2024.slack.com/archives/C0800NB3B9R,lbd-477-song-review-generation
478,UNCOVERING THE MICROTONES IN A RAAG FROM NOTE TRANSCRIPTIONS,Neeraja Abhyankar (Unaffiliated)*,"The presentation of Indian Classical Music admits the use of microtones (""shruti""s or frequency ratios) that are characteristic of each Raag which can be described as a set of notes (a scale) plus a ""chalan"" (melody patterns and rules regarding permissible/emphasized combinations of notes) meant to create a certain mood. Often, the Raag's definition as appearing in pedagogy does not include microtonal information. Hindustani Classical Music is transcribed and spoken of with a 12-note system. Despite this, most advanced vocalists or musicians with a continuous-frequency-domain instrument will tend to use appropriate microtones during rendition. This suggests that the microtonal information may be consistently derived based on the given description of the Raag. In this study, we attempt to use empirical analysis and MIR techniques to discover rules, if any, that may guide this derivation algorithmically.",CN,1,1,https://drive.google.com/file/d/1QFPmPiDR5vC1a2WMSSxwCed99eKZtZ4h/view,1,https://drive.google.com/file/d/1c_3zvZlB3HV5lFAw-KpyPmuo0WKdJZeE/view,1,https://drive.google.com/file/d/1jtzxS9nM74o2vV6zkFSr5Ei_nyoIvSEu/view,1,https://drive.google.com/file/d/1EXexA2VGCFyevvYUkCY-6CtpUiR9NQE5/view,1.0,https://drive.google.com/file/d/1fae5eM1GTtEkCuR-mL2hqam63YBv7QoV/view,0,,1,,https://ismir2024.slack.com/archives/C07V094SSBH,lbd-478-uncovering-the-microtones
479,Exploring Transformer-Based Music Overpainting for Jazz Piano Variations,Eleanor Row (Queen Mary University of London)*; Ivan Shanin (Queen Mary University of London); George Fazekas (QMUL),"This paper explores transformer-based models for music overpainting, focusing on jazz piano variations. Music overpainting generates new variations while preserving the melodic and harmonic structure of the input. Existing approaches are limited by small datasets, restricting scalability and diversity. We introduce VAR4000, a subset of a larger dataset for jazz piano performances, consisting of 4,352 training pairs. Using a semi-automatic pipeline, we evaluate two transformer configurations on VAR4000, comparing their performance with the smaller JAZZVAR dataset. Preliminary results show promising improvements in generalisation and performance with the larger dataset configuration, highlighting the potential of transformer models to scale effectively for music overpainting on larger and more diverse datasets.",CN,1,1,https://drive.google.com/file/d/1F20Tk5pAtYqSnbhhtQnESO7NrjyxxlYZ/view,1,https://drive.google.com/file/d/1pv2-7r1dl2ccStz7r9MS4nknyxS9ASe3/view,1,https://drive.google.com/file/d/1lYkDhhkYExFgwMAhABdXsPjWXz6nxFMQ/view,1,https://drive.google.com/file/d/19NNcdk-paVXOOj1kMNDHb8AHK7uYbYsT/view,1.0,https://drive.google.com/file/d/14TGGmfIceUe9WFQPrWFnIZ8rRwtbOtNY/view,0,,1,,https://ismir2024.slack.com/archives/C080PK0GN64,lbd-479-exploring-transformer-based
480,Pitch ControlNet: Continuous Pitch Control for Monophonic Instrument Sound Generation,Dabin Kim (Korea Advanced Institute of Science and Technology)*; Junwon Lee (KAIST); Minseo Kim (KAIST); Juhan Nam (KAIST),"In monophonic instrument sound generation tasks, integrating continuous pitch control with text-to-audio (TTA) models is crucial for practical music production. To address this, we propose Pitch-ControlNet, a framework leveraging ControlNet to reflect time-varying pitch expressions in the generated audio of pretrained AudioLDM. Our approach enables sequence-level pitch manipulation by utilizing fundamental frequency (f0) contours, while retaining the benefits of high-quality text-prompted generation from the pretrained model. Experimental results show that our framework consistently achieves high pitch accuracy across a wide frequency range, preserving the target instrument’s timbre and high audio quality. The model’s potential for practical application in music production is showcased on our demo website.",CN,1,1,https://drive.google.com/file/d/1nL3CiY3aOAG6ixGGJ4pjwCJCURJ1afy6/view,1,https://drive.google.com/file/d/18be9r0WcGflWwevqbkB6jGGyuj9U9t2a/view,0,https://drive.google.com/file/d/1EIXNshmmCWNGp7kgvAFhX6RtNexgm8JB/view,1,https://drive.google.com/file/d/1fcmuZecOb2uT_AUjvUYwBupOpH1Cdm_q/view,1.0,https://drive.google.com/file/d/1gPELuSzvgSWVNOtI03gwyhgKrmKlxkI-/view,0,,1,"generated captions don't show, srt link failed",https://ismir2024.slack.com/archives/C0803AHJVJQ,lbd-480-pitch-controlnet-continuous
481,CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music,Mateusz Modrzejewski (Warsaw University of Technology)*; Aleksander Tym (Closer Music); Aleksandra Piekarzewicz (Closer Music); Tomasz Sroka (Closer Music),"In this paper, we introduce CloserMusicDB, a collection of full length studio quality tracks annotated by a team of human experts. We describe the selected qualities of our dataset, along with three example tasks possible to perform using this dataset: hook detection, contextual tagging and artist identification. We conduct baseline experiments and provide initial benchmarks for these tasks.",CN,1,1,https://drive.google.com/file/d/1w0dImmfItt-cqYhu8rtsDN-lRMuFPxF7/view,1,https://drive.google.com/file/d/1R7ldjD-qv5jJZfXIvGUi_AM6qys5GlEO/view,1,https://drive.google.com/file/d/1qhslAD32qliU4wrHUjPb6v2ZdeJ1BBpu/view,1,https://drive.google.com/file/d/10gixKCAMp-VJ3RhlcAD4s6VFHdXvRdeo/view,1.0,https://drive.google.com/file/d/1Dcsp3-ABOQDXH-UbMcQLXuOJf0CcbqdM/view,0,,1,,https://ismir2024.slack.com/archives/C0803AHRMK6,lbd-481-closermusicdb-a-modern
482,Enhanced Automatic Drum Transcription via Drum Stem Source Separation,Xavier Riley (C4DM)*; Simon Dixon (Queen Mary University of London),"Automatic Drum Transcription (ADT) remains a challenging task in MIR but recent advances allow accurate transcription of drum kits with up 5 classes - kick, snare, hi-hats, toms and cymbals - via the ADTOF package. In addition, several drum kit stem separation models in the open source community support separation for more than 6 stem classes, including for distinct crash and ride cymbals. In this work we explore the benefits of combining these tools to improve the realism and accuracy of drum transcriptions. We describe a simple post-processing step which expands the transcription output from five to seven classes. Furthermore, we are able to estimate MIDI velocity values based on the separated stems. Our solution achieves strong performance when assessed against a baseline of 8-class drum transcription and produces realistic MIDI transcriptions suitable for MIR or music production tasks.",CN,1,1,https://drive.google.com/file/d/1B3pPQSAnmjSU9iXGEPVB349J02oMcLu9/view,1,https://drive.google.com/file/d/1ANCBR8YXKfm8XkC0DlcgrZSm6MpCyDk0/view,1,https://drive.google.com/file/d/1qMCt0rdZAsIw-UnxZ-2Fw3TuX6PwSwEy/view,1,https://drive.google.com/file/d/17jcT-O-gmFEH01BIXlABFkin7KlCskfR/view,1.0,https://drive.google.com/file/d/1PGc12ryY9qozbJ6xMPxoj1LMdNDhbvnE/view,1,https://drive.google.com/file/d/1VU_0SFUH24kB1Ff0kOvWfe4i-iqA7x3u/view,1,,https://ismir2024.slack.com/archives/C080PK14BUG,lbd-482-enhanced-automatic-drum
483,MODELING PREDOMINANT INSTRUMENTATION WITH DIFFUSION,Charis Cochran (Drexel University)*; Youngmoo Kim (Drexel University),"Most music consumption involves multi-instrument audio, and thus modeling complex timbres is vital for real-world music analysis and Music Information Retrieval (MIR) tasks. However, this task remains challenging in musical mixtures where definitions of timbre are less clear. Additionally, the advent of generative and multi-modal music models necessitates precise timbre representation for enhanced control in music generation and editing. Based on previous work highlighting the difficulties with learning instrument timbres in the context of Predominant Instrument Recognition (PIR), we explore the potential of diffusion networks to learn salient timbre representations for predominant instrument generation and timbre transfer. The results highlight the power of generative models to deepen our understanding of instrument timbre for complex scenarios with limited real-world data.",CN,1,1,https://drive.google.com/file/d/1-yxzdZ1dHHF-bA36iLflfnqC0_RU7Bs-/view?usp=drive_link,1,https://drive.google.com/file/d/1fWiB1jQQRPivvGGXxdsRU8m01HvMnt6_/view,1,https://drive.google.com/file/d/1Ni2bgAALj0j_K7c85PNuKjjvZ7S720XO/view,1,https://drive.google.com/file/d/1WKoRXEMpd1aFVQmNzBy12xEy7OSrEarl/view,1.0,https://drive.google.com/file/d/19pLOw32AA_2JIzfaDONXFGwIJC87sXGG/view,0,,1,,https://ismir2024.slack.com/archives/C080PK1A924,lbd-483-modeling-predominant-instrumentation
485,Skip That Beat: Augmenting Meter Tracking Models for Underrepresented Time Signatures,Giovana V Morais (New York University)*; Brian McFee (New York University); Magdalena Fuentes (New York University),"Beat and downbeat tracking models are predominantly developed using datasets with music in 4/4 meter, which decreases their generalization to repertories in other time signatures.  In this work, we propose a simple augmentation technique to increase the representation of time signatures beyond 4/4, namely 2/4 and 3/4. Our augmentation procedure works by removing beat intervals from 4/4 annotated tracks. We show that the augmented
data helps to improve downbeat tracking for underrepresented meters while preserving the overall performance of beat tracking in two different models. We also show that this technique helps improve downbeat tracking in an unseen Samba dataset.",CN,1,1,https://drive.google.com/file/d/1TPHc51Lik7ELDVLOIcOcpNZtxouayADa/view,1,https://drive.google.com/file/d/1w0ZIqMIwTD6c4PdF8FMzyrLwwO1UnE6i/view,1,https://drive.google.com/file/d/1joPj1oYl_Aku7mn5BEaZSOZnMvsfZbmH/view,1,https://drive.google.com/file/d/1gcjqDdfykdHhKJTLXqRA4BSLQG45yCzT/view,1.0,https://drive.google.com/file/d/18tzMlHmtMFszJIWZnxysIC0jeW2gG1OH/view,0,,1,,https://ismir2024.slack.com/archives/C080DFQB58R,lbd-485-skip-that-beat
486,BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset,Jiyeoung Sim (The CUNY Graduate Center); Rebecca Moranis (CUNY Graduate Center); Johanna Devaney (Brooklyn College)*,"This paper presents a set of harmonic function annotations for Isophonics' The Beatles datasets. Harmonic function annotations characterize chord labels as stable (tonic) or unstable (predominant, dominant). They operate at the level of musical phrases and thus serve as a link between chord labels and higher-level formal structures.",CN,1,1,https://drive.google.com/file/d/1_4xol6NStCCYCmyq-pYja7MKartt4SxB/view?usp=drive_link,1,https://drive.google.com/file/d/1usG1LJFI97rFMlXHRayjwv9whqWMRURe/view,1,https://drive.google.com/file/d/11UoGxhU3h1baCWavRom2qGS5hu-O1SMK/view,1,https://drive.google.com/file/d/103FvNjWyXtEf2AdHY1HTPtwNF1UmK7A3/view,1.0,https://drive.google.com/file/d/1qO4UNCqQGbu1obBC-Pc5vwaJkOQYh2zE/view,0,,1,,https://ismir2024.slack.com/archives/C080DFQHN1F,lbd-486-beatlesfc-harmonic-function
487,Zero-Shot Structure Labeling with Audio and Language Model Embeddings,Morgan Buisson (Telecom-Paris)*; Christopher A Ick (New York University); Qingyang Xi (NYU); Brian McFee (New York University),"Recent progress on audio-based music structure analysis has closely aligned with the appearance of new deep learning paradigms, notably for the extraction of robust spectro-temporal audio features and their sequential modeling. However, most recent methods resort to supervised learning, which requires careful annotation of audio music pieces. Such annotations may sometimes operate at different temporal scales from one dataset to another or comprise inconsistent variation markers across repetitions of identical segments. This work explores language models as an alternative to manual pre-processing of the section label space, thus facilitating training and predictions across different annotated corpora. We propose a joint audio-to-text embedding space in which latent representations of audio frames and their respective section labels are close. We take inspiration from recent works on cross-modal contrastive learning and demonstrate the plausibility of this paradigm in the context of music structure analysis.",CW,1,1,https://drive.google.com/file/d/12sYEWI74iqizusx056SPyoBQFtWc6HAC/view?usp=drive_link,1,https://drive.google.com/file/d/1Fa2ZpA7eMEWnYMSyCCm9FpBepGaIPofu/view?usp=drive_link,1,https://drive.google.com/file/d/1EFLNqKaTrBC4fkA37xFqTVa-Y_LKXRjR/view?usp=drive_link,1,https://drive.google.com/file/d/1MS7k9CrzCip--jVAOwOMez0n9v3RkCyw/view?usp=drive_link,1.0,https://drive.google.com/file/d/17XfHckMwT1iHt7XcsIEsn9YHEJbZux9t/view?usp=drive_link,0,,1,the author used the main paper track format... ,https://ismir2024.slack.com/archives/C0800NCSDFV,lbd-487-zero-shot-structure
488,Local Deployment of Large-Scale Music AI Models on Commodity Hardware,Xun Zhou (Carnegie Mellon University)*; Charlie Ruan (Carnegie Melllon University); Zihe Zhao (Carnegie Melllon University); Chris Donahue (Carnegie Mellon University),"We present the Infinite Music Player, a web application
capable of generating symbolic music using a large-scale
generative AI model locally on commodity hardware. Creating this demo involved porting the Anticipatory Music
Transformer, a large language model (LLM) pre-trained
on the Lakh MIDI dataset, to the Machine Learning Compilation (MLC) framework. Once the model is ported,
MLC facilitates inference on a variety of runtimes including C++, mobile, and the browser. We envision that MLC
has the potential to bridge the gap between the landscape
of increasingly capable music AI models and technology
more familiar to music software developers. As a proof
of concept, we build a web application that allows users to
generate endless streams of multi-instrumental MIDI in the
browser, either from scratch or conditioned on a prompt.
On commodity hardware (an M3 Macbook Pro), our demo
can generate 51 notes per second, which is faster than real-
time playback for 72.9% of generations, and increases to
86.3% with 2 seconds of upfront buffering.",CW,1,1,https://drive.google.com/file/d/14aVJldAuXTsJe5ks_KrJ9_kxNIY9RgNO/view?usp=drive_link,1,https://drive.google.com/file/d/1lf3P5jxIRKkEkK-1eFyFm79uME6qa5rM/view?usp=drive_link,1,https://drive.google.com/file/d/17bL8Qveou9BrGbg0xC0yg2VVph6SS8nJ/view?usp=drive_link,1,https://drive.google.com/file/d/1LU5e5aVFPlDnlh416CjbaNcdT88MxasJ/view?usp=drive_link,1.0,https://drive.google.com/file/d/1T4JpGI8yTaOnA3lzJuBSzNZFnYj5SeW2/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VC00PN21,lbd-488-local-deployment-of
489,Hookpad Aria: A Copilot for Songwriters,Chris Donahue (CMU)*; Shih-Lun Wu (Carnegie Mellon University); Yewon Kim (KAIST); Dave Carlton (Hooktheory); Ryan Miyakawa (Hooktheory); John Thickstun (University of Washington),"We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets - symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation—since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.",CN,1,1,https://drive.google.com/file/d/1U7O9BMdK3RSznEAn9qCrp3RjW93GynGB/view,1,https://drive.google.com/file/d/1999UU9h6yiTv5VKlz281Syl73j1zZsK_/view,1,https://drive.google.com/file/d/1z0fVeAudNzE-KUkLMjQ3EM45aQJGLgma/view,1,https://drive.google.com/file/d/177BlzC5EaogFxSaOpM3ZkwhnCsPEVLB9/view,1.0,https://drive.google.com/file/d/1dSbZ-essFc7jvSl-OwmzfWKXp9J4dDaX/view,0,,1,,https://ismir2024.slack.com/archives/C0800NDALPM,lbd-489-hookpad-aria-a
490,Musical Source Separation of Brazilian Percussion,Richa Namballa (New York University)*; Giovana V Morais (New York University); Magdalena Fuentes (New York University),"Musical source separation (MSS) has recently seen a big breakthrough in separating instruments from a mixture in the context of Western music, but research on non-Western instruments is still limited due to a lack of data. In this demo, we use an existing dataset of Brazilian samba percussion to create artificial mixtures for training a U-Net model to separate the surdo drum, a traditional instrument in samba. Despite limited training data, the model effectively isolates the surdo, given the drum's repetitive patterns and its characteristic low-pitched timbre. These results suggest that MSS systems can be successfully harnessed to work in more culturally-inclusive scenarios without the need of collecting extensive amounts of data.",CW,1,1,https://drive.google.com/file/d/1Rzp6aBpGhnIOC2PbkBqAaJ0jsMQTe6-V/view?usp=drive_link,1,https://drive.google.com/file/d/1NCccreEwDCs4CHAJWfwr7j1r41UXQWeG/view?usp=drive_link,1,https://drive.google.com/file/d/11Zf_1KvBG3JLbP9rACJ2KdGXZgVMmAox/view?usp=drive_link,1,https://drive.google.com/file/d/1pmN1K2q_u-HjBsHp1p496N51LtQ7aC7q/view?usp=drive_link,1.0,https://drive.google.com/file/d/1MU-bHkPlDln1Js29J4RHfTuKH0hxMrAZ/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800R0EJD8,lbd-490-musical-source-separation
491,Collecting & Managing the Metadata on the Data of ISMIR,Ashley Luna (Smith College); Diana Diaz (Smith College); Charis Cochran (Drexel University); Andrew Wiggins (Drexel University); Katherine M. Kinnaird (Smith College)*,"Much of the research in Music Information Retrieval (MIR) relies on data: collections of audio recordings, musical scores, meta-information about music, human opinions, and more. The International Society for MIR (ISMIR) Conference offers a glimpse into the data being used in MIR research and thus what is (and is not) studied by the field. Unlike past ISMIR conference proceedings with consistent metadata on each paper, there does not exist a comprehensive repository with detailed metadata on the datasets used in ISMIR papers. We propose a collection tool for community-sourcing the meta information about the data of ISMIR as well as a procedure for processing and cleaning that collected data. We view this as a necessary first step towards being able to quantify the influence of certain datasets across MIR as well as address questions of data diversity and generalizability of MIR work.",CW,1,1,https://drive.google.com/file/d/1aunNbJ-A4ZYRnn3m4VDIBW5EWKbOui83/view?usp=drive_link,1,https://drive.google.com/file/d/1sYkke96_uJUUwdb7k-81kzaFB9d-AwrI/view?usp=drive_link,1,https://drive.google.com/file/d/19aHrIBqW7BKAFQSNlMP7DrZ_FKjOX5_d/view?usp=drive_link,1,https://drive.google.com/file/d/15eZCBxyBu-wAGsG_w0D3-ebUuAFZdjxS/view?usp=drive_link,1.0,https://drive.google.com/file/d/1H7GXbKnM_W5ZUGyM7c6eVtw3-XWe6J_n/view?usp=drive_link,0,,1,Author reached ... will send by Monday...; fixed,https://ismir2024.slack.com/archives/C080DFRL6TT,lbd-491-collecting-managing-the
492,ARTIFICIAL ACOUSTIC PIANO RESONANCE WITH A SOUNDBOARD-MOUNTED SHAKER,William A Thompson (University of Southern Mississippi)*; Austin A Franklin (Louisiana State University),"This project demonstrates a method for artificially manipulating acoustic piano resonance through real-time sound filtering and reproduction using a soundboard-mounted shaker. Various filters are applied to shape the harmonic and timbral characteristics of the instrument, which are then reproduced through vibrations in the soundboard. The acoustic and filtered sounds are played simultaneously, allowing listeners to perceive how different filters color the natural resonance of the piano. This investigation focuses on how specific filters alter the tonal quality of the instrument and explores their potential for creative sound manipulation within acoustic contexts.",CW,1,1,https://drive.google.com/file/d/1zd0d85K7LZXPh92444z4jruiLzi5yO3T/view?usp=drive_link,1,https://drive.google.com/file/d/1tjiLt3D3vsuocMLbvooE-2Hq1FaeLe1p/view?usp=drive_link,0,,1,https://drive.google.com/file/d/13MwBSaizaLoZCreaq05WNrGtU9r2w9Wh/view?usp=drive_link,1.0,https://drive.google.com/file/d/1MXI_JnJJwRgdEk8QlOMfe3w5kcAQ1OzN/view?usp=drive_link,0,,1,no speech,https://ismir2024.slack.com/archives/C07VC01JTEH,lbd-492-artificial-acoustic-piano
493,AN EXPLORATION OF MUSIC STRUCTURE SEGMENTATION USING EEG DATA AND MSAF ALGORITHMS,Neha Rajagopalan (Stanford University)*; Blair Kaneshiro (Stanford University),"Inter-subject correlation (ISC) of electroencephalographic
(EEG) responses to natural stimuli is thought to index at-
tention, engagement, and salient stimulus events. We in-
vestigate correspondences between EEG ISC and com-
putationally derived music structure segmentation bound-
aries. Using a publicly available EEG dataset, we com-
puted time-resolved ISC of spatially optimized responses
to a full-length Bollywood song. We also used the MSAF
toolbox to compute structural segmentation boundaries
from the stimulus audio. Preliminary results reveal that dif-
ferent segmentation algorithms produce different boundary
timestamps and that some but not all segmentation bound-
aries align with ISC peaks. These findings encourage fur-
ther research aimed toward advancing music neuroscience
and multimodal approaches to music structure analysis",CW,1,1,https://drive.google.com/file/d/1llC7I49UWrA3JQnznqgbl064ORKMi9fX/view?usp=drive_link,1,https://drive.google.com/file/d/18fxMJXopRF7ARl-HY9Du9o_awZSoPXGt/view?usp=drive_link,1,https://drive.google.com/file/d/1T85z1qOF8V5_DnZOZzSnZrxh3zGT4d4U/view?usp=drive_link,1,https://drive.google.com/file/d/10yJC7IDI7-3qk09y82nvc0IHgQ07kt8o/view?usp=drive_link,1.0,https://drive.google.com/file/d/1pQCKN61Iu_Ad0f1RmGxoLQTmGKGo8uUk/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VC01N9JR,lbd-493-an-exploration-of
494,Audio Data Defenses: Protecting Music and Speech Data from Targeted Attacks,Julia Barnett (Northwestern University)*; William Agnew (Carnegie Mellon University); Robin Netzorg (UC Berkeley); Patrick O'Reilly (Northwestern University); Ezra Awumey (Carnegie Mellon University); Chris Donahue (Carnegie Mellon University); Sauvik Das (Carnegie Mellon University),"Use of data in AI systems without consent or in violation of copyright agreements has become a pressing concern as AI systems have become increasingly capable and commercialized. In the audio domain, these concerns extend to music styles, voice prints, and lyrics being replicated and extended, causing economic, security, and representation harms. In this paper we present initial work on protecting audio from unauthorized AI inference, including voice cloning and music extension. We utilize encoder-based attacks to add noise to audio to distort the encoded latent while minimally changing the original audio. We conduct small-scale experiments showing the effectiveness of our protection, and discuss next steps needed to develop a defense that is both effective and acceptable to audio workers. We present our results at tinyurl.com/audio-protection.",CW,1,1,https://drive.google.com/file/d/1X2nEZW8kMfymVLhqeuuyoj-XttQ4nGX0/view?usp=drive_link,1,https://drive.google.com/file/d/1gOlpLG22nvjClQGbWZyFq995c0AjxymO/view?usp=drive_link,1,https://drive.google.com/file/d/1-Kl-3zCqysYa7l512ZUzI02VbM5N7sX6/view?usp=drive_link,1,https://drive.google.com/file/d/1D4hLx_aTwlBE7pnULiyJswd9kQxsSfgL/view?usp=drive_link,1.0,https://drive.google.com/file/d/1VBrBWrGf0Y_mfv1NF_ajbdygf_mhWHJw/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C080PK3BQL8,lbd-494-audio-data-defenses
495,Interval Mover’s Distance: Melodic Stylistic Analysis Using Theoretical Frameworks,Valeri Sazonov (University of Alabama)*,"Quantifying similarity of melodic features has been a chal-
lenge not only in Music Information Retrieval (MIR), but
music theory in general. Despite this, music theoretical
models are often underutilized in music informatics. Mu-
sic theory informed models can provide certain advan-
tages over pure machine learning models, such as resource-
efficiency and interpretability. This paper proposes the ap-
plication of Earth Mover’s Distance onto a novel represen-
tation of the interval content of a song based on the cir-
cle of fifths, dubbing the resulting distance metric Inter-
val Mover’s Distance. Interval Mover’s Distance may be
applied to various MIR tasks concerning stylistic similar-
ity, such as music recommendation algorithms, and par-
ticularly, novel machine learning tasks such as manifold
learning. Uniform Manifold Approximation and Projec-
tion (UMAP) is then used to demonstrate this metric in
manifold learning resulting in a large corpus of music rep-
resented on a ""music map"" by melodic content. This mu-
sic map, dubbed ""Every Interval at Once"", visualizes
and sonifies the spectrum of data available, thus blurring
the lines between music data visualization and data-driven
sound art.In addition, the MIDI dataset collected for this project,
spanning 23,059 songs across 75 unique genres, is released
for public use.",CW,1,1,https://drive.google.com/file/d/1P69DGziJlGmpOgWGF6c-PBcMBZTQ7h-G/view?usp=drive_link,1,https://drive.google.com/file/d/1cb-As6_pVMicgKaChL8bWsUD6gUdZ-Gv/view?usp=drive_link,1,https://drive.google.com/file/d/1Vn9lcazpIyHmwaLrQw0LN98-jOt7t-f1/view?usp=drive_link,0,https://drive.google.com/file/d/1eDJmTZgRkBEq2Jwzs5owUMWlju0Mn8eb/view?usp=drive_link,1.0,https://drive.google.com/file/d/1rxbs8ipunHDzkMCBUgwwnRipg0MiJp4v/view?usp=drive_link,1,https://drive.google.com/file/d/1-gLX6jr16m176qzzZXlUhOIiNC67R7RI/view?usp=drive_link,1,poster is landscape....but the presenter is virtual... so no problem,https://ismir2024.slack.com/archives/C080DFSCDQR,lbd-495-interval-mover-s
496,Towards Robust Transcription: Exploring Noise Injection Strategies for Training Data Augmentation,Yonghyun Kim (Georgia Institute of Technology)*; Alexander Lerch (Georgia Institute of Technology),"Recent advancements in Automatic Piano Transcription (APT) have significantly improved system performance, but the impact of noisy environments on the system performance remains largely unexplored. This study investigates the impact of white noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art APT models and evaluates the performance of the Onsets and Frames model when trained on noise-augmented data. We hope this research provides valuable insights as preliminary work toward developing transcription models that maintain consistent performance across a range of acoustic conditions.",CW,1,1,https://drive.google.com/file/d/1aSexYfacd5b-f5wHzqRdgK9brbb1zglx/view?usp=drive_link,1,https://drive.google.com/file/d/1WsCfHqi4TkTj1Zjv-gJU3R61MbrLZQo-/view?usp=drive_link,1,https://drive.google.com/file/d/1RC6eJQgNE9nnn9NsuXQQJLNWsjfFAnbj/view?usp=drive_link,1,https://drive.google.com/file/d/17f_nH6HkSKCRJA70PyJ2B7-R9juCQXvK/view?usp=drive_link,1.0,https://drive.google.com/file/d/1_wgbgPzozI6HG16lvrmoDyZpq5JaBWn_/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800R1G84S,lbd-496-towards-robust-transcription
497,"HARP 2.0: Expanding Hosted, Asynchronous, Remote Processing for Deep Learning in the DAW",Christodoulos Benetatos (University of Rochester); Frank Cwitkowitz (University of Rochester); Nathan Pruyne (Northwestern University); Hugo Flores García (Northwestern University); Patrick O'Reilly (Northwestern University)*; Zhiyao Duan (Unversity of Rochester); Bryan Pardo (Northwestern University),"HARP 2.0 brings deep learning models to digital audio workstation (DAW) software through hosted, asynchronous, remote processing, allowing users to route audio from a plug-in interface through any compatible Gradio endpoint to perform arbitrary transformations. HARP renders endpoint-defined controls and processed audio in-plugin, meaning users can explore a variety of cutting-edge deep learning models without ever leaving the DAW. In the 2.0 release we introduce support for MIDI-based models and audio/MIDI labeling models, provide a streamlined pyharp Python API for model developers, and implement numerous interface and stability improvements. Through this work, we hope to bridge the gap between model developers and creatives, improving access to deep learning models by seamlessly intrgrating them into DAW workflows.",CW,1,1,https://drive.google.com/file/d/1R7SaIBcpzQyzAz5crMVYfcYji2Hm3OxR/view?usp=drive_link,1,https://drive.google.com/file/d/1zTeocgH_ADCjMr7TtLxgvt_zFX5QouD3/view?usp=drive_link,1,https://drive.google.com/file/d/1bag_-5osgxjUUGduspDOSFEcH0Nnxj0K/view?usp=drive_link,1,https://drive.google.com/file/d/1TcRdar3TXokcV4rTRC9XdjVJkM5iLN2S/view?usp=drive_link,1.0,https://drive.google.com/file/d/1vmZ1avUjUrpbOPENb6Ebo0biFvakVMSN/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07V098F0KZ,lbd-497-harp-2-0
498,"Zero-shot Crate Digging: DJ Tool retrieval using Speech Activity, Music Structure and CLAP embeddings",Iroro Orife (Netflix)*,"In genres like Hip-Hop, RnB, Reggae, Dancehall and just about every Electronic/Dance/Club style, DJ tools are a special set of audio files curated to heighten the DJ's musical performance and creative mixing choices. In this work we demonstrate an approach to discovering DJ tools in personal music collections. Leveraging open-source libraries for speech/music activity, music boundary analysis and a Contrastive Language-Audio Pretraining (CLAP) model for zero-shot audio classification, we demonstrate a novel system designed to retrieve (or rediscover) compelling DJ tools for use live or in the studio.",CN,1,1,https://drive.google.com/file/d/11Cxs8IIUnsG0UtGTxhqG6mNjulxln9lp/view,1,https://drive.google.com/file/d/14yU-TGYS4KkIoHnQUWCg8Tu9Dtr6T7b1/view,1,https://drive.google.com/file/d/1eYFp7tY0Uy_OVfgukuAAriY79nrFDnAn/view,1,https://drive.google.com/file/d/1PUY_7pTlbFXE28LtOVn8lmyzPkchL0pU/view,1.0,https://drive.google.com/file/d/1TL-dgfy2bBIh_HxyPKw6_Vgkhe9wM3BQ/view,0,,1,,https://ismir2024.slack.com/archives/C07V86X2ETG,lbd-498-zero-shot-crate
499,The Voice of an Instrument: Analysis of X-vectors for Music Emotion Recognition,Mariana Rodríguez Castañeda (UNAM); Iran R Roman (Stanford University)*,"This work explores the feature of x-vector to analyze the extent to which it embeds musical content by recreating the experiment in the work 'You’re Not You When You’re Angry: Robust Emotion Features Emerge by Recognizing Speakers' of emotion modulation in natural speech. As actors, we take instruments playing with four emotions; we use an autoencoder to observe changes in the reconstruction error, which allows us to find a similarity between the emotional modulation that occurred in speech in instruments.",CW,1,1,https://drive.google.com/file/d/18OeeUgaUx8jNavkbAsWW_XJqJ0a3Lul1/view?usp=drive_link,1,https://drive.google.com/file/d/1Itj1l2UImzSlAlfL1D2K4DwESodz9Jay/view?usp=drive_link,1,https://drive.google.com/file/d/135kqeDTgCQAHpNt5_qyg8HHP9L1dz0hs/view?usp=drive_link,1,https://drive.google.com/file/d/1IXUKmu6-aVapv-JNr14RqtovIRSrK1Ah/view?usp=drive_link,1.0,https://drive.google.com/file/d/115fixi3GEtLEbECsAdrGhpS2uKUG-OI6/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800R22GBU,lbd-499-the-voice-of
500,Perception of ragas is influenced by enculturation and musical training - a pilot study,"Vidya Rangasayee (Stanford University)*; Prahlad Saravanapriyan (Washington High School); Takako Fujioka (Department of Music, Center for Computer Research in Music and Acoustics, Stanford University, Stanford, CA, USA)","Music perception is shaped by both enculturation and musical training. This pilot study explores how musicians from different cultural and training backgrounds perceive music. Participants rated the perceived ""Westernness"" of raga samples in an online survey. Our analysis revealed significant differences in ratings between Western and Indian musicians, with a strong correlation between the perceived Westernness of the raga samples and their pitch class distributions. These findings suggest that musical training and cultural background influence familiarity with specific pitch class structures, leading to variations in music perception. Ongoing research will investigate the neural correlates of music perception across diverse musical cultures.
",CN,1,1,https://drive.google.com/file/d/1y_ywiyikAb0Ok5RfxMDCiCCwM-pi-zUy/view?usp=drive_link,1,https://drive.google.com/file/d/16AvAvBxPurGfZquw8egMdnPWEfD2ms0T/view?usp=drive_link,1,https://drive.google.com/file/d/15uUS-xC30rRtlClIqvvR44qtk-t9KQWQ/view?usp=drive_link,1,https://drive.google.com/file/d/1wCSTVV2JmlUJodczm9jTvJYlHr3LVLXU/view?usp=drive_link,1.0,https://drive.google.com/file/d/1WGLvcSNMJs6dwWhfvfsz1TvbS8RNyTeF/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C07VC02TH0V,lbd-500-perception-of-ragas
501,Emotion-based Piano Score Generation via Two-stage Transformer VAE,Jiahao Zhao (Kyoto University)*; Kazuyoshi Yoshii (Kyoto University),"With the advancement of generative models, controllable music generation (e.g., emotion-based) has gradually become feasible. However, achieving a balance between controllability and musicality/generative quality remains a challenging task in current research. Existing latent space-based methods, such as MusER, have struggled to capture the complex patterns underlying emotional expression and music theory. In this late-breaking demo, we present a preliminary study on emotion-based piano score generation. Our approach aims to enhance the controllability and accuracy of emotional content by disentangling musical elements. Additionally, we introduce a two-stage generation structure with pre-training to address the scarcity of emotion-labeled datasets, thereby improving the quality of the generated content and the robustness of the generation process.",CN,1,1,https://drive.google.com/file/d/1hWK-oBkhBOLtixHnD1hDMyAtO6Aov0rF/view,1,https://drive.google.com/file/d/1ciXOueg2BIR4eo9AnEEWx2U0s4yb9OKv/view,1,https://drive.google.com/file/d/1tXvmKKdqoofsPThIna8B2nRljqctGxly/view,1,https://drive.google.com/file/d/1oaSWiS2fqlcsaAm4KqKcc-2qjZIrbAlr/view,1.0,https://drive.google.com/file/d/192nH4fjbYJwSbYHHtZN53TW5uaSLL7AU/view,0,,1,,https://ismir2024.slack.com/archives/C07V86XCZN2,lbd-501-emotion-based-piano
502,Self-Supervised Multi-View Learning for Disentangled Music Audio Representations,Julia Wilkins (New York University)*; Sivan Ding (NYU); Magdalena Fuentes (New York University); Juan P Bello (New York University),"Self-supervised learning (SSL) offers a powerful way to learn robust, generalizable representations without labeled data. In music, where labeled data is scarce, existing SSL methods typically use generated supervision and multi-view redundancy to create pretext tasks. However, these approaches often produce entangled representations and lose view-specific information. We propose a novel self-supervised multi-view learning framework for audio designed to incentivize separation between private and shared representation spaces. A case study of results on audio disentanglement in a controlled setting demonstrates the effectiveness of our method.",CN,1,1,https://drive.google.com/file/d/1aeuTeYshjMy-0-ZlXW9POPLyZ1Q69o5D/view?usp=drive_link,1,https://drive.google.com/file/d/1KYaEUEH1kSoR2mbXaUjIpzLmccGi0lNA/view,1,https://drive.google.com/file/d/1vJl0vT_t-LY0R_DF-zAdXhJQYEo1d_hP/view,1,https://drive.google.com/file/d/179jo8HRA6RYK4MnuBsP6C3qS7k8p14N-/view,0.0,https://drive.google.com/file/d/1whATCmVvemt12xxHozhVNFVlOfVev-jX/view,0,,1,,https://ismir2024.slack.com/archives/C07V86XK0F8,lbd-502-self-supervised-multi
503,Towards Computational Analysis of Pansori Singing,Sangheon Park (Georgia Institute of Technology)*; Danbinaerin Han (KAIST); Dasaem Jeong (Sogang University),"Pansori is one of the most representative vocal genres
of Korean traditional music, which has an elaborated vocal melody line with strong vibrato. Although the music
is transmitted orally without any music notation, transcribing pansori music in Western staff notation has been introduced for several purposes, such as documentation of
music, education, or research. In this paper, we introduce
computational analysis of pansori based on both audio and
corresponding transcription, how modern Music Information Retrieval tasks can be used in analyzing traditional
music and how it revealed different audio characteristics
of what pansori contains.",CN,1,1,https://drive.google.com/file/d/1rT0G9Fq-IIlMaqYoxeFQT6HoWAAOuTG3/view,1,https://drive.google.com/file/d/1Nzu8SE9gVgNEez0fGSi0UzwCBVGFeSsz/view,1,https://drive.google.com/file/d/1vTSoQ9v79htifh72De5gb5c9ojg7wz0U/view,1,https://drive.google.com/file/d/1ArDfju4xuaAb4v5lJ0tJOxTL1GuB0Kyc/view,1.0,https://drive.google.com/file/d/1_ipvmThqBnVQFZGXahotm4_vNS3PWMqW/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C080DFTMUD7,lbd-503-towards-computational-analysis
504,Automatic Album Sequencing,"Vincent Herrmann (IDSIA/USI/SUPSI)*; Dylan R Ashley (The Swiss AI Lab IDSIA, USI, SUPSI); Jürgen Schmidhuber (IDSIA - Lugano)","Album sequencing has long been a critical part of the album production process. Recently, a data-driven approach was proposed by Ashley and Herrmann et al. (2024) for sequencing general collections of independent media by extracting the narrative essence of the items in the collection. While their approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of the work of Ashley and Herrmann et al., we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this—alongside a full copy of our implementation—is publicly available at REDACTED",CN,1,1,https://drive.google.com/file/d/16rmuNDIR0V8TJGVEyZdShUL9ugreq4hq/view,1,https://drive.google.com/file/d/1LVURXADHlAsJuxNT5_tTBhPe0ihn8vvA/view,1,https://drive.google.com/file/d/1Hireo2-3HeQHlGeM17tyNbi4TMVq5NoR/view,1,https://drive.google.com/file/d/1zIbVoBWa6WkNjhk8roNDxRXHsNDGzTxG/view,1.0,https://drive.google.com/file/d/1WGEv2fymDinOlGXCfnlrIhIG7qnDJ0pe/view,0,,1,,https://ismir2024.slack.com/archives/C080DFTUFHP,lbd-504-automatic-album-sequencing
505,Do Captioning Metrics Reflect Music Semantic Alignment?,Jinwoo Lee (Huawei Tech.)*; Kyogu Lee (Seoul National University),"Music captioning has emerged as a promising task, fueled by the advent of advanced language generation models. However, the evaluation of music captioning relies heavily on traditional metrics such as BLEU, METEOR, and ROUGE which were developed for other domains, without proper justification for their use in this new field. We present cases where traditional metrics are vulnerable to syntactic changes, and show they do not correlate well with human judgments. By addressing these issues, we aim to emphasize the need for a critical reevaluation of how music captions are assessed.",CN,1,1,https://drive.google.com/file/d/19XGhIXS3RpD86rxExymTWZAWJyB7n_jj/view,1,https://drive.google.com/file/d/1o87GLdeMC8j4-J_cxjQW2F-WbHMElz9m/view,1,https://drive.google.com/file/d/1GIccgsbioqrRfTHjFMw65ELWe2L2bzW7/view,1,https://drive.google.com/file/d/1kxCu2GaSpeOj47_LiN0lWCJ8dDnLSRsf/view,1.0,https://drive.google.com/file/d/1CMR5_34krYyaiEwDCfYYBdAmfGvQOEhM/view,0,,1,,https://ismir2024.slack.com/archives/C0803AN66GL,lbd-505-do-captioning-metrics
506,Boundary Regression for Leitmotif Detection in Music Audio,Sihun Lee (Sogang University)*; Dasaem Jeong (Sogang University),"Leitmotifs are musical phrases that reprise in various forms throughout a piece. Detecting the occurrence of leitmotifs from audio recordings is a highly challenging task, due to diverse variations and instrumentation. Leitmotif detection can be regarded as a subcategory of audio event detection, where the appearance of leitmotifs is predicted at the frame level. However, as leitmotifs carry distinct temporal structures and musical coherence, a more holistic approach akin to bounding box regression in visual object detection can be helpful. This would capture the entirety of the motif rather than fragmenting it into individual frames, thereby preserving its musical integrity and enhancing detection accuracy. We present our results on tackling leitmotif detection as a boundary regression task.",CN,1,1,https://drive.google.com/file/d/1bU62ZNRH9kZ30G0GVt3SKLSNzkE9K5jc/view?usp=drive_link,1,https://drive.google.com/file/d/1GQ0H1X4-cgq6K6a3npZFKJFlZDdMq5fd/view?usp=drive_link,1,https://drive.google.com/file/d/1oR33QYNhYZJslmRNR-DlkV-jQwJXQfFV/view?usp=drive_link,1,https://drive.google.com/file/d/1DlxEBkWsL8c5Nuufn9GYAMl6LfcZJ6-S/view?usp=drive_link,1.0,https://drive.google.com/file/d/1hL87cZgwfDgo_0qrwOmbL2qOHvBln3_a/view?usp=drive_link,0,,1,,https://ismir2024.slack.com/archives/C0800R37TJ6,lbd-506-boundary-regression-for
507,Score Reduction for Guitar Through Reinforcement Learning,Christodoulos Benetatos (University of Rochester)*; Zhiyao Duan (Unversity of Rochester),"We are interested in score reduction, specifically the task of adapting musical scores originally composed for another instrument (e.g. piano) into scores playable on the guitar. This process, traditionally performed by experienced guitarists, is complex and time-consuming. This study proposes to automate this task by framing score reduction as a combinatorial optimization problem under constraints and use Reinforcement Learning (RL) to solve it. The RL agent sequentially evaluates notes in the input score, deciding whether to keep or discard each note and selecting its position on the guitar fretboard. We use a graph representation for scores and utilize a transformer encoder to capture the state. The guitar's physical characteristics and the need to retain the musicality of the original score present challenging constraints. We design two reward functions to balance the trade-off between musicality and playability and train the agent using Proximal Policy Optimization (PPO).",CN,1,1,https://drive.google.com/file/d/10aESFmx6hjW810w637WQwptQze3ove1P/view,1,https://drive.google.com/file/d/1B6ba99qlae5IofApvfjfOSRQ-smQdcOl/view,1,https://drive.google.com/file/d/11a4cq5KqFpl7xVAJpzJfGvUnwQtuXwpM/view,1,https://drive.google.com/file/d/1hFGNR5_urs3xDsLEzhI23oXqgJiFZf-E/view,1.0,https://drive.google.com/file/d/1jLM3RKNRVDGIfmU2-oLCVFntjIY-DVVy/view,0,,1,,https://ismir2024.slack.com/archives/C0800R3HRJN,lbd-507-score-reduction-for
508,MIRFLEX: Music Information Retrieval Feature Library for Extraction,Anuradha Chopra (Singapore University of Technology and Design)*; Abhinaba Roy (SUTD); Dorien Herremans (Singapore University of Technology and Design),"This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.",CN,1,1,https://drive.google.com/file/d/1w06odNTa2ux3lenO4XyHYcPZA0n2kPV2/view,1,https://drive.google.com/file/d/14inEgdotibJQkUmjjDMsaHEhonFAbZoq/view,1,https://drive.google.com/file/d/1OclIu7JBipt00y38B5IoBFFiqv6tshno/view,1,https://drive.google.com/file/d/11TIKVBXUEHlhNYo9JY-R_lW2xIBlsVYp/view,1.0,https://drive.google.com/file/d/1SguuMvEAd8GtTTC3bIOnX5z8k6w-5KEv/view,0,,1,,https://ismir2024.slack.com/archives/C07VC04DG7P,lbd-508-mirflex-music-information
509,REFFLY: MELODY-CONSTRAINED LYRICS EDITING MODEL,"Songyan Zhao (University of California, Los Angeles)*",,CN,1,1,https://drive.google.com/file/d/1RTR39hNb-RXLEWeSlWyo730scymleuMN/view,1,https://drive.google.com/file/d/1AxJXBsHaJNJNoZJ6QicNgZyTGgupD_dX/view,1,https://drive.google.com/file/d/1FOzLhRu3kkwZfwv494WZ6aJzJTkuVS9g/view,1,https://drive.google.com/file/d/14m8PHoLJ_Nn8HN8nWp9Y-ffDOjEEzvTL/view,1.0,https://drive.google.com/file/d/1NI9Nil50alAUmg19VbiqyHYj__GiGFTR/view,0,,1,,https://ismir2024.slack.com/archives/C080PK60M5W,lbd-509-reffly-melody-constrained
