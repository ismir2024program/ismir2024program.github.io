uid,title,day,start_date,start_time,end_time,category,description,organiser,organiser_emails,organiser_affiliation,organiser_bio,image,web_link,slack_channel,channel_url,Zoom link
1,Registration,1,2024-11-10,8:00,9:00,Registration,Register!,,,,,,,,,
2,Online Q&A w/ volunteers,1,2024-11-10,8:00,9:00,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,
3,Morning Tutorial T1,1,2024-11-10,9:00,12:30,Tutorials,"Language serves as an efficient interface for communication between humans as well as between humans and machines. Through the integration of recent advancements in deep learning-based language models, the understanding, search, and creation of music is becoming capable of catering to user preferences with better diversity and control. This tutorial will start with an introduction to how machines understand natural language, alongside recent advancements in language models, and their application across various domains. We will then shift our focus to MIR tasks that incorporate these cutting-edge language models. The core of our discussion will be segmented into three pivotal themes: music understanding through audio annotation and beyond, text-to-music retrieval for music search, and text-to-music generation to craft novel sounds. In parallel, we aim to establish a solid foundation for the emergent field of music-language research, and encourage participation from new researchers by offering comprehensive access to 1) relevant datasets, 2) evaluation methods, and 3) coding best practices.","Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim and Ke Chen",,,,,,t1-connecting-music-audio,https://ismir2024.slack.com/archives/C07USUU43NF,https://us02web.zoom.us/j/87844902244?pwd=K6qQ3aCfsUKaNalKwZZKJ07wnRpNOa.1
4,Morning Tutorial T2,1,2024-11-10,9:00,12:30,Tutorials,"This tutorial reflects on the journey of Music Information Retrieval (MIR) over the last 25 years, offering insights from three distinct perspectives: research, community, and education. Drawing from the presenters' personal experiences and reflections, it provides a holistic view of MIR's evolution, covering historical milestones, community dynamics, and pedagogical insights. Through this approach, the tutorial aims to give attendees a nuanced understanding of MIR’s past, present, and future directions, fostering a deeper appreciation for the field and its interdisciplinary and educational aspects.
 

 The tutorial is structured into three parts, each based on one of the aforementioned perspectives. The first part delves into the research journey of MIR. It covers the inception of query-by-humming and the emergence of MP3s, discusses the establishment of standard tasks such as beat tracking and genre classification, and highlights significant advancements, applications, and future challenges in the field. The second part explores the community aspect of ISMIR. It traces the growth of the society from a small symposium to a well-recognized international community, emphasizing core values such as interdisciplinary collaboration and diversity, and invites the audience to imagine the future of the ISMIR community together. Lastly, the third part discusses the role of music as an educational domain. It examines the broad implications of MIR research, the value of pursuing a PhD in MIR, and the significant educational resources available.
 

 Each part invites audience interaction, aiming to provide attendees with a deeper appreciation of MIR's past achievements and insights into its potential future directions. This tutorial is not just a historical overview but also a platform for fostering a deeper understanding of the interplay between technology and music.","Masataka Goto, Jin Ha Lee, and Meinard Müller",,,,,,t2-exploring-25-years,https://ismir2024.slack.com/archives/C07UW844R7U,https://us02web.zoom.us/j/82635337236?pwd=hZdTOdR5SaXa3tES9kbfYkZPnUM4sn.1
5,Morning Tutorial T3,1,2024-11-10,9:00,12:30,Tutorials,"This tutorial will cover the theory and practice of diffusion models for music and sound. We will explain the methodology, explore its history, and demonstrate music and sound-specific applications such as real-time generation and various other downstream tasks. By bridging the gap from computer vision techniques and models, we aim to spark further research interest and democratize access to diffusion models for the music and sound domains. 
 

 The tutorial comprises four sections. The first provides an overview of deep generative models and delves into the fundamentals of diffusion models. The second section explores applications such as sound and music generation, as well as utilizing pre-trained models for music/sound editing and restoration. In the third section, a hands-on demonstration will focus on training diffusion models and applying pre-trained models for music/sound restoration. The final section outlines future research directions.
 

 We anticipate that this tutorial, emphasizing both the foundational principles and practical implementation of diffusion models, will stimulate interest among the music and sound signal processing community. It aims to illuminate insights and applications concerning diffusion models, drawn from methodologies in computer vision.","Chieh-Hsin Lai, Koichi Saito, Bac Nguyen Cong, Yuki Mitsufuji, and Stefano Ermon",,,,,,t3-from-white-noise,https://ismir2024.slack.com/archives/C07UPKD2SS2,https://us02web.zoom.us/j/82145705825?pwd=BgRaFJb9Zabtlqr8ylgUPNIikQkLcl.1
7,Online Q&A w/ volunteers,1,2024-11-10,13:00,14:00,Meetup,,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
8,Afternoon Tutorial T4,1,2024-11-10,14:00,17:30,Tutorials,"In one form or another, most MIR research depends on the judgment of humans. Humans provide our ground-truth data, whether through explicit annotation or through observable behavior (e.g., listening histories); Humans also evaluate our results, whether in academic research reports or in the commercial marketplace. Will users like it? Will customers buy it? Does it sound good? These are all critical questions for MIR researchers which can only be answered by asking people. Unfortunately, measuring and interpreting the judgments and experiences of humans in a rigorous manner is difficult. Human responses can be fickle, changeable, and inconsistent—they are, by definition, subjective. There are many factors that influence human responses, some of which can be controlled or accounted for in experimental design, and others which must be tolerated but ameliorated through statistical analysis. Fortunately, researchers in the field of behavioral psychology have amassed extensive expertise and institutional knowledge related to the practice and pedagogy of human-subject research, but MIR researchers receive little exposure to research methods involving human subjects. This tutorial, led by MIR researchers with training (and publications) in psychological research, aims to share these insights with the ISMIR community. The tutorial will introduce key concepts, terminology, and concerns in carrying out human-subject research, all in the context of MIR. Through the discussion of real and hypothetical human research, we will explore the nuances of experiment and survey design, stimuli creation, sampling, psychometric modeling, and statistical analysis. We will review common pitfalls and confounds in human research, and present guidelines for best practices in the field. We will also cover fundamental ethical and legal requirements of human research. Any and all ISMIR members are welcome and encouraged to attend: it is never too early, or too late, in one’s research career to learn (or practice) these essential skills.","Claire Arthur, Nat Condit-Schultz, David R. W. Sears, John Ashley Burgoyne, and Josuha Albrecht",,,,,,t4-humans-at-the,https://ismir2024.slack.com/archives/C07V8U99MMX,https://us02web.zoom.us/j/83619614732?pwd=zG13FTM11QpdnTmGM6D6rA4IIfChs0.1
9,Afternoon Tutorial T5,1,2024-11-10,14:00,17:30,Tutorials,"Audio-based MIR (MIR based on the processing of audio signals) covers a broad range of tasks, including analysis (pitch, chord, beats, tagging), similarity/cover identification, and processing/generation of samples or music fragments. A wide range of techniques can be employed for solving each of these tasks, spanning from conventional signal processing and machine learning algorithms to the whole zoo of deep learning techniques.
 

 This tutorial aims to review the various elements of this deep learning zoo commonly applied in Audio-based MIR tasks. We review typical audio front-ends (such as waveform, Log-Mel-Spectrogram, HCQT, SincNet, LEAF, quantization using VQ-VAE, RVQ), as well as projections (including 1D-Conv, 2D-Conv, Dilated-Conv, TCN, WaveNet, RNN, Transformer, Conformer, U-Net, VAE), and examine the various training paradigms (such as supervised, self-supervised, metric-learning, adversarial, encoder-decoder, diffusion). Rather than providing an exhaustive list of all of these elements, we illustrate their use within a subset of (commonly studied) Audio-based MIR tasks such as multi-pitch/chord-estimation, cover-detection, auto-tagging, source separation, music-translation or music generation. This subset of Audio-based MIR tasks is designed to encompass a wide range of deep learning elements. For each tack we address a) the goal of the tasks, b) how it is evaluated, c) provide some popular datasets to train a system, and d) explain (using slides and pytorch code) how we can solve it using deep learning.
 

 The objective is to provide a 101 lecture (introductory lecture) on deep learning techniques for Audio-based MIR. It does not aim at being exhaustive in terms of Audio-based MIR tasks nor on deep learning techniques but to provide an overview for newcomers to Audio-Based MIR on how to solve the most common tasks using deep learning. It will provide a portfolio of codes (Colab notebooks and Jupyter book) to help newcomers achieve the various Audio-based MIR Tasks.","Geoffroy Peeters, Gabriel Meseguer Brocal, Alain Riou, and Stefan Lattner",,,,,,t5-deep-learning-101,https://ismir2024.slack.com/archives/C07UFMQUV7H,https://us02web.zoom.us/j/87836725996?pwd=s1D2bvzrbPplz8ZrYVQSkY1Gqg7ENn.1
10,Afternoon Tutorial T6,1,2024-11-10,14:00,17:30,Tutorials,"Singing, a universal human practice, intertwines with lyrics to form a core part of profound musical experiences, conveying emotions, narratives, and real-world connections. This tutorial explores the commonly used techniques and practices in lyrics and singing voice processing, which are vital in numerous music information retrieval tasks and applications.

Despite the importance of song lyrics in MIR and the industry, high-quality paired audio & transcript annotations are often scarce. In the first part of this tutorial, we'll delve into automatic lyrics transcription and alignment techniques, which significantly reduce the annotation cost and enable more performant solutions. Our tutorial provides insights into the current state-of-the-art methods for transcription and alignment, highlighting their capabilities and limitations while fostering further research into these systems.

Moreover, we present ""lyrics information processing"", which encompasses lyrics generation and leveraging lyrics to discern musically relevant aspects such as emotions, themes, and song structure. Understanding the rich information embedded in lyrics opens avenues for enhancing audio-based tasks by incorporating lyrics as supplementary input. 

Finally, we discuss singing voice conversion as one such task, which involves the conversion of acoustic features embedded in a vocal signal, often relating to timbre and pitch. We explore how lyric-based features can facilitate a model's inherent disentanglement between acoustic and linguistic content, which leads to more convincing conversions. This section closes with a brief discussion on the ethical concerns and responsibilities that should be considered in this area.

This tutorial caters especially to new researchers with an interest in lyrics and singing voice modeling, or those involved in improving lyrics alignment and transcription methodologies. It can also inspire researchers to leverage lyrics for improved performance on tasks like singing voice separation, music and singing voice generation, and cover song and emotion recognition.
","Daniel Stoller, Emir Demirel, Kento Watanabe, and Brendan O’Connor",,,,,,t6-lyrics-and-singing,https://ismir2024.slack.com/archives/C07UW5HJH7D,https://us02web.zoom.us/j/84459641676?pwd=ZAwXfgHHP9bh9eWa1uEcGYyZ2sOaG2.1
11,Welcome Reception,1,2024-11-10,19:00,21:00,Social,"Join us for a warm and lively welcome reception to reconnect with old friends and meet new colleagues! Enjoy drinks, appetizers, and an incredible live band (Camilo y los Cruzers) as we celebrate our community and look forward to an exciting conference ahead. Don’t miss this fantastic opportunity to connect and unwind!",,,,,,,,,
12,Online Q&A w/ volunteers,1,2024-11-10,20:00,21:00,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
13,All Tutorials,1,2024-11-10,21:00,6:00,Tutorials,Replay of all tutorials.,,,,,,,,,
14,Online Q&A w/ volunteers,2,2024-11-11,1:00,2:00,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
15,Online social event,2,2024-11-11,6:00,7:30,Social,,,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
16,Registration,2,2024-11-11,8:00,8:30,Registration,,,,,,,,,,
17,Online Q&A w/ volunteers,2,2024-11-11,8:00,8:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
18,Opening Remarks,2,2024-11-11,8:30,9:00,Special Session,Join the General Chairs of ISMIR 2024 to kick off this year’s conference!,"Gautham Mysore, Oriol Nieto, Blair Kaneshiro",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
20,Poster Session - 1,2,2024-11-11,10:15,11:45,Poster session,"**In-person presentations:**

* [Formal Modeling of Structural Repetition using Tree Compression - Zeng Ren (École Polytechnique Fédérale de Lausanne)*, Yannis Rammos (EPFL), Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne)](/poster_207.html)
* [Saraga Audiovisual: a large multimodal open data collection for the analysis of Carnatic Music - Adithi Shankar (Music Technology Group- Universitat Pompeu Fabra)*, Genís Plaja-Roglans (Music Technology Group), Thomas Nuttall (Universitat Pompeu Fabra, Barcelona), Martín Rocamora (Universitat Pompeu Fabra), Xavier Serra (Universitat Pompeu Fabra )](/poster_352.html)
* [X-Cover: Better music version identification system by integrating pretrained ASR model - Xingjian Du (University of Rochester)*, Zou Pei (ByteDance), Mingyu Liu (ByteDance), Xia Liang (Bytedance), Huidong Liang (University of Oxford), Minghang Chu (Bytedance), Zijie Wang (ByteDance), Bilei Zhu (ByteDance AI Lab)](/poster_328.html)
* [FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs - Hitoshi Suda (National Institute of Advanced Industrial Science and Technology (AIST))*, Shunsuke Yoshida (The University of Tokyo), Tomohiko Nakamura (National Institute of Advanced Industrial Science and Technology (AIST)), Satoru Fukayama (National Institute of Advanced Industrial Science and Technology (AIST)), Jun Ogata (AIST)](/poster_38.html)
* [Classical Guitar Duet Separation using GuitarDuets - a Dataset of Real and Synthesized Guitar Recordings - Marios Glytsos (National Technical University of Athens)*, Christos Garoufis (Athena Research Center), Athanasia Zlatintsi (Athena Research Center), Petros Maragos (National Technical University of Athens)](/poster_262.html)
* [Can LLMs ""Reason"" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation - Ziya Zhou (HKUST)*, Yuhang Wu (Multimodal Art Projection), Zhiyue Wu (Shenzhen University), Xinyue Zhang (Multimodal Art Projection), Ruibin Yuan (CMU), Yinghao MA (Queen Mary University of London), Lu Wang (Shenzhen University), Emmanouil Benetos (Queen Mary University of London), Wei Xue (The Hong Kong University of Science and Technology), Yike Guo (Hong Kong University of Science and Technology)](/poster_31.html)
* [Music2Latent: Consistency Autoencoders for Latent Audio Compression - Marco Pasini (Queen Mary University of London)*, Stefan Lattner (Sony Computer Science Laboratories, Paris), George Fazekas (QMUL)](/poster_172.html)
* [Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models - Johannes Zeitler (International Audio Laboratories Erlangen)*, Ben Maman (Tel Aviv University), Meinard Müller (International Audio Laboratories Erlangen)](/poster_8.html)
* [Selective Annotation of Few Data for Beat Tracking of Latin American Music Using Rhythmic Features - Lucas S. Maia, Martín Rocamora, Luiz W. P. Biscainho, Magdalena Fuentes*](/poster_510.html)
* [Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval - Takayuki Nakatsuka (National Institute of Advanced Industrial Science and Technology (AIST))*, Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST)), Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))](/poster_155.html)
* [Towards Automated Personal Value Estimation in Song Lyrics - Andrew M. Demetriou (Delft University of Technology)*, Jaehun Kim (Pandora / SiriusXM), Cynthia Liem (Delft University of Technology)](/poster_335.html)
* [Audio Conditioning for Music Generation via Discrete Bottleneck Features - Simon Rouard (Meta AI Research)*, Alexandre Defossez (Kyutai), Yossi Adi (Facebook AI Research ), Jade Copet (Meta AI Research), Axel Roebel (IRCAM)](/poster_41.html)
* [Variation Transformer: New datasets, models, and comparative evaluation for symbolic music variation generation - Chenyu Gao (University of York)*, Federico Reuben (University of York), Tom Collins (University of York, MAIA, Inc.)](/poster_72.html)
* [Automatic Detection of Moral Values in Music Lyrics - Vjosa Preniqi (Queen Mary University of London)*, Iacopo Ghinassi (Queen Mary University of London), Julia Ive (Queen Mary University of London), Kyriaki Kalimeri (ISI Foundation), Charalampos Saitis (Queen Mary University of London)](/poster_326.html)
* [Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques - Sebastian Strahl (International Audio Laboratories Erlangen)*, Meinard Müller (International Audio Laboratories Erlangen)](/poster_214.html)
* [Note-Level Transcription of Choral Music - Huiran Yu (University of Rochester)*, Zhiyao Duan (University of Rochester)](/poster_364.html)
* [Learning Multifaceted Self-Similarity over Time and Frequency for Music Structure Analysis - Tsung-Ping Chen (Kyoto University)*, Kazuyoshi Yoshii (Kyoto University)](/poster_12.html)
* [A Contrastive Self-Supervised Learning scheme for beat tracking amenable to few-shot learning - Antonin Gagneré (LTCI - Télécom Paris, IP Paris)*, Slim Essid (  LTCI - Télécom Paris, IP Paris), Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)](/poster_283.html)
* [Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis - Morgan Buisson (Telecom-Paris)*, Brian McFee (New York University), Slim Essid (Telecom Paris - Institut Polytechnique de Paris)](/poster_405.html)

**Remote presentations:**

* [Harmonic and Transposition Constraints Arising from the Use of the Roland TR-808 Bass Drum - Emmanuel Deruty (Sony Computer Science Laboratories)*](/poster_86.html)",Claire Arthur,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
,,,,,,,,,,,,,,,,
22,Mindfulness session,2,2024-11-11,11:45,12:45,Meetup,"soundBrilliance is an innovative digital health company using enhanced music, psychology, and measurement techniques to create tools and exercises which empower people to better self-manage fundamental health – emotional balance, fitness, quality sleep and pain control. The experiences presented in the ISMIR 2024 Mindfulness sessions are designed to help guide you into a deeper sense of Calm. All visuals are naturally produced and captured, with no AI intervention.<br/><br/> ![SoundBrilliance](/static/images/soundbrilliance.svg ""SoundBrilliance"")",soundBrilliance,,,,,,,,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
23,Towards a fairer approach to generative AI training,2,2024-11-11,13:00,14:00,All Meeting,"Ed will discuss the issues that arise when generative AI companies scrape training data without consent, and the alternative - licensing training data - that is being embraced by many AI music companies.",Ed Newton-Rex,,,"Ed Newton-Rex is the founder of Fairly Trained, a non-profit that certifies generative AI companies for fair training data practices. He is also a Visiting Scholar at Stanford University.

In 2010, Ed founded Jukedeck, one of the first AI music generation startups. Jukedeck let video creators generate music for their videos, and was used to create more than a million pieces of music. It was acquired by ByteDance in 2019\. At ByteDance, Ed led the AI Music lab, then led Product for TikTok in Europe.

In 2022 Ed joined Stability AI, the company behind Stable Diffusion, to lead their Audio team. His team launched Stable Audio, Stability’s music generation product, which was named one of TIME Magazine’s best inventions of the year in 2023\. He resigned from Stability in November 2023 due to the company’s policy of training AI models on copyrighted work without consent, and in 2024 founded Fairly Trained. He is a published composer of choral music.",https://confcats-siteplex.s3.us-east-1.amazonaws.com/ismir24/large_ed_newton_rex_fairly_trained_ai_music_companies_copy_5c8e87ef06.jpg,,keynote-1-newton-rex,https://ismir2024.slack.com/archives/C0805NPRK8A,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
26,Poster Session - 2,2,2024-11-11,15:15,16:45,Poster session,"**In-person presentations:**

* [Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding - Danbinaerin Han (KAIST), Mark R H Gotham (Durham), DongMin Kim (Sogang University), Hannah Park (Sogang University), Sihun Lee (Sogang University), Dasaem Jeong (Sogang University)*](/poster_35.html)
* [Lessons learned from a project to encode Mensural music on a large scale with Optical Music Recognition - David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana)*, Jorge Calvo-Zaragoza (University of Alicante), Patricia García-Iasci (University of Alicante), Teresa Delgado-Sánchez (Biblioteca Nacional de España)](/poster_104.html)
* [The Changing Sound of Music: An Exploratory Corpus Study of Vocal Trends Over Time - Elena Georgieva (NYU)*, Pablo Ripollés (New York University), Brian McFee (New York University)](/poster_57.html)
* [Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context - Pedro Ramoneda (Universitat Pompeu Fabra)*, Martín Rocamora (Universidad de la República), Taketo Akama (Sony CSL)](/poster_77.html)
* [Notewise Evaluation of Source Separation: A Case Study For Separated Piano Tracks - Yigitcan Özer (International Audio Laboratories Erlangen)*, Hans-Ulrich Berendes (International Audio Laboratories Erlangen), Vlora Arifi-Müller (International Audio Laboratories Erlangen ), Fabian-Robert Stöter (AudioShake, Inc.), Meinard Müller (International Audio Laboratories Erlangen)](/poster_13.html)
* [Automatic Estimation of Singing Voice Musical Dynamics - Jyoti Narang (Student)*, Nazif Can Tamer (Universitat Pompeu Fabra), Viviana De La Vega (Escola Superior de Música de Catalunya), Xavier Serra (Universitat Pompeu Fabra )](/poster_296.html)
* [Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation - Or Tal (The Hebrew University of Jerusalem)*, Alon Ziv (The Hebrew University of Jerusalem), Felix Kreuk (Bar-Ilan University), Itai Gat (Meta), Yossi Adi (The Hebrew University of Jerusalem)](/poster_65.html)
* [DIFF-A-RIFF: MUSICAL ACCOMPANIMENT CO-CREATION VIA LATENT DIFFUSION MODELS - Javier Nistal (Sony CSL)*, Marco Pasini (Queen Mary University of London), Cyran Aouameur  (Sony CSL), Stefan Lattner (Sony Computer Science Laboratories, Paris), Maarten Grachten (Machine Learning Consultant)](/poster_225.html)
* [Exploring Internet Radio Across the Globe with the MIRAGE Online Dashboard - Ngan V.T. Nguyen (University of Science, Vietnam Nation University Ho Chi Minh City), Elizabeth A.M. Acosta (Texas Tech University), Tommy Dang (Texas Tech University), David R.W. Sears (Texas Tech University)*](/poster_334.html)
* [The Sound Demixing Challenge 2023 – Cinematic Demixing Track - Stefan Uhlich, Giorgio Fabbro*, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji](/poster_511.html)
* [MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling - Andrew C Edwards (QMUL)*, Xavier Riley (C4DM), Pedro Pereira Sarmento (Centre for Digital Music), Simon Dixon (Queen Mary University of London)](/poster_175.html)
* [Transcription-based lyrics embeddings: simple extraction of effective lyrics embeddings from audio - Jaehun Kim (Pandora / SiriusXM)*, Florian Henkel (SiriusXM + Pandora), Camilo Landau (Pandora / SiriusXM), Samuel E. Sandberg (SiriusXM + Pandora), Andreas F. Ehmann (SiriusXM + Pandora)](/poster_365.html)
* [A Method for MIDI Velocity Estimation for Piano Performance by a U-Net with Attention and FiLM - Hyon Kim (Universitat Pompeu Fabra)*, Xavier Serra (Universitat Pompeu Fabra )](/poster_42.html)
* [MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation - Yun-Han Lan (Taiwan AI Labs)*, Wen-Yi Hsiao (Taiwan AI Labs), Hao-Chung Cheng (National Taiwan University), Yi-Hsuan Yang (National Taiwan University)](/poster_91.html)
* [End-to-end Piano Performance-MIDI to Score Conversion with Transformers - Tim Beyer (Technical University of Munich)*, Angela Dai (Technical University of Munich)](/poster_158.html)
* [From Real to Cloned Singer Identification - Dorian Desblancs (Deezer Research)*, Gabriel Meseguer Brocal (Deezer), Romain Hennequin (Deezer Research), Manuel Moussallam (Deezer)](/poster_271.html)
* [Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation - Jingyue Huang (New York University)*, Ke Chen (University of California San Diego), Yi-Hsuan Yang (National Taiwan University)](/poster_376.html)
* [EFFICIENT ADAPTER TUNING FOR JOINT SINGING VOICE BEAT AND DOWNBEAT TRACKING WITH SELF-SUPERVISED LEARNING FEATURES - Jiajun Deng (The Chinese University of HongKong)*, Yaolong Ju (Huawei), Jing Yang (Huawei 2012 Labs), Simon Lui (Huawei), Xunying Liu (The Chinese University of Hong Kong)](/poster_48.html)
* [Which audio features can predict the dynamic musical emotions of both composers and listeners? - Eun Ji Oh (KAIST), Hyunjae Kim (KAIST), Kyung Myun Lee (KAIST)*](/poster_305.html)
* [Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model - Julia Barnett (Northwestern University)*, Bryan Pardo (Northwestern University), Hugo Flores García (Northwestern University)](/poster_81.html)
",Anna Kruspe,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
27,Listening For Diversity: The ways in which critical attention to words helps move us closer towards realizing our full humanity,2,2024-11-11,16:45,17:45,All Meeting,"Using her experience as a dancer, therapist, mediator, diversity trainer, anthropologist, college educator, and originator of Grounded Knowledge Panels®, Valerie Joseph distills lessons learned about the power of intentional and principled listening. She offers ideas on how to harness the energy derived from listening differently to fuel the capacity to have uncomfortable, rich, dynamic and productive thinking. This forms the basis upon which we are challenged to make transformative choices about how we operate with those other humans with whom we share the planet.",Valerie Joseph,,,"Valerie Joseph earned a Ph.D. in Cultural Anthropology from the University of Massachusetts at Amherst. Her doctoral research investigated the enduring legacies of British colonialism and African heritage memory among the members of the African Diaspora in Carriacou, Grenada. Specifically, she mapped how the game songs and dance play of Carriacouan Black girls as well as their words, beliefs, and attitudes reflected both the detrimental internalization of colonial ideology and the restorative nature of African retentions.

Prior to her fieldwork in Carriacou, Dr. Joseph lived and worked in Botswana for seven years starting as a Peace Corps Volunteer science teacher in a junior secondary school, then as a training coordinator at the Cheshire Foundation's Mogoditshane Rehabilitation Center. She closed out her years in the country by working as co-director of the School for International Training's college semester abroad program. During her time in Botswana, Dr. Joseph sharpened her interest in cross-cultural conflicts, including those that seemed to be intractable, though traceable, in part, to cultural mores as well as historical and social patterns embedded in racial or ethnic bias and discrimination.

Dr. Joseph has a Masters in Movement Therapy with a concentration in counseling psychology and a Masters in Social Justice Education. Her supplemental training, work and experience in several fields includes gymnastics coaching, dance performance, diversity training, Authentic Movement (a contemplative dance form), mediation, teaching and management in higher education.

Dr. Joseph is an educator-interventionist working at Smith College as the Mentoring Administrative Director for AEMES (Achieving Excellence in Math Engineering and Science). In that role, she manages programs to support the most marginalized students who are pursuing STEM. She also teaches college success seminars within the AEMES Scholars Program.

Dr. Joseph is co-founder of the Smith Roundtable Group. Started in 2020, the SRG is a small contingent of staff, faculty, and students dedicated to creating opportunities for information sharing and conversation about important current events. Past Roundtable offerings included: “Daring to be Hopeful: A Critical Response to the White Supremacist Storming of Our Capitol” and “Why is the Power of Young People so Threatening to the Status Quo?” The most recent Roundtable event took place in September of this year: ""'Calling In' for Democracy and Human Rights: A Consideration of Project 2025.""

In and outside of Smith, Dr. Joseph convenes a unique form of public discourse that she originated. Grounded Knowledge Panels® are public conversations by small groups of people who have realistic, authentic and personal experience and understanding of a particular topic or question. Emerging from core Black culture, Grounded Knowledge Panels are a synthesis of Dr. Joseph's study and work in various fields including anthropology, Authentic Movement, education and mediation. As panelists converse among themselves, audience members are invited as “witnesses” to observe the discussion. Both groups \- panelists and witnesses – bring a distinctive power, depth and responsibility to the experience of speaking and listening.

Dr. Joseph is a five time recipient of the Smith College Spotlight Award, an honor presented to staff members, chosen by peers, in appreciation of exceptional service. She is a 2020 recipient of the Elizabeth B. Wyant Gavel Award awarded by students to staff members who have performed outstanding work in the Smith community.

Dr. Joseph's first children's book, [This is What Maisie Believes](https://www.groundedspaceconsulting.com/books), is published by 619 Wreath Publishing.",https://confcats-siteplex.s3.us-east-1.amazonaws.com/ismir24/small_valerie_j_6510850013.png,,keynote-dei-joseph,https://ismir2024.slack.com/archives/C07VD4K0CJ2,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
28,Remembering Don Byrd,2,2024-11-11,17:45,18:00,Special Session,"Don Byrd, the General Chair of the very first ISMIR conference in 2000, has recently left us. This session remembers and celebrates the vision and contributions of this legend for our community. ",Zhiyao Duan,,,,,,remembering-don-byrd,https://ismir2024.slack.com/archives/C07VARAKF5J,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
29,Online special session I,2,2024-11-11,18:00,19:00,Special Session,"Join us for short presentations and informal conversations with invited researchers from the MIR community\!

**Amanda Krause**

Title: Can we categorise younger adult listeners?

Abstract: The evolution of digital listening technologies continues to impact the way we think about music consumption and music listening practices. Krause and North’s (2016) findings suggest that, in addition to demographic characteristics, psychological constructs should be considered when investigating listening practices and technology use. The present study uses latent profile analysis (LPA), which is a statistical technique that focuses on identifying latent subgroups within a population based on a set of variables. With this study, LPA affords us the opportunity to attempt to categorise types of music listeners. To explore this possibility, we draw on data collected from a sample of 584 younger adults residing in Australia (Mage \= 19.62; 74.10% female). Participants were asked to complete an online questionnaire that included demographics, the musicianship module of the MUSEBAQ (Chin, et al., 2018), the Music Engagement Test (MET; Greenberg & Rentfrow, 2015), Langford’s (2003) Big Five proxy personality scale, Krause and Hargreave’s (2013) Music Self-Images Questionnaire, and Krause and Brown’s (2021) format use measure. With analyses underway, preliminary indications suggest that format use and MET scores may differentiate listener typologies. Study findings further our theoretical understanding of how individuals consume music in everyday life.


**Sebastian Stober**

Title: ""Generative AI Training and Copyright Law”

Abstract: Training generative AI models requires extensive amounts of data. A common practice is to collect such data through web scraping. In the USA, AI developers rely on ""fair use"" and in Europe, the prevailing view is that the exception for ""Text and Data Mining"" (TDM) applies. In a recent interdisciplinary tandem-study with a legal expert, I have argued in detail that this is actually not the case because generative AI training fundamentally differs from TDM. In this talk, I will share our main findings and the implications for both public and corporate research on generative models. I will further discuss how the phenomenon of training data memorization leads to copyright issues independently from the ""fair use"" and TDM exceptions. Finally, I would like to outline how the ISMIR could contribute to the ongoing discussion about fair practices with respect to generative AI that satisfy all stakeholders.","Amanda Krause, Sebastian Stober",,,"**Dr Amanda E. Krause** is a Senior Lecturer (Psychology) in the College of Healthcare Sciences at James Cook University (Queensland, Australia). As a music psychology scholar based at James Cook University, she studies how we experience music in our everyday lives.
Her passion for researching the social and applied psychology of music has led her to give guest lectures and public talks and serve as President of the Australian Music & Psychology Society (AMPS). She is the author of numerous peer-reviewed academic publications and has spoken on her research to academics and industry leaders at conferences around the world. Her research has made significant contributions to understanding how listening technologies influence people’s experiences and how musical engagement impacts well-being. Dr Krause’s current programs of research concern how everyday music and radio experiences influence people’s well-being.

**Sebastian Stober** is professor for Artificial Intelligence at the Otto-von-Guericke-University Magdeburg, Germany. He studied computer science with focus on intelligent systems in Magdeburg until 2005 and received his PhD with distinction on the topic of adaptive methods for user-centered organization of music collections in 2011\. From 2013 to 2015, he was postdoctoral fellow at the Brain and Mind Institute in London, Ontario where he pioneered deep learning techniques for studying brain activity during music perception and imagination. Afterwards, he was head of the Machine Learning in Cognitive Science Lab at the University of Potsdam, before returning to Magdeburg in 2018\. In his current research, he investigates and develops generative models for music and speech as well as methods to better understand what an artificial intelligence has learned and how it solves specific problems. To this end, he combines the fields of artificial intelligence and machine learning with cognitive neuroscience and music information retrieval.",,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
30,Banquet/Jam Session,2,2024-11-11,19:00,23:59,Social,"Get ready for an unforgettable night at the Regency Ballroom in the heart of San Francisco! Join us for a spectacular banquet featuring the incredible Will Baldes, a multi-instrumentalist jazz artist who will blow your mind with his talent. And don’t miss the infamous ISMIR Jam Session—where anything can happen and probably will!

Indulge in delicious food and drinks while mingling with fellow MIR researchers. We’ve got you covered with buses to whisk you away from the conference venue to the banquet, so you can focus on having a fantastic time. This is one night you won’t want to miss—bring your dancing shoes and your appetite for fun! 🎷🍷🍴🚌",,,,,,,,,
31,Online Q&A w/ volunteers,2,2024-11-11,20:00,20:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
40,Remembering Don Byrd,3,2024-11-12,5:45,6:00,time,,,,,,,,remembering-don-byrd,https://ismir2024.slack.com/archives/C07VARAKF5J,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
41,Online special session II,3,2024-11-12,6:00,7:00,Special Session,"Join us for short presentations and informal conversations with invited researchers from the MIR community\!

**Martin Hartmann**

Title: Music and Movement: exploring Social and Multimodal Dimensions of Rhythmic Entrainment.

Abstract: The talk addresses key challenges in the field of music and movement through two ongoing studies at the Centre of Excellence in Music, Mind, Body and Brain at the University of Jyväskylä. The first challenge explores rhythmic-social entrainment within the context of free dyadic dance. We present a study that examines the relationship between rhythmic-social entrainment and social as well as musical affiliation during adolescence, using markerless motion capture technology. Following a 2x2 factorial design, participants dance freely in dyads with a friend and with a stranger to music of their choice and to music selected by us. The second challenge focuses on the multimodality of rhythmic-social entrainment. We discuss a study that employs motion capture and surface electromyography to investigate the impact of visual cues and performed activities on acoustic features, physiological responses, and kinematic responses in choir singing. The goal is to understand how the visibility of other choir members and the performed activities (chat, homophony, polyphony, and musical improvisation) influence different types of individual and group responses. In addition to exploring the social aspects of rhythmic entrainment in dance and its multimodal nature in choir singing, we emphasize the extraction of musical features and individual and social acoustic and kinematic features. We also consider potential take-home messages from these studies for the music information retrieval community and beyond.




**Kathleen Rose Agres** 
Session II: 6 AM PT, 12th November, Online - ISMIR 2024

Title: Affective music generation for emotion regulation in listeners

Abstract: There has been a surge of interest in automatic music generation in recent years, particularly in affective music generation. Numerous systems now offer controllable AI-based affective music generation (AI-AMG), as highlighted in Dash & Agres (2024). While these systems have been developed for various applications—including soundtrack creation in gaming and virtual reality, co-creativity, and health and well-being—this talk focuses on the use of AI-AMG to support emotion regulation in listeners. One such system, AffectMachine (Agres, Dash, & Chua, 2023), is designed to generate affective music in real time, and is capable of composing in both classical and pop-music styles. Recent findings across several studies demonstrate AffectMachine’s efficacy in producing music perceived as emotional and capable of inducing emotions, as shown by subjective emotion ratings and physiological responses. This talk will explore the implications of systems like AffectMachine for supporting emotion self-regulation.




Kaustuv Kanti Ganguli
Session II: 6 AM PT, 12th November, Online - ISMIR 2024

Title: Harmonic Convergence: Orchestrating the Synergy of Human Intuition and Machine Intelligence in Music

Abstract: In the rapidly evolving landscape of computational musicology, we stand at a fascinating crossroads where human perception intertwines with machine-driven analysis. This convergence offers unprecedented opportunities to unravel the complexities of musical structures, particularly in rich non-Eurogenetic traditions such as Indian art music. By harmonizing human cognition with artificial intelligence, we can decode the intricate artifacts of audio signal processing, revealing new dimensions in our understanding of music. This approach not only enhances our appreciation of musical nuances but also challenges us to rethink the boundaries between human creativity and computational analysis. 

As we navigate this confluence, we must consider the profound implications for music education, composition, and appreciation. How can we leverage machine learning to augment human musical intuition? What new insights into musical cognition can emerge from this synthesis? By exploring these questions, we open doors to innovative pedagogical tools, more nuanced music recommendation systems, and perhaps even new forms of musical expression. The future of music analysis lies not in choosing between human expertise and artificial intelligence but in orchestrating a symphony where both play in perfect harmony, each enhancing the other's strengths and compensating for limitations.",Martin Hartmann,,,"**Martin Hartmann** is an Assistant Professor of Musicology at the University of Jyväskylä, where he works for the Centre of Excellence in Music, Mind, Body, and Brain and for the European Research Council project MUSICONNECT. His research encompasses music and movement, perception, information retrieval, and therapy. Currently, he specializes in the computational modeling of multimodal interactions in music and dance contexts. He is an executive group member of the Finnish Doctoral Network for Music Research and the local coordinator of the EU-funded FORTHEM Alliance Lab for Arts and Aesthetics in Contemporary Society. He led the project “Interaction in Music Therapy for Depression”, maintains the MoCap (Motion Capture) Toolbox for MATLAB, and holds editorial roles for the journals *Music Perception* and *Psychology of Music*.

**Dr. Kat Agres** is an Assistant Professor at the Yong Siew Toh Conservatory of Music at the National University of Singapore (NUS), and Founding Director of the Centre for Music and Health, the first dedicated research centre in Southeast Asia to spearhead evidence-based research leveraging the efficacy of music for health and well-being. Kat received her PhD in Cognitive Psychology from Cornell University and completed her postdoctoral fellowships in Music Cognition and Computational Creativity at the University of London. She also holds a bachelor's degree in Cello Performance and Psychology from Carnegie Mellon University. Kat’s research explores music interventions and technologies for healthcare and well-being, music perception and cognition, and computational creativity. She has received numerous grants to support her research in Singapore, the US, and UK. Kat has presented her research in over twenty countries around the world, and has also performed professionally as a cellist.

**Dr. Kaustuv Kanti Ganguli** is an Associate Professor of Artificial Intelligence at Zayed University and a Scholar at New York University Abu Dhabi Scholar, spearheading computational musicology and machine learning research. His innovative work bridges AI and music, focusing on Arabian Gulf and South Indian repertoires. Dr. Ganguli develops AI models that enhance music understanding, preservation, and education by combining engineering approaches with human cognition. A President's Gold Medal recipient and accomplished Hindustani vocal performer, his expertise spans machine learning, virtual reality, and audio processing. His groundbreaking projects include Raga/Makam characterization, multi-sensory perception, and crossmodal correspondence that collectively foster a deeper appreciation for diverse musical traditions through the lens of artificial intelligence. Kaustuv envisions blending humanistic and computational methods in a cross-disciplinary environment within a liberal arts framework, focusing on cutting-edge research and sustainable, innovative teaching.",,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
42,Registration,3,2024-11-12,8:00,8:30,Registration,,,,,,,,,,
43,Online Q&A w/ volunteers,3,2024-11-12,8:00,8:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
45,Poster Session - 3,3,2024-11-12,9:45,11:00,Poster session,"**In-person presentations:**

* [Field Study on Children's Home Piano Practice: Developing a Comprehensive System for Enhanced Student-Teacher Engagement - Seikoh Fukuda (PTNA Research Institute of Music)*, Yuko Fukuda (Kyoritsu Women’s University, To-on Kikaku Company), Ami Motomura (To-on Kikaku Company), Eri Sasao (To-on Kikaku Company), Masamichi Hosoda (NTT East Corporation), Masaki Matsubara (University of Tsukuba), Masahiro Niitsuma (Keio University)](/poster_128.html)
* [Human-AI Music Process: A Dataset of AI-Supported Songwriting Processes from the AI Song Contest - Lidia J Morris (University of Washington)*, Rebecca Leger (Fraunhofer IIS), Michele Newman (University of Washington), John Ashley Burgoyne (University of Amsterdam), Ryan Groves (Self-employed), Natasha Mangal (CISAC), Jin Ha Lee (University of Washington)](/poster_79.html)
* [Cue Point Estimation using Object Detection - Giulia Arguello (ETH Zurich), Luca A Lanzendoerfer (ETH Zurich)*, Roger Wattenhofer (ETH Zurich)](/poster_19.html)
* [The ListenBrainz Listens Dataset - Kartik Ohri (MetaBrainz Foundation Inc.)*, Robert Kaye (MetaBrainz Foundation Inc.)](/poster_317.html)
* [SpecMaskGIT: Masked Generative Modelling of Audio Spectrogram for Efficient Audio Synthesis and Beyond - Marco Comunità (Queen Mary University of London), Zhi Zhong (Sony Group Corporation)*, Akira Takahashi (Sony Group Corporation), Shiqi Yang (Sony), Mengjie Zhao (Sony Group Corporation), Koichi Saito (Sony Gruop Corporation), Yukara Ikemiya (Sony Research), Takashi Shibuya (Sony AI), Shusuke Takahashi (Sony Group Corporation), Yuki Mitsufuji (Sony AI)](/poster_96.html)
* [Long-form music generation with latent diffusion - Zach Evans (Stability AI), Julian D Parker (Stability AI)*, CJ Carr (Stability AI), Zachary Zuckowski (Stability AI), Josiah Taylor (Stability AI), Jordi Pons (Stability AI)](/poster_258.html)
* [Composer's Assistant 2: Interactive Multi-Track MIDI Infilling with Fine-Grained User Control - Martin E Malandro (Sam Houston State University)*](/poster_60.html)
* [Piano Concerto Dataset (PCD): A Multitrack Dataset of Piano Concertos - Yigitcan Özer, Simon Schwär, Vlora Arifi-Müller, Jeremy Lawrence, Emre Sen, and Meinard Müller*](/poster_512.html)
* [Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control - Yu-Hua Chen (NTU)*, Yen-Tung Yeh (National Taiwan University), Yuan-Chiao Cheng  (Positive Grid), Jui-Te Wu (Positive Grid), Yu-Hsiang Ho (Positive Grid ), Jyh-Shing Roger Jang (National Taiwan University), Yi-Hsuan Yang (National Taiwan University)](/poster_184.html)
* [Mel-RoFormer for Vocal Separation and Vocal Melody Transcription - Ju-Chiang Wang (ByteDance)*, Wei-Tsung Lu (New Your University), Jitong Chen (ByteDance)](/poster_374.html)
* [Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition - Noelia N Luna-Barahona (Universidad de Alicante), Adrián Roselló (Universidad de Alicante), María Alfaro-Contreras (University of Alicante), David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana), Jorge Calvo-Zaragoza (University of Alicante)*](/poster_45.html)
* [MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT - Jinlong ZHU (Hokkaido University)*, Keigo Sakurai (Hokkaido University), Ren Togo (Hokkaido University), Takahiro Ogawa (Hokkaido University), Miki Haseyama (Hokkaido University)](/poster_101.html)
* [Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata - Recep Oguz Araz (Universitat Pompeu Fabra)*, Xavier Serra (Universitat Pompeu Fabra ), Dmitry Bogdanov (Universitat Pompeu Fabra)](/poster_166.html)

**Remote presentations:**

* [Green MIR? Investigating computational cost of recent music-Ai research in ISMIR - Andre Holzapfel (KTH Royal Institute of Technology in Stockholm)*, Anna-Kaisa Kaila (KTH Royal Institute of Technology, Stockholm), Petra Jääskeläinen (KTH)](/poster_113.html)
* [Inner Metric Analysis as a Measure of Rhythmic Syncopation - Brian Bemman (Durham University)*, Justin Christensen (The University of Sheffield)](/poster_251.html)
* [WHO'S AFRAID OF THE `ARTYFYSHALL BYRD'? HISTORICAL NOTIONS AND CURRENT CHALLENGES OF MUSICAL ARTIFICIALITY - Nicholas Cornia (Orpheus Instituut)*, Bruno Forment (Orpheus Instituut)](/poster_212.html)
* [End-to-end automatic singing skill evaluation using cross-attention and data augmentation for solo singing and singing with accompaniment - Yaolong Ju (Huawei)*, Chun Yat Wu (Huawei), Betty Cortiñas Lorenzo  (Huawei), Jing Yang (Huawei 2012 Labs), Jiajun Deng (Huawei), Fan Fan (Huawei), Simon Lui (Huawei)](/poster_89.html)
",Nicholas Bryan,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
46,Industry Session I,3,2024-11-12,11:00,12:00,Industry,"This session will be the sponsor presentations.

- MusicAI
- Suno
- Riffusion
- Pro Sound Effects
- Yamaha","Brandi Frisbie, Minz Won",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
49,Creative Practice Session I,3,2024-11-12,13:15,14:15,Music,"Music information technology has the potential to transform creative and artistic practice. Many technologists working in music information retrieval also at least are music lovers (if not skilled players), and as such have strong commitment to having their tools and technologies being useful in practice. At the same time, are these technologists indeed sufficiently aligning to musical and creative practice? Are the needs and interests of relevant real-life music stakeholders (players, composers, producers, other types of practitioners) who never heard about ‘music information retrieval’ sufficiently identified and recognized in technological research and development?

As Creative Practice chairs, considering ISMIR 2024’s special focus on ‘Bridging Technology and Musical Creativity’, we want to stimulate more awareness of (and joint learning on) these questions. In order to do this, we wish to facilitate dialogues and collaborations on this topic between technologists and creatives. While [several community members contributed ideas](https://bit.ly/ismir24-creative-practice-ideas) on which you can respond to collaborate ([https://bit.ly/ismir24-creative-practice-ideas](https://bit.ly/ismir24-creative-practice-ideas)), at ISMIR 2024, we also will host two panels featuring invited guests who all are active on the bridges between technology and creative practice.

In today’s panel, we will host and have a conversation with:

* **Mark Goldstein** \- Percussionist, programmer, teacher, inventor; with an interest in the nexus of musical gesture, sound, and expression.  
* **Michelle Alexander** \-  Musician, Music Analyst & Mood Specialist at Pandora  
* **Carlos Mosquera** \- Musician, Programmer, CEO at CM MEDIA LLC","Cynthia Liem, Tomàs Peire",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
51,Poster Session - 4,3,2024-11-12,15:30,17:00,Poster session,"**In-person presentations:**

* [Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving - Francesco Foscarin (Johannes Kepler University Linz)*, Emmanouil Karystinaios (Johannes Kepler University), Eita Nakamura (Kyoto University), Gerhard Widmer (Johannes Kepler University)](/poster_9.html)
* [From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano - Huan Zhang (Queen Mary University of London)*, Jinhua Liang (Queen Mary University of London), Simon Dixon (Queen Mary University of London)](/poster_132.html)
* [Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-efficient Approach - Pedro Ramoneda (Universitat Pompeu Fabra)*, Vsevolod E Eremenko (Music Technology Group at Universitat Pompeu Fabra), Alexandre D'Hooge (Université de Lille), Emilia Parada-Cabaleiro (Nuremberg University of Music), Xavier Serra (Universitat Pompeu Fabra )](/poster_78.html)
* [Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications with Children - Michele Newman (University of Washington)*, Lidia Morris (University of Washington), Jun Kato (National Institute of Advanced Industrial Science and Technology (AIST)), Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)), Jason Yip (University of Washington), Jin Ha Lee (University of Washington)](/poster_83.html)
* [El Bongosero: A Crowd-sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired with Drum Patterns - Behzad Haki (Universitat Pompeu Fabra), Nicholas Evans (Universitat Pompeu Fabra)*, Daniel Gómez (MTG), Sergi Jordà (Universitat Pompeu Fabra)](/poster_24.html)
* [Utilizing Listener-Provided Tags for Music Emotion Recognition: A Data-Driven Approach - Joanne Affolter (Ecole Polytechnique Fédérale de Lausanne (EPFL))*, Yannis Rammos (EPFL), Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne)](/poster_173.html)
* [PiCoGen2: Piano cover generation with transfer learning approach and weakly aligned data - Chih-Pin Tan (National Taiwan University)*, Hsin Ai (National Taiwan University), Yi-Hsin Chang (National Taiwan University), Shuen-Huei Guan (KKCompany Techonologies), Yi-Hsuan Yang (National Taiwan University)](/poster_114.html)
* [Diff-MST: Differentiable Mixing Style Transfer - Soumya Sai Vanka (QMUL)*, Christian J. Steinmetz (Queen Mary University of London), Jean-Baptiste Rolland (Steinberg Media Technologies GmbH), Joshua D. Reiss (Queen Mary University of London), George Fazekas (QMUL)](/poster_299.html)
* [Semi-Supervised Contrastive Learning of Musical Representations - Julien Guinot (Queen Mary University of London)*, Elio Quinton (Universal Music Group), György Fazekas (QMUL)](/poster_84.html)
* [A Dataset of Larynx Microphone Recordings for Singing Voice Reconstruction - Simon Schwär*, Michael Krause, Michael Fast, Sebastian Rosenzweig, Frank Scherbaum, Meinard Müller](/poster_513.html)
* [Improved symbolic drum style classification with grammar-based hierarchical representations - Léo Géré (Cnam)*, Nicolas Audebert (IGN), Philippe Rigaux (Cnam)](/poster_293.html)
* [Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation - Jiwoo Ryu (Sogang University), Hao-Wen Dong (University of Michigan), Jongmin Jung (Sogang University), Dasaem Jeong (Sogang University)*](/poster_5.html)
* [Continual Learning for Music Classification - Pedro González-Barrachina (University of Alicante), María Alfaro-Contreras (University of Alicante), Jorge Calvo-Zaragoza (University of Alicante)*](/poster_46.html)
* [TheGlueNote: Learned Representations for Robust and Flexible Note Alignment - Silvan David Peter (JKU)*, Gerhard Widmer (Johannes Kepler University)](/poster_103.html)
* [GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model - Xavier Riley (C4DM)*, Zixun Guo (Singapore University of Technology and Design), Andrew C Edwards (QMUL), Simon Dixon (Queen Mary University of London)](/poster_171.html)
* [Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation - Alain Riou (Sony CSL Paris), Stefan Lattner (Sony Computer Science Laboratories, Paris), Gaëtan Hadjeres (Sony CSL)*, Michael Anslow (Sony Computer Science Laboratories, Paris), Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)](/poster_306.html)
* [Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning - Fang Duo Tsai (National Taiwan University)*, Shih-Lun Wu (Carnegie Mellon University), Haven Kim (University of California San Diego), Bo-Yu Chen (National Taiwan University, Rhythm Culture Corporation), Hao-Chung Cheng (National Taiwan University), Yi-Hsuan Yang (National Taiwan University)](/poster_278.html)
* [MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing - Shangda Wu (Central Conservatory of Music), Yashan Wang (Central Conservatory of Music), Xiaobing Li (Central Conservatory of Music), Feng Yu (Central Conservatory of Music), Maosong Sun (Tsinghua University)*](/poster_90.html)
* [GraphMuse: A Library for Symbolic Music Graph Processing - Emmanouil Karystinaios (Johannes Kepler University)*, Gerhard Widmer (Johannes Kepler University)](/poster_142.html)

**Remote presentations:**

* [A Kalman Filter model for synchronization in musical ensembles - Hugo T. Carvalho (Federal University of Rio de Janeiro)*, Min S. Li (University of Birmingham), Massimiliano Di Luca (University of Birmingham), Alan M. Wing (University of Birmingham)](/poster_347.html)
",Dasaem Jeong,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
52,"Navigating the Intersection of AI and Music: Innovation, Ethics, and the Future of the Industry",3,2024-11-12,17:00,18:00,All Meeting,"This speech explores the complex relationship between artificial intelligence and the music industry, tracing the evolution from early digital disruptions like Napster to today's AI-driven landscape. It examines how streaming platforms revolutionized music consumption and distribution, while also introducing new challenges such as streaming fraud. The speech delves into AI's multifaceted impact on music creation, production, and personalization, highlighting both its transformative potential and ethical concerns. The presentation also addresses controversial uses of voice AI technology and the legal and ethical considerations surrounding AI training data, including a fair use arguments and budding internal laws. Finally, we address proposed solutions, including the use of transparent attribution systems modeled after YouTube’s Content ID and policies for opt-in/out rights management. This keynote calls for a balanced approach, urging collaboration between artists, technologists, and policymakers to ensure that AI’s integration into music creation and distribution respects artistic integrity and promotes innovation.",Elizabeth Moody,,,"ELIZABETH MOODY, partner and chair of Granderson Des Rochers, LLP's New Media Group, is a pioneer in the digital media world. Moody has been spearheading digital music and video initiatives since the post-Napster era, both as outside counsel, and as a business executive in-house at companies like YouTube and Pandora. Today, Moody remains positioned at the intersection of technology and music rights and continues to advise her technology and rightsholder clients toward new and innovative business models and licensing deals.

Moody is at the forefront of the developing issues and opportunities that AI presents to the music and entertainment industries. She counsels several prominent generative voice and audio AI companies, advises the non-profit **Fairly Trained**, which certifies AI companies who are training the data sets with fairly acquired, licensed or owned data, and **Audioshake**, an AI-based stem separation tool in use by record labels, movie studios, and entertainment companies today to ease production and marketing.  
   
She is also keyed into the gaming and the web 3.0 world. She is partnerships counsel for the gaming company **Roblox** and also works closely with **Wave XR**, a virtual reality concerts start-up that works with artists to create unique live performances as avatar versions of themselves in imaginative digital landscapes. She developed and continues to grow **Styngr’s** efforts to power music in video games and online gaming experiences.  
   
Along with gaming and the metaverse, she is passionate about the opportunities web 3.0 will bring to the music community and creators. She represents **Audius**, the blockchain-based music streaming service, in its efforts to help creators and their fans connect more authentically by embracing the opportunities offered through a decentralized network and **Revelator**, an all-in-one music platform providing digital distribution, analytics, and web 3.0 services to artists, record labels and publishers.  She advises **Copyright Delta**, providing data connections to rights holders and AI tech platforms.

Moody is excited to bring opportunities to the music industry by forging deals with those in industries outside of music, including at the intersection of music and fitness. She represents connected fitness, yoga, pilates, mindfulness, cycling, and dance services to help them integrate music into their services. She has worked closely with **Hydrow**, the successful Peloton-style live reality-connected rowing experience, since its launch in 2019\. She believes that VR plays an important role in fitness and works with **Litesport** and **FitXR** to ensure they have access to top-notch music experiences. She has also been working in the medical and wellness space exploring licensing structures to use music in the treatment of pain, dementia, and mental illness concerns through her work with **MediMusic** and her advisory participation on the board of **Music Health**.",https://confcats-siteplex.s3.us-east-1.amazonaws.com/ismir24/large_Elizabeth_Moody_USE_c3dcedcde8.jpeg,,keynote-2-moody,https://ismir2024.slack.com/archives/C080JDEU39P,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
53,Creative Practice Event at Riffusion,3,2024-11-12,18:00,22:00,Music,"*Thank you to Riffusion for hosting this event\!*

Join us for an evening of music, drinks, and creative dialogue at the ISMIR Creative Practice x Riffusion SOCIAL on Tuesday, November 12th\! Hosted at the Riffusion HQ and the audiophile lounge, Phonobar, this evening promises a great lineup of live performances and conversations bridging technology and musical creativity.

**RSVP here**: [https://partiful.com/e/9HjldDxJTtSHXk2B5Mxc](https://partiful.com/e/9HjldDxJTtSHXk2B5Mxc)? . We have a few more spots than what capacity states now, but it will be important for people to bring their ISMIR badge.

Riffusion is a small team of musicians, engineers, and researchers building creative AI tools in San Francisco. We train foundation models for music generation, and envision Riffusion as a new musical instrument. <br/> <br/>
![Riffusion](/static/images/sponsors/riffusion.png ""Riffusion"")","Cynthia Liem, Tomàs Peire, Riffusion",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
54,Online Q&A w/ volunteers,3,2024-11-12,20:00,20:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
64,Online Q&A w/ volunteers,4,2024-11-13,8:00,8:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
66,Poster Session - 5,4,2024-11-13,9:45,11:00,Poster session,"**In-person presentations:**

* [ST-ITO: Controlling audio effects for style transfer with inference-time optimization - Christian J. Steinmetz (Queen Mary University of London)*, Shubhr singh (Queen Mary University of London), Marco Comunita (Queen Mary University of London), Ilias Ibnyahya (Queen Mary University of London), Shanxin Yuan (Queen Mary University of London), Emmanouil Benetos (Queen Mary University of London), Joshua D. Reiss (Queen Mary University of London)](/poster_249.html)
* [ComposerX: Multi-Agent Music Generation with LLMs - Qixin Deng (University of Rochester), Qikai Yang (University of Illinois at Urbana-Champaign), Ruibin Yuan (CMU)*, Yipeng Huang  (Multimodal Art Projection Research Community), Yi Wang (CMU), Xubo Liu (University of Surrey), Zeyue Tian (Hong Kong University of Science and Technology), Jiahao Pan (The Hong Kong University of Science and Technology), Ge Zhang (University of Michigan), Hanfeng Lin (Multimodal Art Projection Research Community), Yizhi Li (The University  of Sheffield), Yinghao MA (Queen Mary University of London), Jie Fu (HKUST), Chenghua Lin (University of Manchester), Emmanouil Benetos (Queen Mary University of London), Wenwu  Wang (University of Surrey), Guangyu Xia (NYU Shanghai), Wei Xue (The Hong Kong University of Science and Technology), Yike Guo (Hong Kong University of Science and Technology)](/poster_237.html)
* [Do Music Generation Models Encode Music Theory? - Megan Wei (Brown University)*, Michael Freeman (Brown University), Chris Donahue (Carnegie Mellon University), Chen Sun (Brown University)](/poster_189.html)
* [PolySinger: Singing-Voice to Singing-Voice Translation from English to Japanese - Silas Antonisen (University of Granada)*, Iván López-Espejo (University of Granada)](/poster_218.html)
* [Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music - Venkatakrishnan Vaidyanathapuram Krishnan (Georgia Institute of Technology)*, Noel Alben (Georgia Institute Of Technology), Anish Nair (Georgia Institute of Technology), Nathaniel Condit-Schultz (Georgia Institute of Technology)](/poster_354.html)
* [Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music - Pedro Pereira Sarmento (Centre for Digital Music), Jackson J Loth (Queen Mary University of London)*, Mathieu Barthet (Queen Mary University of London)](/poster_131.html)
* [Combining audio control and style transfer using latent diffusion - Nils Demerlé (IRCAM)*, Philippe  Esling (IRCAM), Guillaume Doras (Ircam), David Genova (Ircam)](/poster_316.html)
* [Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants - Mequanent Argaw Muluneh (Academia Sinica, National Chengchi University, Debre Markos University)*, Yan-Tsung Peng (National Chengchi University), Li Su (Academia Sinica)](/poster_160.html)
* [Wagner Ring Dataset: A Complex Opera Scenario for Music Processing and Computational Musicology - Christof Weiß, Vlora Arifi-Müller*, Michael Krause, Frank Zalkow, Stephanie Klauk, Rainer Kleinertz, and Meinard Müller](/poster_514.html)
* [Lyrics Transcription for Humans: A Readability-Aware Benchmark - Ondřej Cífka (AudioShake)*, Hendrik Schreiber (AudioShake), Luke Miner (AudioShake), Fabian-Robert Stöter (AudioShake)](/poster_272.html)
* [Content-based Controls for Music Large-scale Language Modeling - Liwei Lin (New York University Shanghai)*, Gus Xia (New York University Shanghai), Junyan Jiang (New York University Shanghai), Yixiao Zhang (Queen Mary University of London)](/poster_61.html)
* [Exploring the inner mechanisms of large generative music models - Marcel A Vélez Vásquez (University of Amsterdam)*, Charlotte Pouw (University of Amsterdam), John Ashley Burgoyne (University of Amsterdam), Willem Zuidema (ILLC, UvA)](/poster_119.html)
* [Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases - Saebyul Park (KAIST)*, Halla Kim (KAIST), Jiye Jung (Heinrich Heine University Düsseldorf), Juyong Park (KAIST), Jeounghoon Kim (KAIST), Juhan Nam (KAIST)](/poster_181.html)
* [Robust lossy audio compression identification - Hendrik Vincent Koops (Universal Music Group)*, Gianluca Micchi (Universal Music Group), Elio Quinton (Universal Music Group)](/poster_304.html)
* [RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis - Malcolm Sailor (Yale University)*](/poster_322.html)
* [Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset - Silvan David Peter*, Carlos Eduardo Cancino-Chacón, Francesco Foscarin, Andrew Philip McLeod, Florian Henkel, Emmanouil Karystinaios, Gerhard Widmer](/poster_518.html)

**Remote presentations:**

* [On the validity of employing ChatGPT for distant reading of music similarity - Arthur Flexer (Johannes Kepler University Linz)*](/poster_1.html)
* [A Critical Survey of Research in Music Genre Recognition - Owen Green (Max Planck Institute for Empirical Aesthetics)*, Bob L. T.  Sturm (KTH Royal Institute of Technology), Georgina Born (University College London), Melanie Wald-Fuhrmann (Max Planck Institute for Empirical Aesthetics)](/poster_149.html)
",Rachel Bittner,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
67,Creative Practice Session II,4,2024-11-13,11:00,12:15,Music,"Music information technology has the potential to transform creative and artistic practice. Many technologists working in music information retrieval also at least are music lovers (if not skilled players), and as such have strong commitment to having their tools and technologies being useful in practice. At the same time, are these technologists indeed sufficiently aligning to musical and creative practice? Are the needs and interests of relevant real-life music stakeholders (players, composers, producers, other types of practitioners) who never heard about ‘music information retrieval’ sufficiently identified and recognized in technological research and development?

As Creative Practice chairs, considering ISMIR 2024’s special focus on ‘Bridging Technology and Musical Creativity’, we want to stimulate more awareness of (and joint learning on) these questions. In order to do this, we wish to facilitate dialogues and collaborations on this topic between technologists and creatives. While [several community members contributed ideas](https://bit.ly/ismir24-creative-practice-ideas) on which you can respond to collaborate ([https://bit.ly/ismir24-creative-practice-ideas](https://bit.ly/ismir24-creative-practice-ideas)), at ISMIR 2024, we also will host two panels featuring invited guests who all are active on the bridges between technology and creative practice.

In today’s panel, we will host and have a conversation with:

* **Eyal Amir** \- musician, software developer and musical instruments creator. Co-founder and CTO of the audio plugins company Modalics.  
* **Ben Cantil** aka Encanti \- electronic music producer, software designer, educator, and scholar  
* **Seth Forsgren** \- Amateur Musician, CEO at Riffusion  
* **Spencer Salazar** \- Principal Engineer (formerly CTO) at Output","Cynthia Liem, Tomàs Peire",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,
70,Status Report: AI Music in Q1 of the 21st Century,4,2024-11-13,13:30,14:30,All Meeting,"I finished my PhD in 2000; a lot has happened over the ensuing ~25 years in the field of music and computation. It seems like an appropriate moment to look back at where we were, how far we’ve come, and where we’re going next.  I will discuss early experiments in RNN-generated music, the open-source Magenta project, the rise of LLM and diffusion models for music generation, and more recent work we’ve done at Google DeepMind in text, image, video and music generation. I’ll also address the question of how AI might help us better understand music and maybe even give rise to new forms of musical expression.",Douglas Eck,,,"Doug is a Senior Research Director at Google, and leads research efforts at Google DeepMind in Generative Media, including image, video, 3D, music and audio generation. His own research lies at the intersection of machine learning and human-computer interaction (HCI). In 2015, Doug created Magenta, an ongoing research project exploring the role of AI in art and music creation. Before joining Google in 2010, Doug did research in music perception, aspects of music performance, machine learning for large audio datasets and music recommendation. He completed his PhD in Computer Science and Cognitive Science at Indiana University in 2000 and went on to a postdoctoral fellowship with Juergen Schmidhuber at IDSIA in Lugano Switzerland. From 2003-2010, Doug was faculty in Computer Science in the University of Montreal machine learning group (now MILA machine learning lab), where he became Associate Professor.  ",https://confcats-siteplex.s3.us-east-1.amazonaws.com/ismir24/large_Doug_Eck_39e3112975.jpg,,keynote-3-eck,https://ismir2024.slack.com/archives/C0805L30JTV,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
72,Poster Session - 6,4,2024-11-13,15:45,17:00,Poster session,"**In-person presentations:**

* [MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models - Benno Weck (Music Technology Group, Universitat Pompeu Fabra (UPF))*, Ilaria Manco (Queen Mary University of London), Emmanouil Benetos (Queen Mary University of London), Elio Quinton (Universal Music Group), George Fazekas (QMUL), Dmitry Bogdanov (Universitat Pompeu Fabra)](/poster_333.html)
* [Human Pose Estimation for Expressive Movement Descriptors in Vocal Musical Performance - Sujoy Roychowdhury (Indian Institute of Technology Bombay)*, Preeti Rao (Indian Institute of Technology  Bombay), Sharat Chandran (IIT Bombay)](/poster_273.html)
* [Enhancing predictive models of music familiarity with EEG: Insights from fans and non-fans of K-pop group NCT127 - Seokbeom Park (KAIST), Hyunjae Kim (KAIST), Kyung Myun Lee (KAIST)*](/poster_373.html)
* [Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-based Stem Modification And Precise Beat-Grid Estimation - Robert Sowula (TU Wien)*, Peter Knees (TU Wien)](/poster_261.html)
* [MidiCaps: A Large-scale MIDI Dataset with Text Captions - Jan Melechovsky (Singapore University of Technology and Design), Abhinaba Roy (SUTD)*, Dorien Herremans (Singapore University of Technology and Design)](/poster_198.html)
* [A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis - Stephen Hahn (Duke)*, Weihan Xu (duke), Zirui Yin (Duke University), Rico Zhu (Duke University), Simon Mak (Duke University), Yue Jiang (Duke University), Cynthia Rudin (Duke)](/poster_367.html)
* [DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation - Zachary Novack (UC San Diego)*, Julian McAuley (UCSD), Taylor Berg-Kirkpatrick (UCSD), Nicholas J. Bryan (Adobe Research)](/poster_146.html)
* [The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing - Christopher J. Tralie (Ursinus College)*, Ben Cantil (DataMind Audio)](/poster_340.html)
* [DEEP RECOMBINANT TRANSFORMER: ENHANCING LOOP COMPATIBILITY IN DIGITAL MUSIC PRODUCTION - Muhammad Taimoor Haseeb (Mohamed bin Zayed University of Artificial Intelligence)*, Ahmad Hammoudeh (Mohamed bin Zayed University of Artificial Intelligence), Gus Xia (Mohamed bin Zayed University of Artificial Intelligence)](/poster_231.html)
* [Repertoire-Specific Vocal Pitch Data Generation for Improved Melodic Analysis of Carnatic Music - Genís Plaja-Roglans*, Thomas Nuttall, Lara Pearson, Xavier Serra, Marius Miron](/poster_515.html)
* [I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition - Yannis Vasilakis (Queen Mary University of London)*, Rachel Bittner (Spotify), Johan Pauwels (Queen Mary University of London)](/poster_275.html)
* [Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection - Weixing Wei (Kyoto University)*, Jiahao Zhao (Kyoto University), Yulun Wu (Fudan University), Kazuyoshi Yoshii (Kyoto University)](/poster_18.html)
* [Towards Universal Optical Music Recognition: A Case Study on Notation Types - Juan Carlos Martinez-Sevilla (University of Alicante)*, David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana), Jorge Calvo-Zaragoza (University of Alicante)](/poster_66.html)
* [Controlling Surprisal in Music Generation via Information Content Curve Matching - Mathias Rose Bjare (Johannes Kepler University Linz)*, Stefan Lattner (Sony Computer Science Laboratories, Paris), Gerhard Widmer (Johannes Kepler University)](/poster_143.html)
* [Toward a More Complete OMR Solution - Guang Yang (University of Washington)*, Muru Zhang (University of Washington), Lin Qiu (University of Washington), Yanming Wan (University of Washington), Noah A Smith (University of Washington and Allen Institute for AI)](/poster_193.html)
* [Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning - Ilaria Manco (Queen Mary University of London)*, Justin Salamon (Adobe), Oriol Nieto (Adobe)](/poster_336.html)
* [Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Model - Seungheon Doh (KAIST)*, Keunwoo Choi (Genentech), Daeyong Kwon (KAIST), Taesoo Kim (KAIST), Juhan Nam (KAIST)](/poster_396.html)
* [STONE: Self-supervised tonality estimator - Yuexuan KONG (Deezer)*, Vincent Lostanlen (LS2N, CNRS), Gabriel Meseguer Brocal (Deezer), Stella Wong (Columbia University), Mathieu Lagrange (LS2N), Romain Hennequin (Deezer Research)](/poster_260.html)
* [Beat this! Accurate beat tracking without DBN postprocessing - Francesco Foscarin (Johannes Kepler University Linz)*, Jan Schlüter (JKU Linz), Gerhard Widmer (Johannes Kepler University)](/poster_10.html)
* [The Sound Demixing Challenge 2023 – Music Demixing Track - Giorgio Fabbro*, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji](/poster_519.html)
",Magdalena Fuentes,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
73,Industry Session II,4,2024-11-13,17:00,18:00,Industry,"This session will be the sponsor presentations.

- Adobe
- Splice
- Bytedance
- Google
- Deezer
- UMG","Brandi Frisbie, Minz Won",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
74,ISMIR 2024 Meetup with Industry Panel: Bridging Technology and Musical Creativity,4,2024-11-13,19:00,20:00,Industry,"Join us for a Meetup with Industry Panel for the 25th International Society for Music Information Retrieval (ISMIR) Conference\!  
The panel aims to explore the intersection of technology and musical creativity, discussing current trends, challenges, and future possibilities in this rapidly evolving field. This year we are excited to be able to offer a set number of tickets to the public\!  
We’re excited to welcome our very special guests:

* Moderated by [Jessica Powell](https://www.linkedin.com/in/jessica-powell-28b8b7a/)  
* [Stephen White](https://www.linkedin.com/in/stephenhwhite/)  
* [Douglas McCausland](https://www.linkedin.com/in/douglasmccausland/)  
* [Tony Brooke](https://www.linkedin.com/in/tonybrooke/)  
* [Heidi Trefethen](https://www.linkedin.com/in/heidi-trefethen-a2901a6/)  
* [Anna Huang](https://czhuang.github.io/)","Brandi Frisbie, Minz Won",,,"![Jessica Powell](/static/images/industry/jessica.jpg ""Jessica"")
**Jessica Powell**
_CEO and co-founder of AudioShake_

Jessica Powell is the CEO and co-founder of AudioShake, a sound-splitting AI technology that makes audio more interactive, accessible, and useful. Named one of TIME’s Best Inventions, AudioShake is used widely across the entertainment industry to help give content owners greater opportunities and control over their audio.

Powell spent over a decade at Google, where she sat on the company's management team, reporting into the CEO. She began her career at CISAC, the International Society of Authors and Composers in Paris. She is also an award-winning novelist and former New York Times contributing opinion writer, and her short stories and essays have been published in the New York Times, TIME, WIRED, and elsewhere.


![Stephen White](/static/images/industry/stephen.jpeg ""Stephen White"")
**Stephen White**
_Chief Product Officer at EMPIRE_

Stephen is the Chief Product Officer at EMPIRE. In this role White leads all technology and product development for the company.  

White has held numerous roles across the music industry for over 25 years.  Prior to joining EMPIRE White was the Chief Executive Officer at StageIt Corp.  White joined StageIt in May of 2020 and lead the company to its successful sale to VNUE.

Prior to joining StageIt White was the Chief Executive and Chairman of the Board for Dubset Media Holdings. White joined Dubset as CEO in February 2015 and lead the company through a repositioning and rebranding, ultimately selling the business to PEX in February of 2020.  

White served as chief executive and President at Gracenote from 2012-2014.  White held numerous roles for the company across 14 years and played a critical role in growing the company from a small start-up, focusing on music technologies and information, into a digital entertainment leader that touches millions of music and movie fans around the globe. In his role as president, White oversaw all company strategy and operations, and was responsible for growing Gracenote’s core business. 

Before joining Gracenote, White was the vice president of product for streaming music start-up Echo.com, which was one of the first companies to combine group content streaming and community features. Prior to Echo.com, White was a senior director and executive producer for CKS, a media consultancy based in Silicon Valley where he led teams in the creation of web properties such as the Apple online store and GM.com. He began his career as a reporter and writer. 


![Douglas McCausland](/static/images/industry/doug.jpg ""Doug McCausland"")
**Douglas McCausland**
_TAC Studio Manager and Faculty Lecturer at San Francisco Conservatory of Music and Center for Computer Research in Music and Acoustics (CCRMA) at Stanford University_

Douglas McCausland is a composer / performer, sound designer, and digital artist whose visceral and often chaotic works explore the extremes of sound, technology, and the digital medium.

As an artist, he researches and leverages the intersections of numerous technologies and creative practices, such as real-time electronic music performance with purpose-built interfaces, spatial audio, interactive systems, intermedia art, musical applications of machine-learning, experimental sound design, and hardware-hacking.

Described as “Tremendously powerful, dark, and sometimes terrifying...” (SEAMUS) and “Ruthlessly visceral...” (The Wire), his works have been performed internationally at numerous festivals, including: Sonorities, SEAMUS, the San Francisco Tape Music Festival, MISE-EN Music Festival, Klingt Gut!, Sounds Like THIS!, NYCEMF, Sonicscape, and Ars Electronica. Recent honors include an award of distinction in the 2021 Prix Ars Electronica for his piece “Convergence”, winning 1st-Prize in the 2021 ASCAP/SEAMUS commission competition, and the gold-prize award for “contemporary computer music” in the Verband Deutscher Tonmeister Student 3D Audio Production Competition.

Douglas is currently the Technology and Applied Composition (TAC) Studio Manager at the San Francisco Conservatory of Music. He holds a DMA in music composition from Stanford University, where he studied with Chris Chafe, Patricia Alessandrini, Jaroslaw Kapuscinski, Fernando Lopez-Lezcano, and Mark Applebaum.

![Tony Brooke](/static/images/industry/tony.jpeg ""Tony Brooke"")
**Tony Brooke**
_Independent Consultant for Music Data Companies_
[https://tonybrooke.com/](https://tonybrooke.com)

Tony Brooke is an independent consultant, helping various music companies deliver data-driven products. He was most recently Senior Director of Product Systems at Warner Music Group, leading the team that develops the label copy systems for the major label. Before that he was Senior Product Manager at Pandora, responsible for the systems that ingest and store all the metadata, audio, and images from labels, distributors, and third-party sources. He led Pandora's project to add full credits in 2019.

Tony also fixes data problems throughout the digital music value chain, and helps creators improve their data. He has been on the Board of Directors at DDEX, Co-Chair of the DDEX ERN Working Group, on the Board of the San Francisco chapter of the Recording Academy, and Chair of the chapter's Producers and Engineers Wing. He presents often at industry events and completed significant research into audiovisual data and media asset management as part of his Master's degree in Library and Information Science.

He has also been an audio engineer specializing in remote multitrack recording since 1992, with over 100 releases in his discography (including two GRAMMY-nominated albums) and over 500 clients. SilentWay.com is his audio info hub with thousands of articles, links, tips, and equipment guides. Tony has worked in many stages of music creation and broadcasting as a producer, engineer, singer, FM program director, and DJ. He has lived in the San Francisco Bay Area since 1991.


![Heidi Trefethen](/static/images/industry/heidi.jpeg ""Heidi Trefethen"")
**Heidi Trefethen**
_Adjunct Professor, SF Conservatory TAC (Technology and Applied Composition) Program_
_FOH/Monitor Engineer, SFJAZZ and Freight & Salvage_
_Co-Chair, Producer's & Engineers Wing, SF Chapter and P&E Advisory Group Committee Member_

Heidi Trefethen is a distinguished live and recording engineer, producer, composer, and adjunct professor at the San Francisco Conservatory. As a professional French hornist, she performs across classical, jazz and cross-over genres. Renowned for her technical and creative expertise, she brings a unique, multifaceted perspective to music production, performance and education. As chair and co-chair of the P&E Wing of the Recording Academy's San Francisco chapter and the Women in the Mix P&E group for several years, she has championed the rights of engineers on the P&E Advisory Committee, advocating for fair treatment and equitable opportunities in the absence of a formal union. You can hear her work several times a week mixing live performances at SFJAZZ and Freight and Salvage, two of SF Bay Area's premier music venues.

Her dual career as an educator and music creator deepens her understanding of the evolving role of AI in music, especially in how it can support and elevate the work of engineers, educators, and musicians. Her contributions have a significant impact both at the local level and throughout the Recording Academy at-large, shaping policy and creating pathways for the next generation of audio professionals.

![Anna Huang](/static/images/industry/anna.jpeg ""Anna Huang"")
**Anna Huang**
_Assistant Professor of Music, Assistant Professor of Electrical Engineering and Computer Science at MIT_

In Fall 2024, Anna started a faculty position at Massachusetts Institute of Technology (MIT), with a shared position between Electrical Engineering and Computer Science (EECS) and Music and Theater Arts (MTA). For the past 8 years, she has been a researcher at Magenta in Google Brain and then Google DeepMind, working on generative models and interfaces to support human-AI partnerships in music making.

Anna is the creator of the ML model Coconet that powered Google’s first AI Doodle, the Bach Doodle. In two days, Coconet harmonized 55 million melodies from users around the world. In 2018, she created Music Transformer, a breakthrough in generating music with long-term structure, and the first successful adaptation of the transformer architecture to music. Their ICLR paper is currently the most cited paper in music generation.

Anna was a Canada CIFAR AI Chair at Mila, and continues to hold an adjunct professorship at University of Montreal. She was a judge then organizer for AI Song Contest 2020-22. Anna completed her PhD at Harvard University, master’s at the MIT Media Lab, and a dual bachelor’s at University of Southern California in music composition and CS.


",,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
75,ISMIR 2024 Meetup with Industry Panel: Bridging Technology and Musical Creativity,4,2024-11-13,20:00,22:00,Industry,Q&A and Networking portion of the Meetup with Industry.,Brandi Frisbie and Minz Won,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
76,Online Q&A w/ volunteers,4,2024-11-13,20:00,20:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
86,Registration,5,2024-11-14,8:00,8:30,Registration,,,,,,,,,,
87,Online Q&A w/ volunteers,5,2024-11-14,8:00,8:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
89,Poster Session - 7,5,2024-11-14,9:45,11:00,Poster session,"**In-person presentations:**

* [Scoring Time Intervals Using Non-Hierarchical Transformer for Automatic Piano Transcription - Yujia Yan (University of Rochester)*, Zhiyao Duan (University of Rochester)](/poster_382.html)
* [CADENZA: A Generative Framework for Expressive Musical Ideas and Variations - Julian Lenz (Lemonaide ), Anirudh Mani (Lemonaide)*](/poster_315.html)
* [Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow - Nathaniel Condit-Schultz (Georgia Institute of Technology)*](/poster_280.html)
* [Exploring GPT's Ability as a Judge in Music Understanding - Kun Fang (McGill University)*, Ziyu Wang (NYU Shanghai), Gus Xia (New York University Shanghai), Ichiro Fujinaga (McGill University)](/poster_345.html)
* [Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio - Roser Batlle-Roca (Universitat Pompeu Fabra)*, Wei-Hsiang Liao (Sony Group Corporation), Xavier Serra (Universitat Pompeu Fabra ), Yuki Mitsufuji (Sony AI), Emilia Gomez (Joint Research Centre, European Commission & Universitat Pompeu Fabra)](/poster_216.html)
* [Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models - Shahan Nercessian (Native Instruments)*, Johannes Imort (Native Instruments), Ninon Devis (Native Instruments), Frederik Blang (Native Instruments)](/poster_22.html)
* [HIERARCHICAL GENERATIVE MODELING OF THE MELODIC VOICE IN HINDUSTANI CLASSICAL MUSIC - Nithya Nadig Shikarpur (Mila, University of Montreal)*, Krishna Maneesha Dendukuri (Mila), Yusong Wu (Mila, University of Montreal), Antoine CAILLON (IRCAM), Cheng-Zhi Anna Huang (Google Brain)](/poster_163.html)
* [SymPAC: Scalable Symbolic Music Generation With Prompts And Constraints - Haonan Chen (Bytedance Inc.)*, Jordan B. L. Smith (TikTok), Janne Spijkervet (University of Amsterdam), Ju-Chiang Wang (ByteDance), Pei Zou (Bytedance Inc.), Bochen Li (University of Rochester), Qiuqiang Kong (Byte Dance), Xingjian Du (University of Rochester)](/poster_366.html)
* [Unsupervised Composable Representations for Audio - Giovanni Bindi (IRCAM)*, Philippe Esling](/poster_254.html)
* [CCOM-HuQin: An Annotated Multimodal Chinese Fiddle Performance Dataset - Yu Zhang, Ziya Zhou*, Xiaobing Li, Feng Yu, Maosong Sun](/poster_516.html)
* [Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk - Pavani B Chowdary (International Institute of Information Technology, Hyderabad)*, Bhavyajeet Singh (International Institute of Information Technology, Hyderabad ), Rajat Agarwal (International Institute of Information Technology), Vinoo  Alluri (IIIT - Hyderabad)](/poster_43.html)
* [A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems - Karn N Watcharasupat (Georgia Institute of Technology)*, Alexander Lerch (Georgia Institute of Technology)](/poster_32.html)
* [Towards Musically Informed Evaluation of Piano Transcription Models - Patricia Hu (Johannes Kepler University)*, Lukáš Samuel Marták (Johannes Kepler University Linz), Carlos Eduardo Cancino-Chacón (Johannes Kepler University Linz), Gerhard Widmer (Johannes Kepler University)](/poster_144.html)
* [Using Item Response Theory to Aggregate Music Annotation Results of Multiple Annotators - Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*, Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))](/poster_205.html)
* [Just Label the Repeats for In-The-Wild Audio-to-Score Alignment - Irmak Bukey (Carnegie Mellon University)*, Michael Feffer (Carnegie Mellon University), Chris Donahue (CMU)](/poster_357.html)
* [Investigating Time-Line-Based Music Traditions with Field Recordings: A Case Study of Candomblé Bell Patterns - Lucas S Maia (Universidade Federal do Rio de Janeiro)*, Richa Namballa (New York University), Martín Rocamora (Universidad de la República), Magdalena Fuentes (New York University), Carlos Guedes (NYU Abu Dhabi)](/poster_360.html)
* [PiJAMA: Piano Jazz with Automatic MIDI Annotations - Drew Edwards*, Simon Dixon, Emmanouil Benetos](/poster_517.html)

**Remote presentations:**

* [In-depth performance analysis of the ADTOF-based algorithm for automatic drum transcription - Mickael Zehren (Umeå University)*](/poster_68.html)
",Oriol Nieto,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
,,,,,,,,,,,,,,,,
91,LBD / MIREX (onsite),5,2024-11-14,11:15,12:45,LBD,"The LBD session is a forum for presenting prototype systems, initial concepts, and early results that are not yet fully matured but hold significance for the Music Information Retrieval (MIR) community. This joint session with MIREX includes poster presentations and live demos from both LBD and MIREX submissions. ","Chih-Wei Wu, Camile Noufi, Gus Xia",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,
92,LBD / MIREX (online),5,2024-11-14,11:15,12:45,LBD,"The LBD session is a forum for presenting prototype systems, initial concepts, and early results that are not yet fully matured but hold significance for the Music Information Retrieval (MIR) community. This joint session with MIREX includes poster presentations and live demos from both LBD and MIREX submissions. ","Chih-Wei Wu, Camile Noufi, Gus Xia",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
95,Society meeting,5,2024-11-14,14:00,15:45,Social ,,ISMIR,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
96,Unconference,5,2024-11-14,15:45,17:45,Meetup,"The unconference is a set of impromptu sessions/discussions on MIR topics of greatest interest.   
During a first plenary, participants vote on their preferred topics, then join a 30-minute discussion group on this topic, after which they summarize their discussions in a second plenary, vote on new topics, and repeat the process with new groups. Topics do not need to be technical (example: what does MIR stands for ? do we need open-review ? does gen-ai cares about copyright ? how many multi-head do I need in my encoder ?). This is an informal and informative opportunity to get to know peers and colleagues from around the world. ",Geoffroy Peeters,,,,,,unconference,https://ismir2024.slack.com/archives/C07V6E270HX,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
97,Online Q&A w/ volunteers,5,2024-11-14,20:00,20:30,Meetup,Having issues with Zoom or Slack? Need help navigating the conference program or materials? Virtual volunteers will be available to meet with you and answer any questions you may have!,,,,,,,help-desk,https://ismir2024.slack.com/archives/C07TPGHFWR0,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
100,LBD / MIREX (online),5,2024-11-14,23:15,23:45,LBD,"The LBD session is a forum for presenting prototype systems, initial concepts, and early results that are not yet fully matured but hold significance for the Music Information Retrieval (MIR) community. This joint session with MIREX includes poster presentations and live demos from both LBD and MIREX submissions. ","Chih-Wei Wu, Camile Noufi, Gus Xia",,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
,,,,,,,,,,,,,,,,
102,Society meeting,6,2024-11-15,2:00,3:45,Awards,,ISMIR,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
103,Ideation sessions,6,2024-11-15,3:45,6:15,Meetup,"As a follow-up to the topics introduced by our Special Online Session Speakers, this ideation session invites ISMIR attendees to delve deeper into interdisciplinary connections. Participants will join break-out rooms to explore how various fields, including those beyond the session topics, can inform Music Information Retrieval (MIR) or, conversely, how MIR can contribute to advancements across disciplines. Each group will talk or present their discussions, highlighting their insights and proposed synergies. The session will conclude with a collective discussion and Q&A with the Speakers, aiming to foster collaboration and generate actionable ideas for pushing the boundaries of MIR research.",Vinoo Alluri,,,,,,session-chatter,https://ismir2024.slack.com/archives/C07V56WJC07,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
104,Mindfulness session,3,2024-11-12,11:45,12:45,Meetup,"soundBrilliance is an innovative digital health company using enhanced music, psychology, and measurement techniques to create tools and exercises which empower people to better self-manage fundamental health – emotional balance, fitness, quality sleep and pain control. The experiences presented in the ISMIR 2024 Mindfulness sessions are designed to help guide you into a deeper sense of Calm. All visuals are naturally produced and captured, with no AI intervention.<br/><br/> ![SoundBrilliance](/static/images/soundbrilliance.svg ""SoundBrilliance"")",soundBrilliance,,,,,,,,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
105,Mindfulness session,4,2024-11-13,11:45,12:45,Meetup,"soundBrilliance is an innovative digital health company using enhanced music, psychology, and measurement techniques to create tools and exercises which empower people to better self-manage fundamental health – emotional balance, fitness, quality sleep and pain control. The experiences presented in the ISMIR 2024 Mindfulness sessions are designed to help guide you into a deeper sense of Calm. All visuals are naturally produced and captured, with no AI intervention.<br/><br/> ![SoundBrilliance](/static/images/soundbrilliance.svg ""SoundBrilliance"")",soundBrilliance,,,,,,,,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1
106,Mindfulness session,5,2024-11-14,11:45,12:45,Meetup,"soundBrilliance is an innovative digital health company using enhanced music, psychology, and measurement techniques to create tools and exercises which empower people to better self-manage fundamental health – emotional balance, fitness, quality sleep and pain control. The experiences presented in the ISMIR 2024 Mindfulness sessions are designed to help guide you into a deeper sense of Calm. All visuals are naturally produced and captured, with no AI intervention.<br/><br/> ![SoundBrilliance](/static/images/soundbrilliance.svg ""SoundBrilliance"")",soundBrilliance,,,,,,,,https://us02web.zoom.us/j/81805661292?pwd=ZZKs8fVNlmJb9zuDHVFQEDshAOcDeG.1