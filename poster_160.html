

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared\u0027s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared\u0027s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.", "abstract": "Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared\u0027s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared\u0027s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.", "author_changes": "Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: \n\n- We have made a modification on our title so that it best fits the content. \n- We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.\n- We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).\n- In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. \n- We have addressed spelling errors and inconsistencies. \n- We have added a link to access our proposed dataset. ", "authors": ["Muluneh, Mequanent Argaw*", " Peng, Yan-Tsung", " Su, Li"], "authors_and_affil": ["Mequanent Argaw Muluneh (Academia Sinica", " National Chengchi University", " Debre Markos University)*", " Yan-Tsung Peng (National Chengchi University)", " Li Su (Academia Sinica)"], "channel_url": "https://ismir2024.slack.com/archives/C07UM5YDGLV", "day": "3", "keywords": ["Applications -\u003e music heritage and sustainability; Knowledge-driven approaches to MIR -\u003e computational ethnomusicology; Knowledge-driven approaches to MIR -\u003e computational music theory and musicology; Knowledge-driven approaches to MIR -\u003e machine learning/artificial intelligence for music; MIR tasks -\u003e automatic classification", "Knowledge-driven approaches to MIR"], "long_presentation": "FALSE", "meta_review": "After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.\n\nAll reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.\n\nHowever, the authors should improve the paper before submitting the camera-ready version on the following aspects.\n\nFirst, the readability of the paper should be improved, since the language at some points is not clear enough.\n\nSecondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)\n\nThirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.\n\nFinally, details about reproducibility, regarding not only access to the data, but to the code, should be given.", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing", "poster_pdf": "https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link", "review_1": "ISMIR Paper Review # 160\n\n\nThe paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. \n\n\u2014 The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. \n\n\u2014 Overall the paper is well written. \n\n\u2014 The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. \n\n\u2014 The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on \u201c\t\tClassifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. \n\n\u2014  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. \n\n\u2014 It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. \n\n\u2014 Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. \n\n\u2014 I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. \n\n\nResult: Somewhere between borderline/weak accept and accept. Could have been much much better.  ", "review_2": "This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.\n\nThe authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the \"vital insights\" aren\u0027t described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors _must_ reconsider the statements in the evaluation part of Section 4.\n\n### Detailed comments ###\nIn the supplied material, it is observable that the singer is louder in the beginning than towards the end. In \"Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3\" there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?\n\nFigure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It\u0027s unclear what exactly the red boxes mean and where the \"recurring consistent melodic patterns\" appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.\n\nFigure 3: Needs units on the axes. Y = number of files, X = seconds.\n\nFigure 4: Suggestion: could superimpose the blue GMM estimations onto the red \u0026 green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.\n\nTable 1: Misspelling: Array -\u003e Araray\n\nTable 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.\n\n161: \"except for color coding\": you mean to say that with color coding it\u0027s easier to identify a mode?\nFootnote 7: what do you mean by \"the reverse is not [...] true\", that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text\n180, 190 \u0026 444: the spelling of Kidase/Qidase-bet varies.\n222: \"way larger\" is a colloquialism\n240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.\n248: what does \"sliding\" refer to?\n253-254: what do you mean by \"during the performance\" and \"along the whole recording\"? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?\n267: \"less number of parameters\" than what?\n270: \"as regard it as\" -\u003e \"regarding it as...\"?\n277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?\n297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?\n304: you mention \"15 seconds\", but in the results (Table 2) you show \"20 seconds\". Apart from that, how much silence do these durations contain?\n312: 23 percentage points is NOT a \"relative small performance drop\"!\n313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.\n318: from the numbers, it is not evident how you make the claim that stabilization helps \"for most of the cases\". The _opposite_ is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!\n320: with -\u003e within\n323: \"...has better classifcation accuracy _than the masking_\", because this is not true for no stabilization.\n336-340: It\u0027s not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to \"no calibration\" and \"no stabilization\". The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.\n377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?\n380: \"Fig. 6\" -\u003e do you mean 4?\n400: remove second \"g_2\"\n\nMinor comment:\nIt would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge\u0027ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.", "review_3": "#ERROR!", "session": ["5"], "slack_channel": "p5-09-computational-analysis-of", "slides_pdf": "https://docs.google.com/presentation/d/1UeQZJTSpW_A_G8IyP67VcTRBZKVTmURQ/edit?usp=drive_link\u0026ouid=100778348236936722074\u0026rtpof=true\u0026sd=true", "title": "Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants", "video": "https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Mequanent Argaw Muluneh (Academia Sinica"
               class="text-muted"
            >Mequanent Argaw Muluneh (Academia Sinica</a
            >,
            
            <a href="papers.html?filter=authors&search= National Chengchi University"
               class="text-muted"
            > National Chengchi University</a
            >,
            
            <a href="papers.html?filter=authors&search= Debre Markos University)*"
               class="text-muted"
            > Debre Markos University)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Yan-Tsung Peng (National Chengchi University)"
               class="text-muted"
            > Yan-Tsung Peng (National Chengchi University)</a
            >,
            
            <a href="papers.html?filter=authors&search= Li Su (Academia Sinica)"
               class="text-muted"
            > Li Su (Academia Sinica)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UM5YDGLV" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p5-09-computational-analysis-of</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification"
                    class="text-secondary text-decoration-none"
            >Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
                    class="text-secondary text-decoration-none"
            >Knowledge-driven approaches to MIR</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://docs.google.com/presentation/d/1UeQZJTSpW_A_G8IyP67VcTRBZKVTmURQ/edit?usp=drive_link&amp;ouid=100778348236936722074&amp;rtpof=true&amp;sd=true" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.</p>
<p>All reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.</p>
<p>However, the authors should improve the paper before submitting the camera-ready version on the following aspects.</p>
<p>First, the readability of the paper should be improved, since the language at some points is not clear enough.</p>
<p>Secondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)</p>
<p>Thirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.</p>
<p>Finally, details about reproducibility, regarding not only access to the data, but to the code, should be given.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>ISMIR Paper Review # 160</p>
<p>The paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. </p>
<p>— The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. </p>
<p>— Overall the paper is well written. </p>
<p>— The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. </p>
<p>— The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on “       Classifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. </p>
<p>—  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. </p>
<p>— It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. </p>
<p>— Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. </p>
<p>— I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. </p>
<p>Result: Somewhere between borderline/weak accept and accept. Could have been much much better.  </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.</p>
<p>The authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the "vital insights" aren't described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors <em>must</em> reconsider the statements in the evaluation part of Section 4.</p>
<h3>Detailed comments</h3>
<p>In the supplied material, it is observable that the singer is louder in the beginning than towards the end. In "Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3" there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?</p>
<p>Figure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It's unclear what exactly the red boxes mean and where the "recurring consistent melodic patterns" appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.</p>
<p>Figure 3: Needs units on the axes. Y = number of files, X = seconds.</p>
<p>Figure 4: Suggestion: could superimpose the blue GMM estimations onto the red &amp; green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.</p>
<p>Table 1: Misspelling: Array -&gt; Araray</p>
<p>Table 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.</p>
<p>161: "except for color coding": you mean to say that with color coding it's easier to identify a mode?
Footnote 7: what do you mean by "the reverse is not [...] true", that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text
180, 190 &amp; 444: the spelling of Kidase/Qidase-bet varies.
222: "way larger" is a colloquialism
240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.
248: what does "sliding" refer to?
253-254: what do you mean by "during the performance" and "along the whole recording"? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?
267: "less number of parameters" than what?
270: "as regard it as" -&gt; "regarding it as..."?
277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?
297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?
304: you mention "15 seconds", but in the results (Table 2) you show "20 seconds". Apart from that, how much silence do these durations contain?
312: 23 percentage points is NOT a "relative small performance drop"!
313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.
318: from the numbers, it is not evident how you make the claim that stabilization helps "for most of the cases". The <em>opposite</em> is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!
320: with -&gt; within
323: "...has better classifcation accuracy <em>than the masking</em>", because this is not true for no stabilization.
336-340: It's not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to "no calibration" and "no stabilization". The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.
377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?
380: "Fig. 6" -&gt; do you mean 4?
400: remove second "g_2"</p>
<p>Minor comment:
It would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge'ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <h1>ERROR!</h1></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: </p>
<ul>
<li>We have made a modification on our title so that it best fits the content. </li>
<li>We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.</li>
<li>We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).</li>
<li>In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. </li>
<li>We have addressed spelling errors and inconsistencies. </li>
<li>We have added a link to access our proposed dataset. </li>
</ul></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;160&#39;, &#39;session&#39;: &#39;5&#39;, &#39;position&#39;: &#39;10&#39;, &#39;forum&#39;: &#39;160&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1VdrhCgcONuBQ_PGqf-4qf3l7isvYMLVN/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Muluneh, Mequanent Argaw*&#39;, &#39; Peng, Yan-Tsung&#39;, &#39; Su, Li&#39;], &#39;authors_and_affil&#39;: [&#39;Mequanent Argaw Muluneh (Academia Sinica&#39;, &#39; National Chengchi University&#39;, &#39; Debre Markos University)*&#39;, &#39; Yan-Tsung Peng (National Chengchi University)&#39;, &#39; Li Su (Academia Sinica)&#39;], &#39;keywords&#39;: [&#39;Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification&#39;, &#39;Knowledge-driven approaches to MIR&#39;], &#39;abstract&#39;: &#34;Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared&#39;s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared&#39;s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.&#34;, &#39;TLDR&#39;: &#34;Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared&#39;s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared&#39;s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.&#34;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;5&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UM5YDGLV&#39;, &#39;slack_channel&#39;: &#39;p5-09-computational-analysis-of&#39;, &#39;slides_pdf&#39;: &#39;https://docs.google.com/presentation/d/1UeQZJTSpW_A_G8IyP67VcTRBZKVTmURQ/edit?usp=drive_link&amp;ouid=100778348236936722074&amp;rtpof=true&amp;sd=true&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#39;ISMIR Paper Review # 160\n\n\nThe paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. \n\n— The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. \n\n— Overall the paper is well written. \n\n— The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. \n\n— The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on “\t\tClassifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. \n\n—  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. \n\n— It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. \n\n— Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. \n\n— I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. \n\n\nResult: Somewhere between borderline/weak accept and accept. Could have been much much better.  &#39;, &#39;review_2&#39;: &#39;This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.\n\nThe authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the &#34;vital insights&#34; aren\&#39;t described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors _must_ reconsider the statements in the evaluation part of Section 4.\n\n### Detailed comments ###\nIn the supplied material, it is observable that the singer is louder in the beginning than towards the end. In &#34;Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3&#34; there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?\n\nFigure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It\&#39;s unclear what exactly the red boxes mean and where the &#34;recurring consistent melodic patterns&#34; appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.\n\nFigure 3: Needs units on the axes. Y = number of files, X = seconds.\n\nFigure 4: Suggestion: could superimpose the blue GMM estimations onto the red &amp; green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.\n\nTable 1: Misspelling: Array -&gt; Araray\n\nTable 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.\n\n161: &#34;except for color coding&#34;: you mean to say that with color coding it\&#39;s easier to identify a mode?\nFootnote 7: what do you mean by &#34;the reverse is not [...] true&#34;, that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text\n180, 190 &amp; 444: the spelling of Kidase/Qidase-bet varies.\n222: &#34;way larger&#34; is a colloquialism\n240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.\n248: what does &#34;sliding&#34; refer to?\n253-254: what do you mean by &#34;during the performance&#34; and &#34;along the whole recording&#34;? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?\n267: &#34;less number of parameters&#34; than what?\n270: &#34;as regard it as&#34; -&gt; &#34;regarding it as...&#34;?\n277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?\n297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?\n304: you mention &#34;15 seconds&#34;, but in the results (Table 2) you show &#34;20 seconds&#34;. Apart from that, how much silence do these durations contain?\n312: 23 percentage points is NOT a &#34;relative small performance drop&#34;!\n313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.\n318: from the numbers, it is not evident how you make the claim that stabilization helps &#34;for most of the cases&#34;. The _opposite_ is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!\n320: with -&gt; within\n323: &#34;...has better classifcation accuracy _than the masking_&#34;, because this is not true for no stabilization.\n336-340: It\&#39;s not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to &#34;no calibration&#34; and &#34;no stabilization&#34;. The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.\n377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?\n380: &#34;Fig. 6&#34; -&gt; do you mean 4?\n400: remove second &#34;g_2&#34;\n\nMinor comment:\nIt would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge\&#39;ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.&#39;, &#39;review_3&#39;: &#39;#ERROR!&#39;, &#39;meta_review&#39;: &#39;After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.\n\nAll reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.\n\nHowever, the authors should improve the paper before submitting the camera-ready version on the following aspects.\n\nFirst, the readability of the paper should be improved, since the language at some points is not clear enough.\n\nSecondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)\n\nThirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.\n\nFinally, details about reproducibility, regarding not only access to the data, but to the code, should be given.&#39;, &#39;author_changes&#39;: &#39;Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: \n\n- We have made a modification on our title so that it best fits the content. \n- We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.\n- We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).\n- In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. \n- We have addressed spelling errors and inconsistencies. \n- We have added a link to access our proposed dataset. &#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>