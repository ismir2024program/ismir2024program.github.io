

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Muluneh, Mequanent Argaw*"
               class="text-muted"
            >Muluneh, Mequanent Argaw*</a
            >,
            
            <a href="papers.html?filter=authors&search= Peng, Yan-Tsung"
               class="text-muted"
            > Peng, Yan-Tsung</a
            >,
            
            <a href="papers.html?filter=authors&search= Su, Li"
               class="text-muted"
            > Su, Li</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07UM5YDGLV" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p5-09-computational-analysis-of</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
                    class="text-secondary text-decoration-none"
            >Knowledge-driven approaches to MIR</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification"
                    class="text-secondary text-decoration-none"
            >Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.</p>
<p>All reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.</p>
<p>However, the authors should improve the paper before submitting the camera-ready version on the following aspects.</p>
<p>First, the readability of the paper should be improved, since the language at some points is not clear enough.</p>
<p>Secondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)</p>
<p>Thirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.</p>
<p>Finally, details about reproducibility, regarding not only access to the data, but to the code, should be given.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>ISMIR Paper Review # 160</p>
<p>The paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. </p>
<p>— The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. </p>
<p>— Overall the paper is well written. </p>
<p>— The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. </p>
<p>— The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on “       Classifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. </p>
<p>—  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. </p>
<p>— It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. </p>
<p>— Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. </p>
<p>— I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. </p>
<p>Result: Somewhere between borderline/weak accept and accept. Could have been much much better.  </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.</p>
<p>The authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the "vital insights" aren't described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors <em>must</em> reconsider the statements in the evaluation part of Section 4.</p>
<h3>Detailed comments</h3>
<p>In the supplied material, it is observable that the singer is louder in the beginning than towards the end. In "Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3" there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?</p>
<p>Figure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It's unclear what exactly the red boxes mean and where the "recurring consistent melodic patterns" appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.</p>
<p>Figure 3: Needs units on the axes. Y = number of files, X = seconds.</p>
<p>Figure 4: Suggestion: could superimpose the blue GMM estimations onto the red &amp; green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.</p>
<p>Table 1: Misspelling: Array -&gt; Araray</p>
<p>Table 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.</p>
<p>161: "except for color coding": you mean to say that with color coding it's easier to identify a mode?
Footnote 7: what do you mean by "the reverse is not [...] true", that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text
180, 190 &amp; 444: the spelling of Kidase/Qidase-bet varies.
222: "way larger" is a colloquialism
240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.
248: what does "sliding" refer to?
253-254: what do you mean by "during the performance" and "along the whole recording"? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?
267: "less number of parameters" than what?
270: "as regard it as" -&gt; "regarding it as..."?
277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?
297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?
304: you mention "15 seconds", but in the results (Table 2) you show "20 seconds". Apart from that, how much silence do these durations contain?
312: 23 percentage points is NOT a "relative small performance drop"!
313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.
318: from the numbers, it is not evident how you make the claim that stabilization helps "for most of the cases". The <em>opposite</em> is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!
320: with -&gt; within
323: "...has better classifcation accuracy <em>than the masking</em>", because this is not true for no stabilization.
336-340: It's not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to "no calibration" and "no stabilization". The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.
377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?
380: "Fig. 6" -&gt; do you mean 4?
400: remove second "g_2"</p>
<p>Minor comment:
It would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge'ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>= SUMMARY =</p>
<p>The authors investigate the Chant of the Tewahido (Ethiopian Orthodox) Church, which has its roots in the 6th century, with MIR.
A primary goal is to classify three main types of the original Yaredawi YeZema Silt standard, which is shown to be feasible best with pitch contours. The pitch distributions that are characteristic for each of the three types are further analyzed with a GMM.
A corresponding, public and manually curated audio dataset will be published upon publication of the paper. The experiments include a cross-corpus comparison with an existing smaller dataset.</p>
<p>= EVALUATION =</p>
<p>STRENGTHS:</p>
<p>(+) Interesting ideas for modelling the chant types by pitch distribution are presented.</p>
<p>(+) Cross-corpus experiments and analysis.</p>
<p>(+) The paper is generally well written and understandable.</p>
<p>WEAKNESSES:</p>
<p>(-) Some parts of the machine learning experiment are not clearly described or well designed (see major comments below). Especially, it is not fully clear which representation is fed into the neural network classifier.</p>
<p>(-) All samples in the dataset originate from a single singer. (However, the cross-database analysis compensates this, partially.)</p>
<p>= MAJOR COMMENTS =</p>
<p>-General comment: It might be mentioned if all EOTC chants are monophonic or if there are also polyphonic ones.</p>
<p>-Introduction, ll.56-57: "based on the analysis of a series of EOTC Chants collected in Addis Ababa, 1975." -&gt; It might be pointed out that the recordings originate from 1975 and if also written documents/oral statements were analyzed in the study. (It is slightly confusing when reading, given that the papers are from the 1990s).</p>
<p>-Section 2.1, ll.95-96: "which need to be manipulated for the language" -&gt; It is not clear why and in which way (especially) audio and video need to be manipulated.</p>
<p>-Section 3: It would be interesting to give some information on the approximate number of EOTC schools and students in Ethiopia.</p>
<p>-Section 4 reads as if only pitch-related features are taken into account. However, they are compared to MFCCs etc. This is slightly confusing when reading through the paper so it should be considered to rephrase.</p>
<p>-Section 4.1: It is not fully clear, how the slope "s" is computed? Is it simply the regression line across all (stable) pitch values over time (i.e., minimizing the squared error)?</p>
<p>-Section 4.1: It is not understandable, which representation is input to the neural network. Is it the frequency contour of the pitch (in Hz or normalized) or a histogram (distribution)? The same applies to Section 4.2. Moreover, it should be considered to have a development split for hyper-parameter optimization and early stopping.</p>
<p>-Section 4.2: Taking other statistics besides the mean, e.g., standard deviation and slope, into account might give a meaningful performance boost.</p>
<p>-Section 4.2: A 5-fold CV might not be the best choice for the within-dataset experiment if different chants were manually segmented beforehand, so that the same chant can appear in both training and test splits.</p>
<p>-Section 5: "The recording which has the highest average correlation with all the other recordings is considered as an anchor:" -&gt; It should be discussed/reasoned whether this approach for pitch shifting is the optimal one. As an example, it might lead to a different solution taking into account the minimum summed-up absolute (or squared) pitch shift across all samples of the dataset.</p>
<p>-Section 8: It might be meaningful to consider the usage restricted to educational purposes in the license when publishing the dataset.</p>
<p>= MINOR COMMENTS =</p>
<p>-General comment: "chant" is sometimes capitalized and sometimes not, which should be unified.
-Section 2.1, l.94: "[11])." -&gt; drop ")"
-Footnote 5: "data in reported" -&gt; "data reported in"
-Section 2.1, l.111: "like some other" -&gt; "like for some other"
-Section 2.2, l.129: "sorrow. [1]." -&gt; "sorrow [1]."
-Section 2.2, l.138: "[3] stated" -&gt; better: "Shelemay et al. [3] stated"
-Section 3, l.176: "departments.There" -&gt; "departments. There"
-Section 3, l.188: "departemnts," -&gt; "departments,"
-Section 3, l.219: "On the other hand," -&gt; drop this
-Section 5, l.345: "musc" -&gt; "music"
-Section 5, l.353: "contain" -&gt; "contains"
-References: The provided information (location, page numbers) should be consistent for papers in ISMIR proceedings.
-References: [22] vs [23] -&gt; Capitalization of the conference name should be consistent.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: </p>
<ul>
<li>We have made a modification on our title so that it best fits the content. </li>
<li>We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.</li>
<li>We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).</li>
<li>In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. </li>
<li>We have addressed spelling errors and inconsistencies. </li>
<li>We have added a link to access our proposed dataset. </li>
</ul></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;160&#39;, &#39;session&#39;: &#39;5&#39;, &#39;position&#39;: &#39;10&#39;, &#39;forum&#39;: &#39;160&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1VdrhCgcONuBQ_PGqf-4qf3l7isvYMLVN/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Muluneh, Mequanent Argaw*&#39;, &#39; Peng, Yan-Tsung&#39;, &#39; Su, Li&#39;], &#39;authors_and_affil&#39;: [&#39;Mequanent Argaw Muluneh (Academia Sinica&#39;, &#39; National Chengchi University&#39;, &#39; Debre Markos University)*&#39;, &#39; Yan-Tsung Peng (National Chengchi University)&#39;, &#39; Li Su (Academia Sinica)&#39;], &#39;keywords&#39;: [&#39;Knowledge-driven approaches to MIR&#39;, &#39;Applications -&gt; music heritage and sustainability; Knowledge-driven approaches to MIR -&gt; computational ethnomusicology; Knowledge-driven approaches to MIR -&gt; computational music theory and musicology; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR tasks -&gt; automatic classification&#39;], &#39;abstract&#39;: &#34;Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared&#39;s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared&#39;s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.&#34;, &#39;TLDR&#39;: &#34;Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared&#39;s establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared&#39;s standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.&#34;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1h_N18Rn6r2wvkPRPipYQgxSj-Du2_zBr/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;5&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1MEwhva_sHlKS0lC0uCgp-c0NdbR0u0aj/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1MvPSM9M5alKJLAMvY18ECo1FGwOpH1vN/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UM5YDGLV&#39;, &#39;slack_channel&#39;: &#39;p5-09-computational-analysis-of&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#39;ISMIR Paper Review # 160\n\n\nThe paper attempts to understand Ethiopian orthodox tewahido church chat using MIR techniques. It is a straightforward classification task, that uses hand-built features. The main contribution of this paper is to explore under-represented music from around the world that is being heard by millions and it should be given some major bonus points to the authors for doing this and to release a dataset openly for the public. \n\n— The paper clearly explains the acoustic characteristics, language associated with the chants. Further the notation that is letter and symbolic is also well explained. \n\n— Overall the paper is well written. \n\n— The dataset that is used for this work is manually collected. One can argue that there is no prior work in this regard, hence it is difficult to compare against other datasets or works. This emphasizes that this is more of a dataset paper having some experiments reported on top of it. \n\n— The authors collected manual features like pitch features and MFCC, and chormagram. This is kind of similar to the work by vidwans, verma, Rao on “\t\tClassifying Cultural Music Using Melodic Features, SPCOM 2020 and should be cited specially because they  used similar ideas on pitch based and timbre based features for understanding culture specific music. \n\n—  The baseline features in section 4.2 cannot be said as belong to timbre based like MFCC and kind of pitch + timbre in chromatogram averaged over time. These capture global characteristics of timbre vs allowing local characteristics of pitch based features through a conv net. I would highly recommend removing the baseline word, and characterize them as timbre, pitch features etc. Dont worry about baseline. This paper aims a new dataset and a new kind of under-represented music to the MIR community. \n\n— It will be great to report the accuracy results by potentially running 1-2 more experiments where they combine pitch + time-average features, Mel-spectogram etc as part of the ablation studies. Further the dataset size in terms of seconds is reasonable enough. It will not be too bad to extract pretrained features from large models trained on audio set and do fine-tuning to see where they stand. \n\n— Another good baseline would be to have a model trained from scratch on small patches of audio and see how it does. This would not involve any hand-built features and directly operate on the audio waveform. \n\n— I like the fact that they have a decent discuss section that explains various connections between the intricacies of the music and the findings of a GMM estimated pitch distribution. \n\n\nResult: Somewhere between borderline/weak accept and accept. Could have been much much better.  &#39;, &#39;review_2&#39;: &#39;This paper provides a careful, detailed and systematic approach to an underrepresented style of music in the MIR community. The introduction of the music and presentation of the research problem at hand is confident and well executed, and readers who are not familiar with this data (like me) should find sufficient explorative potential to navigate this paper.\n\nThe authors generally take good care of detailed explanations of the terminology and methodology, which is a definite strong point of this paper. Occasionally, a lack of depth becomes evident, for example at the end of Section 2, where the &#34;vital insights&#34; aren\&#39;t described further. Unfortunately, the care and consistency fall heavily short in Section 4 - see more comments on this below. In its current version, Section 4 is unacceptable, pushing the review of this paper towards a weak reject. Due to the importance of the research direction, general presentation, clear musictheoretical analysis and provision of this dataset, my evaluation is a weak accept - but: The authors _must_ reconsider the statements in the evaluation part of Section 4.\n\n### Detailed comments ###\nIn the supplied material, it is observable that the singer is louder in the beginning than towards the end. In &#34;Ge_ez_chanting_Mode_for_a_Wednesday_Prayer.mp3&#34; there is a sudden rise in amplitude at the end. How do you explain that? Could that be the beginning of a new chant, meaning that something happened during the cutting of the audio?\n\nFigure 2 is difficult to disseminate. Where exactly are the first two rows of notation patterns to be found in the sentences? I can only find very few correlations. Can you elaborate that in the text? It\&#39;s unclear what exactly the red boxes mean and where the &#34;recurring consistent melodic patterns&#34; appear, as stated at the end of Section 2.3. If the latter are highlighted with the arrows, then why is it first as a lyric, and then as a music notation? You mention that in the caption, but it is rather complicated.\n\nFigure 3: Needs units on the axes. Y = number of files, X = seconds.\n\nFigure 4: Suggestion: could superimpose the blue GMM estimations onto the red &amp; green pitch distributions as well, thereby gain some vertical space and use the space to make a larger depiction of the superimpositions.\n\nTable 1: Misspelling: Array -&gt; Araray\n\nTable 2: Highlighting of most interesting values (e.g. within-dataset maxima, cross-data maxima) is recommended. Do you have an explanation for why pitch countour stabilization (morphetic and masking) performs much worse in general than no stabilization? The only occassional improvements are seen in the cross-data results.\n\n161: &#34;except for color coding&#34;: you mean to say that with color coding it\&#39;s easier to identify a mode?\nFootnote 7: what do you mean by &#34;the reverse is not [...] true&#34;, that Ezil YeZema Silt is not always red? Is it a concensus that these colors are/should be used for the respective modes? This is not established in the text\n180, 190 &amp; 444: the spelling of Kidase/Qidase-bet varies.\n222: &#34;way larger&#34; is a colloquialism\n240: could you have chosen CREPE instead of pYIN? the choices for the pitch detection algorithm and classifier could have been elaborated.\n248: what does &#34;sliding&#34; refer to?\n253-254: what do you mean by &#34;during the performance&#34; and &#34;along the whole recording&#34;? Does the pitch drift occur the same for every audio? Did you analyze all 369 files for that?\n267: &#34;less number of parameters&#34; than what?\n270: &#34;as regard it as&#34; -&gt; &#34;regarding it as...&#34;?\n277: Why does each training run stop at 50 epochs? What is the reasoning behind this? Do the models converge?\n297: For the cross-dataset evaluation, did you not use a validation set? And did you test on the full dataset provided by [4]?\n304: you mention &#34;15 seconds&#34;, but in the results (Table 2) you show &#34;20 seconds&#34;. Apart from that, how much silence do these durations contain?\n312: 23 percentage points is NOT a &#34;relative small performance drop&#34;!\n313-314: from my interpretation, the combination of no calibration and no stabilization leads great results across the board. This configuration seems easiest to implement and process, and thus would be a good trade-off between maximum accuracy and efficiency.\n318: from the numbers, it is not evident how you make the claim that stabilization helps &#34;for most of the cases&#34;. The _opposite_ is true: for within-dataset, ALL values with stabilization are worse. For cross-data, only 5 out of 24 results are better with stabilization!\n320: with -&gt; within\n323: &#34;...has better classifcation accuracy _than the masking_&#34;, because this is not true for no stabilization.\n336-340: It\&#39;s not understandable how the described configuration was chosen for subsequent analysis! The noted performance gap between within-data and cross-data is smaller because this configuration heavily reduces the within-data performance compared to &#34;no calibration&#34; and &#34;no stabilization&#34;. The only case of improvement is in 10-sec cross-data, but you have already established around l.326 that YeZema Silt should be interpreted as a long-term song-level concept, even highlighting that YeZema Silt can be signified to some extent to a 20-sec excerpt as well. Looking at the 20-sec results for your selected configuration, the results are worse again.\n377-379: Is this not an expected observation, given the explanation of the three chanting modes in Section 2.2?\n380: &#34;Fig. 6&#34; -&gt; do you mean 4?\n400: remove second &#34;g_2&#34;\n\nMinor comment:\nIt would be nice to always keep the same order of mention: A E G for Araray, Ezil, and Ge\&#39;ez. You already have that in Table 1 and Table 3, and it could be reflected in the explanation of each mode (2.2), in Figure 4, Table 4, and also in l. 34-35, where you first introduce these modes.&#39;, &#39;review_3&#39;: &#39;= SUMMARY =\n\nThe authors investigate the Chant of the Tewahido (Ethiopian Orthodox) Church, which has its roots in the 6th century, with MIR.\nA primary goal is to classify three main types of the original Yaredawi YeZema Silt standard, which is shown to be feasible best with pitch contours. The pitch distributions that are characteristic for each of the three types are further analyzed with a GMM.\nA corresponding, public and manually curated audio dataset will be published upon publication of the paper. The experiments include a cross-corpus comparison with an existing smaller dataset.\n\n\n= EVALUATION =\n\nSTRENGTHS:\n\n(+) Interesting ideas for modelling the chant types by pitch distribution are presented.\n\n(+) Cross-corpus experiments and analysis.\n\n(+) The paper is generally well written and understandable.\n\nWEAKNESSES:\n\n(-) Some parts of the machine learning experiment are not clearly described or well designed (see major comments below). Especially, it is not fully clear which representation is fed into the neural network classifier.\n\n(-) All samples in the dataset originate from a single singer. (However, the cross-database analysis compensates this, partially.)\n\n\n= MAJOR COMMENTS =\n\n-General comment: It might be mentioned if all EOTC chants are monophonic or if there are also polyphonic ones.\n\n-Introduction, ll.56-57: &#34;based on the analysis of a series of EOTC Chants collected in Addis Ababa, 1975.&#34; -&gt; It might be pointed out that the recordings originate from 1975 and if also written documents/oral statements were analyzed in the study. (It is slightly confusing when reading, given that the papers are from the 1990s).\n\n-Section 2.1, ll.95-96: &#34;which need to be manipulated for the language&#34; -&gt; It is not clear why and in which way (especially) audio and video need to be manipulated.\n\n-Section 3: It would be interesting to give some information on the approximate number of EOTC schools and students in Ethiopia.\n\n-Section 4 reads as if only pitch-related features are taken into account. However, they are compared to MFCCs etc. This is slightly confusing when reading through the paper so it should be considered to rephrase.\n\n-Section 4.1: It is not fully clear, how the slope &#34;s&#34; is computed? Is it simply the regression line across all (stable) pitch values over time (i.e., minimizing the squared error)?\n\n-Section 4.1: It is not understandable, which representation is input to the neural network. Is it the frequency contour of the pitch (in Hz or normalized) or a histogram (distribution)? The same applies to Section 4.2. Moreover, it should be considered to have a development split for hyper-parameter optimization and early stopping.\n\n-Section 4.2: Taking other statistics besides the mean, e.g., standard deviation and slope, into account might give a meaningful performance boost.\n\n-Section 4.2: A 5-fold CV might not be the best choice for the within-dataset experiment if different chants were manually segmented beforehand, so that the same chant can appear in both training and test splits.\n\n-Section 5: &#34;The recording which has the highest average correlation with all the other recordings is considered as an anchor:&#34; -&gt; It should be discussed/reasoned whether this approach for pitch shifting is the optimal one. As an example, it might lead to a different solution taking into account the minimum summed-up absolute (or squared) pitch shift across all samples of the dataset.\n\n-Section 8: It might be meaningful to consider the usage restricted to educational purposes in the license when publishing the dataset.\n\n\n= MINOR COMMENTS =\n\n-General comment: &#34;chant&#34; is sometimes capitalized and sometimes not, which should be unified.\n-Section 2.1, l.94: &#34;[11]).&#34; -&gt; drop &#34;)&#34;\n-Footnote 5: &#34;data in reported&#34; -&gt; &#34;data reported in&#34;\n-Section 2.1, l.111: &#34;like some other&#34; -&gt; &#34;like for some other&#34;\n-Section 2.2, l.129: &#34;sorrow. [1].&#34; -&gt; &#34;sorrow [1].&#34;\n-Section 2.2, l.138: &#34;[3] stated&#34; -&gt; better: &#34;Shelemay et al. [3] stated&#34;\n-Section 3, l.176: &#34;departments.There&#34; -&gt; &#34;departments. There&#34;\n-Section 3, l.188: &#34;departemnts,&#34; -&gt; &#34;departments,&#34;\n-Section 3, l.219: &#34;On the other hand,&#34; -&gt; drop this\n-Section 5, l.345: &#34;musc&#34; -&gt; &#34;music&#34;\n-Section 5, l.353: &#34;contain&#34; -&gt; &#34;contains&#34;\n-References: The provided information (location, page numbers) should be consistent for papers in ISMIR proceedings.\n-References: [22] vs [23] -&gt; Capitalization of the conference name should be consistent.&#39;, &#39;meta_review&#39;: &#39;After the review process and the discussion phase, all reviewers and myself agree unanimously that this paper should be accepted for ISMIR 2024. However, the paper still needs to be revised before the camera-ready version. I strongly suggest the authors to carefully read all the reviews and consider all the suggestions proposed in them, since they will surely improve the current status of the paper. I will also summarize here the reasons for this recommendation and the aspects that need improvement.\n\nAll reviewers have agreed on the great value that the new dataset has for the ISMIR community, since, firstly, it introduces a tradition not previously studied by this community, especially coming from one of the most underrepresented continents in ISMIR, Africa, but also, and equally important, for the quality of the dataset, as described by the authors. Secondly, it offers an interesting analysis performed on this dataset, which proves its validity for MIR studies on it, and provides meaningful insights on its music tradition.\n\nHowever, the authors should improve the paper before submitting the camera-ready version on the following aspects.\n\nFirst, the readability of the paper should be improved, since the language at some points is not clear enough.\n\nSecondly, this paper will be the reference for this dataset in the future. As such, the music tradition should be better, more systematically described. Of course, it is impossible to cover all the dimensions of any music traditions in few pages of a conference paper. Therefore, the authors should focus on those elements of the tradition that are relevant for understanding the data and its potential for computational analysis. Other aspects, such as language or notation, should be minimized, at least at this stage of the dataset. (If in the future lyrics or scores are added to the dataset, those aspects should be then explained.)\n\nThirdly, the design of the experiment should be better explained. Since this not my main field of expertise, I suggest the authors to carefully read the comments by the three reviewers on this aspect, and to consider them for the improvement of the paper.\n\nFinally, details about reproducibility, regarding not only access to the data, but to the code, should be given.&#39;, &#39;author_changes&#39;: &#39;Our meta reviewer and all the three reviewers gave us detailed comments and professional suggestions which were invaluable to improve the quality of our paper. We acknowledge them by addressing their comments, questions and suggestions to the best of our knowledge. Our revision is summarized as below: \n\n- We have made a modification on our title so that it best fits the content. \n- We have made significant modifications on Sections 2, 4, and 5 including their corresponding tables and figures, which raised concerns. In this regard, we removed a figure (the previous Figure 1), modified other figures (e.g. previous figures 2 and 3, now figures 1 and 2) and reduced subsections (e.g. previous Section 2.1 and 2.3) into a single paragraph or part of a paragraph. The structure of Section 4.1 and 4.2 is also rearranged.\n- We have explained in more detail the reason for using masked pitch contour in analysis, although its performance on chanting mode classification is suboptimal (Section 5).\n- In connection to such modifications, some of the footnotes are modified, some others are incorporated into a background section and others deleted to keep the focus. The questions raised on removed contents will, therefore, be no more issues for future readers. \n- We have addressed spelling errors and inconsistencies. \n- We have added a link to access our proposed dataset. &#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>