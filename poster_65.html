

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO", "abstract": "We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO", "author_changes": "We would like to thank all the reviewers for their insightful reviews.\n\nIn this Camera-Ready version we did our best to address most of the highlighted reviewer notes:\n- Added a clear enumeration of our main contributions at the end of the introduction part.\n- Added details regarding the amount of raters that took part in our human evaluation.\n-Added intuition to the information bottleneck design choices (why we use the continuous latent of the first codebook and discard all other).\n- Made clarifications regarding concepts and notations such as \u0027Outpainting\u0027 or \u0027MLD(clf)\u0027.\n- Fixed invalid citation and grammar.\n", "authors": ["Tal, Or*", " Ziv, Alon", " Kreuk, Felix", " Gat, Itai", " Adi, Yossi"], "authors_and_affil": ["Or Tal (The Hebrew University of Jerusalem)*", " Alon Ziv (The Hebrew University of Jerusalem)", " Felix Kreuk (Bar-Ilan University)", " Itai Gat (Meta)", " Yossi Adi (The Hebrew University of Jerusalem)"], "channel_url": "https://ismir2024.slack.com/archives/C07UQ1DBBS6", "day": "1", "keywords": ["Generative Tasks -\u003e music and audio synthesis", "MIR and machine learning for musical acoustics -\u003e applications of musical acoustics to signal synthesis; MIR tasks -\u003e music generation"], "long_presentation": "FALSE", "meta_review": "We have thoroughly reviewed this paper from various aspects, and we all believe that the paper is ready for publication with minor revisions. For more details, please see the comments of each reviewer. It is also highly recommended that you add more details about the user study", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/view?usp=drive_link", "poster_pdf": "https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/view?usp=drive_link", "review_1": "This paper introduces a novel approach to a controllable text-to-music generation model. It leverages pretrained models for chord estimation, f0 classification, and source separation. The preprocessed signals from these models are fed into a conditional flow matching model, which consists of a transformer with residual connections, to enable chord conditioning, melody conditioning, and audio conditioning, along with textual inputs. Classifier-free guidance is employed to ensure control adherence during the inference stage. The model was evaluated both quantitatively and qualitatively. Experimental results demonstrate that the model maintains generation quality while offering a high degree of controllability.\n\nThe paper is well-organized and clearly written. Related works are cited, the experimental design is clearly described, and the evaluation metrics are reasonable. Well-designed experiments and thorough evaluation highlight its strengths. Although the model was trained with proprietary data, the paper provides detailed descriptions of their experiments, and the inclusion of code and pretrained models facilitates reproducible research. Notably, this work bridges the gap between music analysis research from the MIR community and data-driven generative models.\n\nI have three minor questions/comments:\n* Line 417: Why is there a significant gap in FAD? Is it due to the model characteristics or the training scheme? Addressing this will provide more insights into evaluation metrics for generative models.\n* Emboldening the best performance in the table would enhance readability.\n* The reference papers are missing conference names. Adding these would improve the completeness of the citations.", "review_2": "Promising article on text-to-music generative AI, extending model customizability with audio and symbolic input. The algorithm design and details are the strong points here, and while the blind process does not allow reviewers to access the code, the provided audio examples show that it appears to be quite successful in scope, even if improvements could be made in terms of audio quality. This is the type of research where the novelty dictates that human subjective evaluation is paramount, but the authors decided to make it an afterthought and provide little to no detail on this part of the evaluation study. Furthermore, this part of the results is the one with more generous and misleading interpretations, as it is apparent that on all accounts except the one that could not be compared (including drum input), JASCO fared worse (still it is stated \"however, when considering melody conditioning, reaches significantly better scores\"). The fact that it did not fare better is not the problem, the problem is the description of the process. The article\u0027s structure is unconventional and I fail to understand why related work appears between analysis and discussion, and why analysis is more rushed than it should be. It is also not clear why the authors chose to separate results from analysis, as it is not as clear-cut as it usually is. There are some minor typos in the text, particularly singular-plural problems (eg lines 322, 445) and excess spaces (eg lines 153, 249), which should still be revised.", "review_3": "The JASCO paper introduces an innovative model that advances the text-to-music generation domain, employing Flow Matching and hybrid conditioning to enable detailed control over music generation. The model\u0027s integration of symbolic and audio inputs through advanced conditioning mechanisms presents a pioneering approach that enhances the quality and versatility of generated music. The samples showcased on the demo page demonstrate impressively good performance, adhering well to the specified controls.\n\nWhile the paper is generally well-written and informative, there are areas where further details could enhance understanding and application:\n\nIn Section 3.1, the paper would benefit from a more detailed explanation of why specific pre-trained models such as the multi-F0 classifier, the source separation model, and the melody extraction models were selected. For example, it could provide some discussions on their performance metrics, computational efficiency, or effectiveness in diverse settings. Additionally, elaborating on any adaptations made to integrate these pre-trained models into the JASCO framework would be useful. Similarly in Section 3.3, it would be helpful if the paper clarifies why the ODE solver was chosen for the inference process.\n\nIn Section 3.1 Audio, the process for converting discrete tokens back into continuous vectors needs clarification. Is this achieved through interpolation or direct mapping? Furthermore, I am curious about why the authors choose the continuous latent representation converted from the first codebook, rather than using the continuous latent tensor directly from the EnCodec model.\n\nIn Section 3.2 on Inpainting/Outpainting, the term \u0027Outpainting\u0027 is used without a definition, which might lead to confusion. \n\nIn Section 4.1, the paper could include more details about the human study, such as the number of raters involved and the number of samples each assessed.\n\nIn Section 5, it would be helpful if the paper addressed the limitations or common errors associated with those pre-trained models within the JASCO system. For example, how might the errors in chord extraction or melody extraction affect the final generations?\n\nOne minor correction in Table 2, I am not sure what \"Mld(clf)\" refers to?\n\nYou might also want to fix title \"Mo\\\u02c6 usai\" in reference [4] \"F. Schneider, O. Kamal, Z. Jin, and B. Sch\u00f6lkopf, \u201cMo\\\u02c6 usai: Text-to-music generation with long-context latent diffusion,\u201d arXiv preprint arXiv:2301.11757, 2023.\"\n", "session": ["2"], "slack_channel": "p2-07-joint-audio-and", "slides_pdf": "https://docs.google.com/presentation/d/1-cpUayRz1aCuMc02gvqsuo83wKJ3mU16/edit?usp=drive_web\u0026ouid=102045217384450827259\u0026rtpof=true", "title": "Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation", "video": "https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Or Tal (The Hebrew University of Jerusalem)*"
               class="text-muted"
            >Or Tal (The Hebrew University of Jerusalem)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Alon Ziv (The Hebrew University of Jerusalem)"
               class="text-muted"
            > Alon Ziv (The Hebrew University of Jerusalem)</a
            >,
            
            <a href="papers.html?filter=authors&search= Felix Kreuk (Bar-Ilan University)"
               class="text-muted"
            > Felix Kreuk (Bar-Ilan University)</a
            >,
            
            <a href="papers.html?filter=authors&search= Itai Gat (Meta)"
               class="text-muted"
            > Itai Gat (Meta)</a
            >,
            
            <a href="papers.html?filter=authors&search= Yossi Adi (The Hebrew University of Jerusalem)"
               class="text-muted"
            > Yossi Adi (The Hebrew University of Jerusalem)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UQ1DBBS6" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p2-07-joint-audio-and</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; music and audio synthesis"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; music and audio synthesis</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR and machine learning for musical acoustics -&gt; applications of musical acoustics to signal synthesis; MIR tasks -&gt; music generation"
                    class="text-secondary text-decoration-none"
            >MIR and machine learning for musical acoustics -&gt; applications of musical acoustics to signal synthesis; MIR tasks -&gt; music generation</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://docs.google.com/presentation/d/1-cpUayRz1aCuMc02gvqsuo83wKJ3mU16/edit?usp=drive_web&amp;ouid=102045217384450827259&amp;rtpof=true" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>We have thoroughly reviewed this paper from various aspects, and we all believe that the paper is ready for publication with minor revisions. For more details, please see the comments of each reviewer. It is also highly recommended that you add more details about the user study</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This paper introduces a novel approach to a controllable text-to-music generation model. It leverages pretrained models for chord estimation, f0 classification, and source separation. The preprocessed signals from these models are fed into a conditional flow matching model, which consists of a transformer with residual connections, to enable chord conditioning, melody conditioning, and audio conditioning, along with textual inputs. Classifier-free guidance is employed to ensure control adherence during the inference stage. The model was evaluated both quantitatively and qualitatively. Experimental results demonstrate that the model maintains generation quality while offering a high degree of controllability.</p>
<p>The paper is well-organized and clearly written. Related works are cited, the experimental design is clearly described, and the evaluation metrics are reasonable. Well-designed experiments and thorough evaluation highlight its strengths. Although the model was trained with proprietary data, the paper provides detailed descriptions of their experiments, and the inclusion of code and pretrained models facilitates reproducible research. Notably, this work bridges the gap between music analysis research from the MIR community and data-driven generative models.</p>
<p>I have three minor questions/comments:
* Line 417: Why is there a significant gap in FAD? Is it due to the model characteristics or the training scheme? Addressing this will provide more insights into evaluation metrics for generative models.
* Emboldening the best performance in the table would enhance readability.
* The reference papers are missing conference names. Adding these would improve the completeness of the citations.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>Promising article on text-to-music generative AI, extending model customizability with audio and symbolic input. The algorithm design and details are the strong points here, and while the blind process does not allow reviewers to access the code, the provided audio examples show that it appears to be quite successful in scope, even if improvements could be made in terms of audio quality. This is the type of research where the novelty dictates that human subjective evaluation is paramount, but the authors decided to make it an afterthought and provide little to no detail on this part of the evaluation study. Furthermore, this part of the results is the one with more generous and misleading interpretations, as it is apparent that on all accounts except the one that could not be compared (including drum input), JASCO fared worse (still it is stated "however, when considering melody conditioning, reaches significantly better scores"). The fact that it did not fare better is not the problem, the problem is the description of the process. The article's structure is unconventional and I fail to understand why related work appears between analysis and discussion, and why analysis is more rushed than it should be. It is also not clear why the authors chose to separate results from analysis, as it is not as clear-cut as it usually is. There are some minor typos in the text, particularly singular-plural problems (eg lines 322, 445) and excess spaces (eg lines 153, 249), which should still be revised.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>The JASCO paper introduces an innovative model that advances the text-to-music generation domain, employing Flow Matching and hybrid conditioning to enable detailed control over music generation. The model's integration of symbolic and audio inputs through advanced conditioning mechanisms presents a pioneering approach that enhances the quality and versatility of generated music. The samples showcased on the demo page demonstrate impressively good performance, adhering well to the specified controls.</p>
<p>While the paper is generally well-written and informative, there are areas where further details could enhance understanding and application:</p>
<p>In Section 3.1, the paper would benefit from a more detailed explanation of why specific pre-trained models such as the multi-F0 classifier, the source separation model, and the melody extraction models were selected. For example, it could provide some discussions on their performance metrics, computational efficiency, or effectiveness in diverse settings. Additionally, elaborating on any adaptations made to integrate these pre-trained models into the JASCO framework would be useful. Similarly in Section 3.3, it would be helpful if the paper clarifies why the ODE solver was chosen for the inference process.</p>
<p>In Section 3.1 Audio, the process for converting discrete tokens back into continuous vectors needs clarification. Is this achieved through interpolation or direct mapping? Furthermore, I am curious about why the authors choose the continuous latent representation converted from the first codebook, rather than using the continuous latent tensor directly from the EnCodec model.</p>
<p>In Section 3.2 on Inpainting/Outpainting, the term 'Outpainting' is used without a definition, which might lead to confusion. </p>
<p>In Section 4.1, the paper could include more details about the human study, such as the number of raters involved and the number of samples each assessed.</p>
<p>In Section 5, it would be helpful if the paper addressed the limitations or common errors associated with those pre-trained models within the JASCO system. For example, how might the errors in chord extraction or melody extraction affect the final generations?</p>
<p>One minor correction in Table 2, I am not sure what "Mld(clf)" refers to?</p>
<p>You might also want to fix title "Mo\ˆ usai" in reference [4] "F. Schneider, O. Kamal, Z. Jin, and B. Schölkopf, “Mo\ˆ usai: Text-to-music generation with long-context latent diffusion,” arXiv preprint arXiv:2301.11757, 2023."</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We would like to thank all the reviewers for their insightful reviews.</p>
<p>In this Camera-Ready version we did our best to address most of the highlighted reviewer notes:
- Added a clear enumeration of our main contributions at the end of the introduction part.
- Added details regarding the amount of raters that took part in our human evaluation.
-Added intuition to the information bottleneck design choices (why we use the continuous latent of the first codebook and discard all other).
- Made clarifications regarding concepts and notations such as 'Outpainting' or 'MLD(clf)'.
- Fixed invalid citation and grammar.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;65&#39;, &#39;session&#39;: &#39;2&#39;, &#39;position&#39;: &#39;08&#39;, &#39;forum&#39;: &#39;65&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/17bUEpA6WnKZn6bcPrHmY82DqvQ8_VXrd/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Joint Audio and Symbolic Audio Conditioning For Temporally Controlled Text-to-Music Generation&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Tal, Or*&#39;, &#39; Ziv, Alon&#39;, &#39; Kreuk, Felix&#39;, &#39; Gat, Itai&#39;, &#39; Adi, Yossi&#39;], &#39;authors_and_affil&#39;: [&#39;Or Tal (The Hebrew University of Jerusalem)*&#39;, &#39; Alon Ziv (The Hebrew University of Jerusalem)&#39;, &#39; Felix Kreuk (Bar-Ilan University)&#39;, &#39; Itai Gat (Meta)&#39;, &#39; Yossi Adi (The Hebrew University of Jerusalem)&#39;], &#39;keywords&#39;: [&#39;Generative Tasks -&gt; music and audio synthesis&#39;, &#39;MIR and machine learning for musical acoustics -&gt; applications of musical acoustics to signal synthesis; MIR tasks -&gt; music generation&#39;], &#39;abstract&#39;: &#39;We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO&#39;, &#39;TLDR&#39;: &#39;We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This al- lows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page: https://pages.https://pages.cs.huji.ac.il/adiyoss-lab/JASCO&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1kRm2KDVPxnlLMlVFBfI1KvyXlePrLr19/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;2&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1xXUD7VbtkvkfnQXtj81R-7ICInSEFIEB/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/video/captions/edit?id=1-5sdVfvISw1c21AGYwr4wjQ3ke2__1VG&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UQ1DBBS6&#39;, &#39;slack_channel&#39;: &#39;p2-07-joint-audio-and&#39;, &#39;slides_pdf&#39;: &#39;https://docs.google.com/presentation/d/1-cpUayRz1aCuMc02gvqsuo83wKJ3mU16/edit?usp=drive_web&amp;ouid=102045217384450827259&amp;rtpof=true&#39;, &#39;day&#39;: &#39;1&#39;, &#39;review_1&#39;: &#39;This paper introduces a novel approach to a controllable text-to-music generation model. It leverages pretrained models for chord estimation, f0 classification, and source separation. The preprocessed signals from these models are fed into a conditional flow matching model, which consists of a transformer with residual connections, to enable chord conditioning, melody conditioning, and audio conditioning, along with textual inputs. Classifier-free guidance is employed to ensure control adherence during the inference stage. The model was evaluated both quantitatively and qualitatively. Experimental results demonstrate that the model maintains generation quality while offering a high degree of controllability.\n\nThe paper is well-organized and clearly written. Related works are cited, the experimental design is clearly described, and the evaluation metrics are reasonable. Well-designed experiments and thorough evaluation highlight its strengths. Although the model was trained with proprietary data, the paper provides detailed descriptions of their experiments, and the inclusion of code and pretrained models facilitates reproducible research. Notably, this work bridges the gap between music analysis research from the MIR community and data-driven generative models.\n\nI have three minor questions/comments:\n* Line 417: Why is there a significant gap in FAD? Is it due to the model characteristics or the training scheme? Addressing this will provide more insights into evaluation metrics for generative models.\n* Emboldening the best performance in the table would enhance readability.\n* The reference papers are missing conference names. Adding these would improve the completeness of the citations.&#39;, &#39;review_2&#39;: &#39;Promising article on text-to-music generative AI, extending model customizability with audio and symbolic input. The algorithm design and details are the strong points here, and while the blind process does not allow reviewers to access the code, the provided audio examples show that it appears to be quite successful in scope, even if improvements could be made in terms of audio quality. This is the type of research where the novelty dictates that human subjective evaluation is paramount, but the authors decided to make it an afterthought and provide little to no detail on this part of the evaluation study. Furthermore, this part of the results is the one with more generous and misleading interpretations, as it is apparent that on all accounts except the one that could not be compared (including drum input), JASCO fared worse (still it is stated &#34;however, when considering melody conditioning, reaches significantly better scores&#34;). The fact that it did not fare better is not the problem, the problem is the description of the process. The article\&#39;s structure is unconventional and I fail to understand why related work appears between analysis and discussion, and why analysis is more rushed than it should be. It is also not clear why the authors chose to separate results from analysis, as it is not as clear-cut as it usually is. There are some minor typos in the text, particularly singular-plural problems (eg lines 322, 445) and excess spaces (eg lines 153, 249), which should still be revised.&#39;, &#39;review_3&#39;: &#39;The JASCO paper introduces an innovative model that advances the text-to-music generation domain, employing Flow Matching and hybrid conditioning to enable detailed control over music generation. The model\&#39;s integration of symbolic and audio inputs through advanced conditioning mechanisms presents a pioneering approach that enhances the quality and versatility of generated music. The samples showcased on the demo page demonstrate impressively good performance, adhering well to the specified controls.\n\nWhile the paper is generally well-written and informative, there are areas where further details could enhance understanding and application:\n\nIn Section 3.1, the paper would benefit from a more detailed explanation of why specific pre-trained models such as the multi-F0 classifier, the source separation model, and the melody extraction models were selected. For example, it could provide some discussions on their performance metrics, computational efficiency, or effectiveness in diverse settings. Additionally, elaborating on any adaptations made to integrate these pre-trained models into the JASCO framework would be useful. Similarly in Section 3.3, it would be helpful if the paper clarifies why the ODE solver was chosen for the inference process.\n\nIn Section 3.1 Audio, the process for converting discrete tokens back into continuous vectors needs clarification. Is this achieved through interpolation or direct mapping? Furthermore, I am curious about why the authors choose the continuous latent representation converted from the first codebook, rather than using the continuous latent tensor directly from the EnCodec model.\n\nIn Section 3.2 on Inpainting/Outpainting, the term \&#39;Outpainting\&#39; is used without a definition, which might lead to confusion. \n\nIn Section 4.1, the paper could include more details about the human study, such as the number of raters involved and the number of samples each assessed.\n\nIn Section 5, it would be helpful if the paper addressed the limitations or common errors associated with those pre-trained models within the JASCO system. For example, how might the errors in chord extraction or melody extraction affect the final generations?\n\nOne minor correction in Table 2, I am not sure what &#34;Mld(clf)&#34; refers to?\n\nYou might also want to fix title &#34;Mo\\ˆ usai&#34; in reference [4] &#34;F. Schneider, O. Kamal, Z. Jin, and B. Schölkopf, “Mo\\ˆ usai: Text-to-music generation with long-context latent diffusion,” arXiv preprint arXiv:2301.11757, 2023.&#34;\n&#39;, &#39;meta_review&#39;: &#39;We have thoroughly reviewed this paper from various aspects, and we all believe that the paper is ready for publication with minor revisions. For more details, please see the comments of each reviewer. It is also highly recommended that you add more details about the user study&#39;, &#39;author_changes&#39;: &#34;We would like to thank all the reviewers for their insightful reviews.\n\nIn this Camera-Ready version we did our best to address most of the highlighted reviewer notes:\n- Added a clear enumeration of our main contributions at the end of the introduction part.\n- Added details regarding the amount of raters that took part in our human evaluation.\n-Added intuition to the information bottleneck design choices (why we use the continuous latent of the first codebook and discard all other).\n- Made clarifications regarding concepts and notations such as &#39;Outpainting&#39; or &#39;MLD(clf)&#39;.\n- Fixed invalid citation and grammar.\n&#34;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>