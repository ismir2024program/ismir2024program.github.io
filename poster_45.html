

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Noelia N Luna-Barahona (Universidad de Alicante)"
               class="text-muted"
            >Noelia N Luna-Barahona (Universidad de Alicante)</a
            >,
            
            <a href="papers.html?filter=authors&search= Adrián Roselló (Universidad de Alicante)"
               class="text-muted"
            > Adrián Roselló (Universidad de Alicante)</a
            >,
            
            <a href="papers.html?filter=authors&search= María Alfaro-Contreras (University of Alicante)"
               class="text-muted"
            > María Alfaro-Contreras (University of Alicante)</a
            >,
            
            <a href="papers.html?filter=authors&search= David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana)"
               class="text-muted"
            > David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana)</a
            >,
            
            <a href="papers.html?filter=authors&search= Jorge Calvo-Zaragoza (University of Alicante)*"
               class="text-muted"
            > Jorge Calvo-Zaragoza (University of Alicante)*</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UM5T53C5" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p3-13-unsupervised-synthetic-to</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Applications -&gt; digital libraries and archives"
                    class="text-secondary text-decoration-none"
            >Applications -&gt; digital libraries and archives</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; optical music recognition"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; optical music recognition</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1uAPEcIw9skchg5UQxmgCkLc1BsClOgFF/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1bzCCw1CM6f0GbA-yFkX4XjLxGClfb6P0/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/17uyevRjXv8eJp77erGQKQP9Z9AzlI89B/preview" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>All reviewers agree that the presented unsupervised domain adaptation framework to improve Optical Music Recognition performance would be of interest to the ISMIR community, would create discourse, possibly inspire work in other parallel areas, and in that sense the topic has high relevance to the conference. The readability of the paper is good, novelty and the scientific contributions are sufficient for publication.</p>
<p>We recommend acceptance of the paper provided the following issues are addressed in the final version.
- Please see all reviews for detailed suggestions and changes to improve the clarity of the paper.
- Code be made available as mentioned in the paper (but was not provided for this review).
- Add comparison with previous works (Reviewer 1; R1) to the extent possible. Include a comparative discussion of the two references mentioned.
- Consider the work in the papers listed by R3 on leveraging batch normalization statistics for domain adaptation and higher-order moment matching to tie in work in that area for a better grounding of presented approach. Make it clear that idea of domain adaptation is not being newly introduced in this work.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This paper aims to create more practical OMR system. Most ML research suffers from insufficient 'good' dataset, and this is especially true for OMR research. To address this, the authors present an unsupervised Domain Adaptation method using a synthetic dataset. They carefully designed the loss function, and the entire technical process is explained in detail in the paper. Compared to previous works that utilize DA, this work targets an end-to-end approach, and the results clarified the effectiveness of their method. </p>
<p>While this paper uses different DA architectures and loss functions, I feel that it lacks a comparison with previous works. What are the differences and similarities between this work and "Domain adaptation for staff-region retrieval of music score images" by Castellanos, et al., and "Real world music object recognition" by Tuggener, et al.?</p>
<p>The latter also deals with the sheet images written in CWMN, while this paper focuses on Mensural collections. Are you planning to adapt this method to the most common CWMN collections? If successful, this approach could make a more practical contribution to the MIR community, in my opinion.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <h1>ERROR!</h1></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>The article shows how to use a combination of labeled synthetic music notation data for training and unlabeled real images for domain adaptation in order to build an OMR pipeline that can deal with sheet music collections without having to label any. The relationship between synthetic and real data is one of the vital issues for OMR, because labeled training data is still very expensive. While the domain adaptation still is far from solving this problem (based on the results in Table 2), it certainly does help significantly.  I want to especially commend that the article performs these experiments not on one collection, but across five diffferent mensural corpora. </p>
<p>The domain adaptation loss consists of two terms that each implement a clever trick. The adaptation loss term takes advantage of the batch normalization mechanism that remembers a summary of the incoming representations via their mean and standard deviations that are used to perform the batch normalization during the supervised training phase, and during fine-tuning (if I understand this correctly), it adapts the weights of the upstream layers so that the batch norm statistics coming from the in-domain data match these statistics from the supervised training phase as closely as possible (with KL divergence), so that the distributions going to the downstream layers look as close to what the network is used to from the training phase. The regularization term then prevents pathological collapses of the adaptation loss by encouraging predictions that look like plausible OMR outputs.</p>
<p>While these are clearly workable ideas, the paper suffers from a bit of in-domain OMR tunnel vision. By not considering related methods in more broad computer vision terms, the paper implicitly overclaims that the domain adaptation term is an original contribution. This is important and in my view must be fixed for camera-ready, if accepted. The widely cited methods that leverage batch norm statistics for unuspervised domain adaptation are not exactly the same as the proposed domain adaptation term, so this merits also some light discussion of at least what advantages and disadvantages the choices made in this paper have compared to existing methods (new experiments with these existing methods are perhaps a bit unrealistic for camera-ready, and not necessarily valuable anyway).</p>
<p>In order for the paper to move from showing that the idea has potential for OMR into actionable intelligence, I would also recommend a best-of-both-worlds experiment that combines domain adaptation with some amounts of in-domain data to show whether the suggested ceiling is in fact really as fragile as the glass metaphor suggests: does adding already a little bit of in-domain data help significantly?</p>
<p>The error analysis could also benefit from showing if there are errors that domain adaptation introduces. The delta term in Table 2 has in fact two parts — errors fixed with domain adaptation, and errors introduced by it (with the second number likely being much smaller). However, especially if some of the test domains (=different datasets) exhibits a large number of errors introduced by the method, this could offer valuable qualitative insights into the limitations of the presented method — and suggest directions for improving it towards applicability.</p>
<p>I recommend the paper for acceptance, especially because it is a good attempt at tackling this important problem of synthetic-to-real domain adaptation in OMR with purely synthetic training data, but with the important caveat that the domain adaptation using batch norm matching should not be presented as a new thing — it has already been explored repeatedly throughout the past years (see the comments on related work above), hence a weak and not strong accept. This is a serious omission, but it can be easily fixed in camera-ready and in my view does not detract much from the long-term value of the paper. It is a great first step to applying these techniques for OMR. I am looking forward to next steps that moves the results of this method closer to the results with in-domain training data, especially with an eye of adapting other techniques used for this in computer vision.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>In response to the reviewers' comments, we made the following changes to our paper:</p>
<ol>
<li>We corrected minor grammatical and typographical errors.</li>
<li>We ensured all references are accurate and properly formatted.</li>
<li>We revised Fig. 1 to improve readability.</li>
<li>We clarified and expanded explanations in several sections as requested by reviewers.</li>
</ol>
<p>We appreciate the reviewers' valuable feedback.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;45&#39;, &#39;session&#39;: &#39;3&#39;, &#39;position&#39;: &#39;14&#39;, &#39;forum&#39;: &#39;45&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1pS1if-khfQb_HZLoWqkpYsGqsWCTeT5J/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Luna-Barahona, Noelia N&#39;, &#39; Roselló, Adrián&#39;, &#39; Alfaro-Contreras, María&#39;, &#39; Rizo, David&#39;, &#39; Calvo-Zaragoza, Jorge*&#39;], &#39;authors_and_affil&#39;: [&#39;Noelia N Luna-Barahona (Universidad de Alicante)&#39;, &#39; Adrián Roselló (Universidad de Alicante)&#39;, &#39; María Alfaro-Contreras (University of Alicante)&#39;, &#39; David Rizo (University of Alicante. Instituto Superior de Enseñanzas Artísrticas de la Comunidad Valenciana)&#39;, &#39; Jorge Calvo-Zaragoza (University of Alicante)*&#39;], &#39;keywords&#39;: [&#39;Applications -&gt; digital libraries and archives&#39;, &#39;MIR tasks -&gt; optical music recognition&#39;], &#39;abstract&#39;: &#39;The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.&#39;, &#39;TLDR&#39;: &#39;The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1bzCCw1CM6f0GbA-yFkX4XjLxGClfb6P0/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;3&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1uAPEcIw9skchg5UQxmgCkLc1BsClOgFF/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/17uyevRjXv8eJp77erGQKQP9Z9AzlI89B/view?usp=sharing&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UM5T53C5&#39;, &#39;slack_channel&#39;: &#39;p3-13-unsupervised-synthetic-to&#39;, &#39;day&#39;: &#39;2&#39;, &#39;review_1&#39;: &#39;This paper aims to create more practical OMR system. Most ML research suffers from insufficient \&#39;good\&#39; dataset, and this is especially true for OMR research. To address this, the authors present an unsupervised Domain Adaptation method using a synthetic dataset. They carefully designed the loss function, and the entire technical process is explained in detail in the paper. Compared to previous works that utilize DA, this work targets an end-to-end approach, and the results clarified the effectiveness of their method. \n\nWhile this paper uses different DA architectures and loss functions, I feel that it lacks a comparison with previous works. What are the differences and similarities between this work and &#34;Domain adaptation for staff-region retrieval of music score images&#34; by Castellanos, et al., and &#34;Real world music object recognition&#34; by Tuggener, et al.?\n\nThe latter also deals with the sheet images written in CWMN, while this paper focuses on Mensural collections. Are you planning to adapt this method to the most common CWMN collections? If successful, this approach could make a more practical contribution to the MIR community, in my opinion.&#39;, &#39;review_2&#39;: &#39;#ERROR!&#39;, &#39;review_3&#39;: &#39;The article shows how to use a combination of labeled synthetic music notation data for training and unlabeled real images for domain adaptation in order to build an OMR pipeline that can deal with sheet music collections without having to label any. The relationship between synthetic and real data is one of the vital issues for OMR, because labeled training data is still very expensive. While the domain adaptation still is far from solving this problem (based on the results in Table 2), it certainly does help significantly.  I want to especially commend that the article performs these experiments not on one collection, but across five diffferent mensural corpora. \n\nThe domain adaptation loss consists of two terms that each implement a clever trick. The adaptation loss term takes advantage of the batch normalization mechanism that remembers a summary of the incoming representations via their mean and standard deviations that are used to perform the batch normalization during the supervised training phase, and during fine-tuning (if I understand this correctly), it adapts the weights of the upstream layers so that the batch norm statistics coming from the in-domain data match these statistics from the supervised training phase as closely as possible (with KL divergence), so that the distributions going to the downstream layers look as close to what the network is used to from the training phase. The regularization term then prevents pathological collapses of the adaptation loss by encouraging predictions that look like plausible OMR outputs.\n\nWhile these are clearly workable ideas, the paper suffers from a bit of in-domain OMR tunnel vision. By not considering related methods in more broad computer vision terms, the paper implicitly overclaims that the domain adaptation term is an original contribution. This is important and in my view must be fixed for camera-ready, if accepted. The widely cited methods that leverage batch norm statistics for unuspervised domain adaptation are not exactly the same as the proposed domain adaptation term, so this merits also some light discussion of at least what advantages and disadvantages the choices made in this paper have compared to existing methods (new experiments with these existing methods are perhaps a bit unrealistic for camera-ready, and not necessarily valuable anyway).\n\nIn order for the paper to move from showing that the idea has potential for OMR into actionable intelligence, I would also recommend a best-of-both-worlds experiment that combines domain adaptation with some amounts of in-domain data to show whether the suggested ceiling is in fact really as fragile as the glass metaphor suggests: does adding already a little bit of in-domain data help significantly?\n\nThe error analysis could also benefit from showing if there are errors that domain adaptation introduces. The delta term in Table 2 has in fact two parts — errors fixed with domain adaptation, and errors introduced by it (with the second number likely being much smaller). However, especially if some of the test domains (=different datasets) exhibits a large number of errors introduced by the method, this could offer valuable qualitative insights into the limitations of the presented method — and suggest directions for improving it towards applicability.\n\nI recommend the paper for acceptance, especially because it is a good attempt at tackling this important problem of synthetic-to-real domain adaptation in OMR with purely synthetic training data, but with the important caveat that the domain adaptation using batch norm matching should not be presented as a new thing — it has already been explored repeatedly throughout the past years (see the comments on related work above), hence a weak and not strong accept. This is a serious omission, but it can be easily fixed in camera-ready and in my view does not detract much from the long-term value of the paper. It is a great first step to applying these techniques for OMR. I am looking forward to next steps that moves the results of this method closer to the results with in-domain training data, especially with an eye of adapting other techniques used for this in computer vision.&#39;, &#39;meta_review&#39;: &#39;All reviewers agree that the presented unsupervised domain adaptation framework to improve Optical Music Recognition performance would be of interest to the ISMIR community, would create discourse, possibly inspire work in other parallel areas, and in that sense the topic has high relevance to the conference. The readability of the paper is good, novelty and the scientific contributions are sufficient for publication.\n\nWe recommend acceptance of the paper provided the following issues are addressed in the final version.\n- Please see all reviews for detailed suggestions and changes to improve the clarity of the paper.\n- Code be made available as mentioned in the paper (but was not provided for this review).\n- Add comparison with previous works (Reviewer 1; R1) to the extent possible. Include a comparative discussion of the two references mentioned.\n- Consider the work in the papers listed by R3 on leveraging batch normalization statistics for domain adaptation and higher-order moment matching to tie in work in that area for a better grounding of presented approach. Make it clear that idea of domain adaptation is not being newly introduced in this work.\n&#39;, &#39;author_changes&#39;: &#34;In response to the reviewers&#39; comments, we made the following changes to our paper:\n\n1. We corrected minor grammatical and typographical errors.\n2. We ensured all references are accurate and properly formatted.\n3. We revised Fig. 1 to improve readability.\n4. We clarified and expanded explanations in several sections as requested by reviewers.\n\nWe appreciate the reviewers&#39; valuable feedback.&#34;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>