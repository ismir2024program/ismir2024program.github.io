

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Just Label the Repeats for In-The-Wild Audio-to-Score Alignment
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Bukey, Irmak*"
               class="text-muted"
            >Bukey, Irmak*</a
            >,
            
            <a href="papers.html?filter=authors&search= Feffer, Michael"
               class="text-muted"
            > Feffer, Michael</a
            >,
            
            <a href="papers.html?filter=authors&search= Donahue, Chris"
               class="text-muted"
            > Donahue, Chris</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07UHEN7T2S" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p7-16-just-label-the</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; multimodality; MIR tasks -&gt; optical music recognition; MIR tasks -&gt; pattern matching and detection"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; multimodality; MIR tasks -&gt; optical music recognition; MIR tasks -&gt; pattern matching and detection</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; alignment, synchronization, and score following"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; alignment, synchronization, and score following</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% → 82%).</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1UvKKzTuptPSWcUG8eLtoRAW27nLrJMCU/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/10SNBKkgmZ93IS5WNXThWLvpxRXXQpTk_/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1w4trgsz-MIdKaRspq3X0EEUYFMWbYweP/preview" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The paper proposes a simple solution to aligning audio files with sheet music that have structural differences (such as repeats or no repeats): simply annotate them. The authors present a quick workflow for this task and demonstrate the effectiveness of the extra annotations (particularly the repeats) in their evaluation and provided videos.</p>
<p>Main Strengths:
- Clarity and Practicality: The paper is well-written, easy to understand, and presents a practical solution to the problem of aligning audio files with sheet music, especially when there are structural differences such as repeats or jumps.
- Effectiveness: Demonstrates a clear improvement over baseline systems when using annotated information on repeats and jumps, which highlights the practical application and potential benefits in real-world scenarios.
- Human-in-the-Loop Approach and Workflow Integration: Emphasises the value of human annotations in improving alignment accuracy, which is often overlooked in favour of fully automatic solutions. This approach is both quick and low in human effort. The workflow is cohesive and integrates well with existing state-of-the-art approaches, making it a robust engineering solution.
- Reproducibility: The promise to release the code upon acceptance ensures that the results can be reproduced and verified by others in the community.</p>
<p>Main Weaknesses:<br />
- Lack of Novelty: The approach lacks significant novelty from a research perspective. The idea of labelling repeats and using annotated jumps is not new, and the improvements over existing methods are not groundbreaking.
- Evaluation Scope: The evaluations, while showing improvements, lack depth in analysis. For instance, there is a need for a more detailed explanation of the significant accuracy jump attributed to feature representations.
- Dataset Limitations: The datasets used for evaluation are limited, particularly in the diversity of music types. The evaluation predominantly focuses on piano music, which may not fully represent the system’s capabilities.</p>
<p>Further Comments:
- The measure-aware alignment and evaluation make intuitive sense but could be perceived as arbitrary due to the variability in measure lengths.
- The proposed system’s baseline comparison should also consider using similar features to those of existing methods to ensure a fair evaluation.</p>
<p>Requested Improvements:<br />
- References: The references are inconsistently formatted and incomplete in some cases. Please resolve this for a potential final version. Also, some additional references of papers addressing jumps/repeats in the alignment context should be added (see the individual reviews).
- Features: Improvements in feature representation claims should be supported by evaluations on larger datasets like SMR, in addition to the relatively small M13 dataset. The paper could benefit from a more detailed analysis of why the proposed feature representations supposedly lead to significant improvements.</p>
<p>Please also consider the more detailed comments by the individual reviewers.</p>
<p>Despite the lack of significant novelty, the paper presents a practical and effective solution to audio-to-sheet music alignment. Its clear writing, practical workflow, and demonstrated improvements in alignment accuracy make it a valuable contribution.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>In this paper, the authors propose an audio-to-music-sheet synchronization system that combines several state-of-the-art approaches to detect notes and staff lines [16,17], measure positions [21], music transcription [3], and standard DTW [22]. On top of this engineering solution, the authors developed an interface to annotate jumps and repeats and feed the system with this information.</p>
<p>Evaluation has been performed at the measure level using the MeSA-13 and a subset from the SMR dataset. The proposed system is compared with the method in [17], which uses Hierarchical DTW to automatically account for the jumps and repeats. Results demonstrate the superior performance of the proposed system, especially when using the information from jumps and repeats. Other information, such as ground-truth measure and staff metadata, has been evaluated, showing a slight improvement.</p>
<p>The paper is well-structured and easy to read. The context of the paper is well presented, and several demonstrations of the technology are provided.</p>
<p>In my opinion, there is little novelty in the proposal from a research point of view, but I see the potential application from the technological side with several applications to the community and market (Musescore, page-turning systems, etc.). </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>The motivation is clear and reasonable: to manually handle the jumps in alignment. The repetition labeling process brings significant improvement in accuracy with relatively low human effort. With the collected annotations, it is possible to automate the process with a model in the future.</p>
<p>I have a few minor comments:
1. I suggest moving Section 5 before the system description, as it does not depend on the alignment system (in fact, it’s the opposite).
2. In Table 1, it would be helpful to include results with only ground truth repeat annotations for comparison with the human labels.
3. The experimental settings for comparisons among different representations (L399-L402) are unclear. I guess this is related to different combinations of score features and audio features. The authors might want to clarify this.
4. The authors need to pay attention to the references: use a consistent format (proceeding names, conference names, etc.), fill in missing fields (authors, pages, years), and replace arXiv versions with proceedings versions where applicable.</p>
<p>Overall, the novelty and impact are not very high; therefore, I recommend a weak accept.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>Short Paper Summary:
This paper proposes an audio-to-score alignment approach to align arbitrary music performances to sheet images. The approach extends and improves upon existing audio and score representations (bootleg score) for this task and further relies on minimal human intervention in the form of annotated jumps/repeats in the sheet image. This can usually be done quickly and is shown to significantly improve the alignment quality.</p>
<p>Note on Reproducibility:
The authors promise to release their code upon acceptance (and assumingly their annotation tool as well). Alongside the information on which pieces have been used for evaluation (given in the supplementary material) it should be possible to reproduce and verify the results.</p>
<p>Main Review:
While the basic components of the approach are not novel (bootleg-score inspired score representation, Onset &amp; Frames to get a piano roll-like representation of the audio, DTW), I still think the practicality of the approach warrants an acceptance. The paper is clearly written and for the most part easy to follow along. That being said, some aspects of the paper could be improved.</p>
<p>In Section 3.1, it is not immediately clear to me how the shape of S_i (line 240) is derived. 88 for the pitch dimension is clear, but what’s the reasoning behind 48 and how would this impact the resolution of the score representation, e.g. with respect to extremely fast note runs?
I think the (index) variable i is used for two different things in lines 239/240/242. To make the difference more clear, I’d maybe suggest using another variable.
In the same section, when describing how the bootleg score is converted into a piano roll. Is C major always assumed as the default key? I might be reading the sentence (starting at line 253) wrong, or the end of the insertion mark is missing. </p>
<p>In Section 3.2, the use of the onset predictions coming from the Onset &amp; Frames model is introduced. It’s worth noting here that Shan and Tsai [16, 17] already used the onsets derived from the MIDI transcription of Onset &amp; Frames for their bootleg score representation of the audio. I assume this is then similar to what is later on tested in Section 4.5 as onset predictions (with the difference of staying in a “piano roll space”), which has a similar performance as the onset probabilities. However, the evaluation for that seems to be done only on the relatively small M13 dataset. In order to really claim that this refined audio representation results in an improvement, I’d suggest also doing an evaluation on the larger SMR dataset.</p>
<p>Overall, I think the comparison and the claim around superiority “due to our refinements to feature representations” is not entirely fair and clearly shown in the experiment. Which is already acknowledged to some extent in the paper itself due to the proposed measure-wise evaluation.
If I’m not mistaken, nothing prevents the proposed approach to use similar features, i.e. same bootleg score representation as Shan and Tsai [16, 17]. Still performing measure segmentation, but basically stopping before the conversion to piano rolls described in 3.1. Such a model could be used as an additional baseline/vanilla version.
Alternatively, it might be possible to additionally extend the existing measure-aware evaluation to a system-level evaluation which would allow for a more direct and fairer comparison (although maybe not as useful wrt. to the overall alignment precision as the measure-aware version). 
In any case, I’m not doubting that the proposed approach is able to yield more precise results, but I think such a comparison could benefit the paper and better support the claims made. </p>
<p>Minor Remarks:
- Line 493: “... that offers has an interactive …” -&gt; “... that offers an interactive …”
- In the related work section, when writing “We diverge from them by considering a range of different types of raw score images and audio (such as ones with instrumentation beyond solo piano)” (lines 472 onwards). While this is shown in the supplementary material with some examples, the datasets used for evaluation only contain 1 or 2 samples that aren’t strictly piano music. Even though limited to piano music and also in the score following domain, another potentially related line of research to check out could be Henkel and Widmer ”Real-Time Music Following in Score Sheet Images via Multi-Resolution Prediction” as they also experiment with raw sheet image scans.
- I ticked ‘yes’ for “the paper adheres to ISMIR 2024 submission guidelines”, which is the case for the most part, but I want to make clear that the references are not well formatted. Please make sure to cite the proper conference version of papers instead of the arxiv ones, e.g. [1, 3, 12, 13, 15, 16, 37, 38] were published at ISMIR. Also, there are missing venues for [26, 29] and [19] shows a placeholder date.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Updated Table 1 to reflect the final state of evaluation.
Decided against moving Section 5 before the system description to avoid confusion regarding some repeat annotations. For instance, the repeat annotations in MeSA-13 come from a different interface (not ours).
Upon being asked to include a setting with only ground truth repeat annotations in Table 1, we changed our wording in the caption to show that the human-labeled repeats are in fact also ground truth repeats.
Changed wording to clarify the experimental setting in Section 4.5.
Explained the shape of S_i in L240 in the same paragraph. 
Provided motivation for why we consider a half measure radius in our evaluation metric.
Fixed variables in L239/240/242.
Clarified that C major is always the default key.
Added clarification in Section 4.3 to emphasize that Shan et al. used the same model for piano transcription that we used, Onsets and Frames, however a different representation (MIDI transcription directly obtained from the model).
Added Table 2 to report results for the evaluation of different audio feature representations on both M13 and SMR.
Decided against reporting results from additional baselines since our focus in this paper is on showing that human-labeled repeats increase alignment accuracy considerably. Due to this, we did not evaluate the effect of minor algorithmic changes, but a more detailed analysis could be of interest in future work.
We also do not implement a system/line-level evaluation metric because even though that would allow us to evaluate our baseline’s performance in a different setting, our workflow aims for improvement on measure-level alignment, so we keep our focus on measure-level evaluation.
Included citations suggested by reviewers.
Included links to supplementary videos and our code for reproducibility as promised.
References are fixed to have consistent formatting and display all necessary information.
The SMR dataset (subset) is updated to contain 60 pieces instead of 49.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;357&#39;, &#39;session&#39;: &#39;7&#39;, &#39;position&#39;: &#39;17&#39;, &#39;forum&#39;: &#39;357&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/11zO68fEWu30iyUTulZET7OrLb0i5Aa32/view?usp=sharing&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Just Label the Repeats for In-The-Wild Audio-to-Score Alignment&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Bukey, Irmak*&#39;, &#39; Feffer, Michael&#39;, &#39; Donahue, Chris&#39;], &#39;authors_and_affil&#39;: [&#39;Irmak Bukey (Carnegie Mellon University)*&#39;, &#39; Michael Feffer (Carnegie Mellon University)&#39;, &#39; Chris Donahue (CMU)&#39;], &#39;keywords&#39;: [&#39;MIR fundamentals and methodology -&gt; multimodality; MIR tasks -&gt; optical music recognition; MIR tasks -&gt; pattern matching and detection&#39;, &#39;MIR tasks -&gt; alignment, synchronization, and score following&#39;], &#39;abstract&#39;: &#39;We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% → 82%).&#39;, &#39;TLDR&#39;: &#39;We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images). Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% → 82%).&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/10SNBKkgmZ93IS5WNXThWLvpxRXXQpTk_/view?usp=sharing&#39;, &#39;session&#39;: [&#39;7&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1UvKKzTuptPSWcUG8eLtoRAW27nLrJMCU/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1w4trgsz-MIdKaRspq3X0EEUYFMWbYweP/view?usp=sharing&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UHEN7T2S&#39;, &#39;slack_channel&#39;: &#39;p7-16-just-label-the&#39;, &#39;day&#39;: &#39;4&#39;, &#39;review_1&#39;: &#39;In this paper, the authors propose an audio-to-music-sheet synchronization system that combines several state-of-the-art approaches to detect notes and staff lines [16,17], measure positions [21], music transcription [3], and standard DTW [22]. On top of this engineering solution, the authors developed an interface to annotate jumps and repeats and feed the system with this information.\n\nEvaluation has been performed at the measure level using the MeSA-13 and a subset from the SMR dataset. The proposed system is compared with the method in [17], which uses Hierarchical DTW to automatically account for the jumps and repeats. Results demonstrate the superior performance of the proposed system, especially when using the information from jumps and repeats. Other information, such as ground-truth measure and staff metadata, has been evaluated, showing a slight improvement.\n\nThe paper is well-structured and easy to read. The context of the paper is well presented, and several demonstrations of the technology are provided.\n\nIn my opinion, there is little novelty in the proposal from a research point of view, but I see the potential application from the technological side with several applications to the community and market (Musescore, page-turning systems, etc.). &#39;, &#39;review_2&#39;: &#39;The motivation is clear and reasonable: to manually handle the jumps in alignment. The repetition labeling process brings significant improvement in accuracy with relatively low human effort. With the collected annotations, it is possible to automate the process with a model in the future.\n\nI have a few minor comments:\n1. I suggest moving Section 5 before the system description, as it does not depend on the alignment system (in fact, it’s the opposite).\n2. In Table 1, it would be helpful to include results with only ground truth repeat annotations for comparison with the human labels.\n3. The experimental settings for comparisons among different representations (L399-L402) are unclear. I guess this is related to different combinations of score features and audio features. The authors might want to clarify this.\n4. The authors need to pay attention to the references: use a consistent format (proceeding names, conference names, etc.), fill in missing fields (authors, pages, years), and replace arXiv versions with proceedings versions where applicable.\n\nOverall, the novelty and impact are not very high; therefore, I recommend a weak accept.&#39;, &#39;review_3&#39;: &#39;Short Paper Summary:\nThis paper proposes an audio-to-score alignment approach to align arbitrary music performances to sheet images. The approach extends and improves upon existing audio and score representations (bootleg score) for this task and further relies on minimal human intervention in the form of annotated jumps/repeats in the sheet image. This can usually be done quickly and is shown to significantly improve the alignment quality.\n\t\t\nNote on Reproducibility:\nThe authors promise to release their code upon acceptance (and assumingly their annotation tool as well). Alongside the information on which pieces have been used for evaluation (given in the supplementary material) it should be possible to reproduce and verify the results.\n\nMain Review:\nWhile the basic components of the approach are not novel (bootleg-score inspired score representation, Onset &amp; Frames to get a piano roll-like representation of the audio, DTW), I still think the practicality of the approach warrants an acceptance. The paper is clearly written and for the most part easy to follow along. That being said, some aspects of the paper could be improved.\n\nIn Section 3.1, it is not immediately clear to me how the shape of S_i (line 240) is derived. 88 for the pitch dimension is clear, but what’s the reasoning behind 48 and how would this impact the resolution of the score representation, e.g. with respect to extremely fast note runs?\nI think the (index) variable i is used for two different things in lines 239/240/242. To make the difference more clear, I’d maybe suggest using another variable.\nIn the same section, when describing how the bootleg score is converted into a piano roll. Is C major always assumed as the default key? I might be reading the sentence (starting at line 253) wrong, or the end of the insertion mark is missing. \n\nIn Section 3.2, the use of the onset predictions coming from the Onset &amp; Frames model is introduced. It’s worth noting here that Shan and Tsai [16, 17] already used the onsets derived from the MIDI transcription of Onset &amp; Frames for their bootleg score representation of the audio. I assume this is then similar to what is later on tested in Section 4.5 as onset predictions (with the difference of staying in a “piano roll space”), which has a similar performance as the onset probabilities. However, the evaluation for that seems to be done only on the relatively small M13 dataset. In order to really claim that this refined audio representation results in an improvement, I’d suggest also doing an evaluation on the larger SMR dataset.\n\nOverall, I think the comparison and the claim around superiority “due to our refinements to feature representations” is not entirely fair and clearly shown in the experiment. Which is already acknowledged to some extent in the paper itself due to the proposed measure-wise evaluation.\nIf I’m not mistaken, nothing prevents the proposed approach to use similar features, i.e. same bootleg score representation as Shan and Tsai [16, 17]. Still performing measure segmentation, but basically stopping before the conversion to piano rolls described in 3.1. Such a model could be used as an additional baseline/vanilla version.\nAlternatively, it might be possible to additionally extend the existing measure-aware evaluation to a system-level evaluation which would allow for a more direct and fairer comparison (although maybe not as useful wrt. to the overall alignment precision as the measure-aware version). \nIn any case, I’m not doubting that the proposed approach is able to yield more precise results, but I think such a comparison could benefit the paper and better support the claims made. \n\nMinor Remarks:\n- Line 493: “... that offers has an interactive …” -&gt; “... that offers an interactive …”\n- In the related work section, when writing “We diverge from them by considering a range of different types of raw score images and audio (such as ones with instrumentation beyond solo piano)” (lines 472 onwards). While this is shown in the supplementary material with some examples, the datasets used for evaluation only contain 1 or 2 samples that aren’t strictly piano music. Even though limited to piano music and also in the score following domain, another potentially related line of research to check out could be Henkel and Widmer ”Real-Time Music Following in Score Sheet Images via Multi-Resolution Prediction” as they also experiment with raw sheet image scans.\n- I ticked ‘yes’ for “the paper adheres to ISMIR 2024 submission guidelines”, which is the case for the most part, but I want to make clear that the references are not well formatted. Please make sure to cite the proper conference version of papers instead of the arxiv ones, e.g. [1, 3, 12, 13, 15, 16, 37, 38] were published at ISMIR. Also, there are missing venues for [26, 29] and [19] shows a placeholder date.\n&#39;, &#39;meta_review&#39;: &#39;The paper proposes a simple solution to aligning audio files with sheet music that have structural differences (such as repeats or no repeats): simply annotate them. The authors present a quick workflow for this task and demonstrate the effectiveness of the extra annotations (particularly the repeats) in their evaluation and provided videos.\n\nMain Strengths:\n- Clarity and Practicality: The paper is well-written, easy to understand, and presents a practical solution to the problem of aligning audio files with sheet music, especially when there are structural differences such as repeats or jumps.\n- Effectiveness: Demonstrates a clear improvement over baseline systems when using annotated information on repeats and jumps, which highlights the practical application and potential benefits in real-world scenarios.\n- Human-in-the-Loop Approach and Workflow Integration: Emphasises the value of human annotations in improving alignment accuracy, which is often overlooked in favour of fully automatic solutions. This approach is both quick and low in human effort. The workflow is cohesive and integrates well with existing state-of-the-art approaches, making it a robust engineering solution.\n- Reproducibility: The promise to release the code upon acceptance ensures that the results can be reproduced and verified by others in the community.\n\nMain Weaknesses:  \n- Lack of Novelty: The approach lacks significant novelty from a research perspective. The idea of labelling repeats and using annotated jumps is not new, and the improvements over existing methods are not groundbreaking.\n- Evaluation Scope: The evaluations, while showing improvements, lack depth in analysis. For instance, there is a need for a more detailed explanation of the significant accuracy jump attributed to feature representations.\n- Dataset Limitations: The datasets used for evaluation are limited, particularly in the diversity of music types. The evaluation predominantly focuses on piano music, which may not fully represent the system’s capabilities.\n\nFurther Comments:\n- The measure-aware alignment and evaluation make intuitive sense but could be perceived as arbitrary due to the variability in measure lengths.\n- The proposed system’s baseline comparison should also consider using similar features to those of existing methods to ensure a fair evaluation.\n\nRequested Improvements:  \n- References: The references are inconsistently formatted and incomplete in some cases. Please resolve this for a potential final version. Also, some additional references of papers addressing jumps/repeats in the alignment context should be added (see the individual reviews).\n- Features: Improvements in feature representation claims should be supported by evaluations on larger datasets like SMR, in addition to the relatively small M13 dataset. The paper could benefit from a more detailed analysis of why the proposed feature representations supposedly lead to significant improvements.\n\n\nPlease also consider the more detailed comments by the individual reviewers.\n\nDespite the lack of significant novelty, the paper presents a practical and effective solution to audio-to-sheet music alignment. Its clear writing, practical workflow, and demonstrated improvements in alignment accuracy make it a valuable contribution.&#39;, &#39;author_changes&#39;: &#39;Updated Table 1 to reflect the final state of evaluation.\nDecided against moving Section 5 before the system description to avoid confusion regarding some repeat annotations. For instance, the repeat annotations in MeSA-13 come from a different interface (not ours).\nUpon being asked to include a setting with only ground truth repeat annotations in Table 1, we changed our wording in the caption to show that the human-labeled repeats are in fact also ground truth repeats.\nChanged wording to clarify the experimental setting in Section 4.5.\nExplained the shape of S_i in L240 in the same paragraph. \nProvided motivation for why we consider a half measure radius in our evaluation metric.\nFixed variables in L239/240/242.\nClarified that C major is always the default key.\nAdded clarification in Section 4.3 to emphasize that Shan et al. used the same model for piano transcription that we used, Onsets and Frames, however a different representation (MIDI transcription directly obtained from the model).\nAdded Table 2 to report results for the evaluation of different audio feature representations on both M13 and SMR.\nDecided against reporting results from additional baselines since our focus in this paper is on showing that human-labeled repeats increase alignment accuracy considerably. Due to this, we did not evaluate the effect of minor algorithmic changes, but a more detailed analysis could be of interest in future work.\nWe also do not implement a system/line-level evaluation metric because even though that would allow us to evaluate our baseline’s performance in a different setting, our workflow aims for improvement on measure-level alignment, so we keep our focus on measure-level evaluation.\nIncluded citations suggested by reviewers.\nIncluded links to supplementary videos and our code for reproducibility as promised.\nReferences are fixed to have consistent formatting and display all necessary information.\nThe SMR dataset (subset) is updated to contain 60 pieces instead of 49.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>