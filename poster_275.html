

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems\u0027 instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.", "abstract": "Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems\u0027 instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.", "author_changes": "Apart from problems residing in arguments used or claims, most of the critique was into not providing more experimentation and testing some of the hypothesis/proposals left for future work. We need to highlight that the main topic of this paper is to evaluate the SOTA two-tower systems and establish a baseline (or absence). Our results alongside the performance reported from most of the authors for the two-tower systems proposed provide evidence that we are far from properly utilizing extra modalities. As a novel step, we tried to pinpoint the cause of this issue and found that the text branch or alignment is the main issue, as the audio encoders perform greatly. The 6 pages format was a hindering aspect to further provide more experiments and test some of our hypothesis for potential improvements over the problems stated and this is mainly the reason why we abstained from including results as they would decrese the value of this paper as an evaluation one.\n\nFigure 2 was criticized but we argue that it clearly show the following two takeaways: Changing prompts lead to almost erratic performance changes and using Audio only information almost doubles the performance.\n\nWe generally included pretty bold claims, and in some cases unjustified (as claiming that we have an issue with using two-tower systems for multi-label zero-shot classification). We changed the phrasing to imply that this is just a stepping stone and we cannot have a strong conclusion given those experiments. Despite this fact, we still think its a great way of establishing a more thorough way of evaluating Deep Learning systems and sheding more light in the reasons why these systems might fail apart from presenting a table with metrics.", "authors": ["Vasilakis, Yannis*", " Bittner, Rachel", " Pauwels, Johan"], "authors_and_affil": ["Yannis Vasilakis (Queen Mary University of London)*", " Rachel Bittner (Spotify)", " Johan Pauwels (Queen Mary University of London)"], "channel_url": "https://ismir2024.slack.com/archives/C07UM61SQ85", "day": "3", "keywords": ["Evaluation, datasets, and reproducibility -\u003e evaluation methodology; Evaluation, datasets, and reproducibility -\u003e novel datasets and use cases; MIR fundamentals and methodology -\u003e metadata, tags, linked data, and semantic web; MIR tasks -\u003e automatic classification", "MIR fundamentals and methodology -\u003e multimodality"], "long_presentation": "FALSE", "meta_review": "Most reviewers agree that the paper provides a good overview of the problem space with one or two comments regarding some additional work that may be included there. Overall the introduction and definition of the problem is well done. Most reviewer criticisms come down to the interpretation of results, and more incorporation of future work into the current work. Specifically, many of the conclusions drawn in the text are not clearly evident in the results shown, and reviewer\u0027s raise the need for a clearer discussion of results in the text that specifically is proven in the results presented.\n\nPlease take into careful consideration the reviewer\u0027s comments to improve this work for the camera ready version.", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/view?usp=drive_link", "poster_pdf": "https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/view?usp=drive_link", "review_1": "Summary:\n\nThe paper discusses the applicability of multimodal audio-text for musical instrument classification. Such models enable a user to measure the \u0027distance\u0027 between an audio clip and a text prompt. The authors determine that such models are suitable for tasks such as instrument classification due to the scarcity of large datasets containing many labeled instruments as well as the fluidity of the output label space, i.e., new instruments might be needed to be added to a trained classifier.\nThe authors aim to assess the zero-shot capability of such models for the task, specifically probing the importance of the text prompt used, and the power of so-called pre-joint and joint embedding spaces of three models. Through various experiments, they find that models do not utilize text context instead focusing on the label word within the input prompt. They also find that the text branch of these models perform significantly poorly compared to the audio-only branch. The conclusion of this study is that alignment between text and audio is lacking in the chosen music-text joint models and should be the focus of future research.\n\nStrengths:\n- The paper is discussing research questions that are well aligned with latest trends of machine learning research. Multimodal models are being heavily studied and there is a surge of audio/music/text models. The findinds in this paper seem to be helpful in guiding future direction of research in the space.\n- The authors have done a good job organizing the paper and covering various research questions that arise when evaluating the zero-shot capabilities of the models.\n\nWeaknesses:\n- I am not sure why the authors refer to the two \"musically informed\" prompts as such. I do not really see any semantic difference between the MusCALL prompt and the #1 prompt.\n- I believe the study would have been complete if the authors chose to include one of the experiments that has been left as future work: using some kind of prompt upsampling/augmentation to inject stochasticity to the training of such models. It is well-known that such methods improve the generalizability of these models and thus the findings of this paper might potentially have changed significantly using more robustly trained models.", "review_2": "The paper compares three different algorithms for two-tower multimodal systems which produce a joint audio and text embedding based on text and audio encoders. Although the data set used is rather simple and not very large, the authors justify very well its selection.\n\nThe paper is well written, all necessary literature is discussed. After some backgrounds into methodology several experiments are described in detail and supported with figures. Also the problematic issues of the used system are reported.\n\nHowever, the statistical evaluation of experiments can be done in a more thorough way: e.g., statistics over prompts for n different instruments could be reported with standard deviations (cf. Figure 2) and also statistical tests could be applied for some quantitative measurements / comparison of prompts and methods. For instance, the statement that \"MusCALL prompt .. leads to the highest top-1 accuracy\" is not clear: for Music/Speech CLAP it can be observed, but for Music CLAP the values seem to be very close, and for MusCALL it is even below the most of other prompts. ", "review_3": "The authors provide a thorough, well-organized analysis of the pre-joint and joint audio-text embedding spaces in the context of musical instrument recognition. Thanks for your contributions!\n\nStrengths:\n- Related work section is very strong and covers all of the bases, which is especially great in an analysis paper!\n- Very clear motivation and setup of experimentation.\n- Figures 3 and 4 were really interesting and illustrated some of your key discussion points well!\n\nComments:\n- Figure 2 was hard to digest. (1) The inclusion of the joint and pre-joint audio does not fit as the figure mostly illustrates prompt comparisons and while it states in 3.3 that it will be explained exactly what these columns mean in 3.4, I never quite could understand what this represented? (2) Could be helpful to have the prompts on the same page as this figure or even somewhere in the figure, as looking at \"prompt 1...etc\" means nothing to the reader without flipping back and forth\n- I think more qualitative analysis and discussion could enhance your analysis even further. The discussion is mostly centered on metrics for instrument recognition (which also makes sense), but it would be great to have a bit more discussion of what you really saw (qualitatively) in the recognition when using the different embeddings, similar to the discussion you have surrounding the importance of prompt engineering in the text encoder. \n- The metric for semantic meaningfulness based on instrument ontology is quite interesting in theory, but Figure 5 shows that across systems the results were quite similar. Is there a way the metric could be modified to emphasize wider spreads in some way?\n", "session": ["6"], "slack_channel": "p6-11-i-can-listen", "slides_pdf": "https://drive.google.com/file/d/174XlTrh62UtEsTEAAqaU58lyZ38Bx4Tw/view?usp=drive_link", "title": "I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition", "video": "https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/view?usp=drive_link"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Yannis Vasilakis (Queen Mary University of London)*"
               class="text-muted"
            >Yannis Vasilakis (Queen Mary University of London)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Rachel Bittner (Spotify)"
               class="text-muted"
            > Rachel Bittner (Spotify)</a
            >,
            
            <a href="papers.html?filter=authors&search= Johan Pauwels (Queen Mary University of London)"
               class="text-muted"
            > Johan Pauwels (Queen Mary University of London)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UM61SQ85" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p6-11-i-can-listen</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility -&gt; evaluation methodology; Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; automatic classification"
                    class="text-secondary text-decoration-none"
            >Evaluation, datasets, and reproducibility -&gt; evaluation methodology; Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; automatic classification</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=MIR fundamentals and methodology -&gt; multimodality"
                    class="text-secondary text-decoration-none"
            >MIR fundamentals and methodology -&gt; multimodality</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems' instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://drive.google.com/file/d/174XlTrh62UtEsTEAAqaU58lyZ38Bx4Tw/preview?usp=drive_link" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>Most reviewers agree that the paper provides a good overview of the problem space with one or two comments regarding some additional work that may be included there. Overall the introduction and definition of the problem is well done. Most reviewer criticisms come down to the interpretation of results, and more incorporation of future work into the current work. Specifically, many of the conclusions drawn in the text are not clearly evident in the results shown, and reviewer's raise the need for a clearer discussion of results in the text that specifically is proven in the results presented.</p>
<p>Please take into careful consideration the reviewer's comments to improve this work for the camera ready version.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>Summary:</p>
<p>The paper discusses the applicability of multimodal audio-text for musical instrument classification. Such models enable a user to measure the 'distance' between an audio clip and a text prompt. The authors determine that such models are suitable for tasks such as instrument classification due to the scarcity of large datasets containing many labeled instruments as well as the fluidity of the output label space, i.e., new instruments might be needed to be added to a trained classifier.
The authors aim to assess the zero-shot capability of such models for the task, specifically probing the importance of the text prompt used, and the power of so-called pre-joint and joint embedding spaces of three models. Through various experiments, they find that models do not utilize text context instead focusing on the label word within the input prompt. They also find that the text branch of these models perform significantly poorly compared to the audio-only branch. The conclusion of this study is that alignment between text and audio is lacking in the chosen music-text joint models and should be the focus of future research.</p>
<p>Strengths:
- The paper is discussing research questions that are well aligned with latest trends of machine learning research. Multimodal models are being heavily studied and there is a surge of audio/music/text models. The findinds in this paper seem to be helpful in guiding future direction of research in the space.
- The authors have done a good job organizing the paper and covering various research questions that arise when evaluating the zero-shot capabilities of the models.</p>
<p>Weaknesses:
- I am not sure why the authors refer to the two "musically informed" prompts as such. I do not really see any semantic difference between the MusCALL prompt and the #1 prompt.
- I believe the study would have been complete if the authors chose to include one of the experiments that has been left as future work: using some kind of prompt upsampling/augmentation to inject stochasticity to the training of such models. It is well-known that such methods improve the generalizability of these models and thus the findings of this paper might potentially have changed significantly using more robustly trained models.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>The paper compares three different algorithms for two-tower multimodal systems which produce a joint audio and text embedding based on text and audio encoders. Although the data set used is rather simple and not very large, the authors justify very well its selection.</p>
<p>The paper is well written, all necessary literature is discussed. After some backgrounds into methodology several experiments are described in detail and supported with figures. Also the problematic issues of the used system are reported.</p>
<p>However, the statistical evaluation of experiments can be done in a more thorough way: e.g., statistics over prompts for n different instruments could be reported with standard deviations (cf. Figure 2) and also statistical tests could be applied for some quantitative measurements / comparison of prompts and methods. For instance, the statement that "MusCALL prompt .. leads to the highest top-1 accuracy" is not clear: for Music/Speech CLAP it can be observed, but for Music CLAP the values seem to be very close, and for MusCALL it is even below the most of other prompts. </p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>The authors provide a thorough, well-organized analysis of the pre-joint and joint audio-text embedding spaces in the context of musical instrument recognition. Thanks for your contributions!</p>
<p>Strengths:
- Related work section is very strong and covers all of the bases, which is especially great in an analysis paper!
- Very clear motivation and setup of experimentation.
- Figures 3 and 4 were really interesting and illustrated some of your key discussion points well!</p>
<p>Comments:
- Figure 2 was hard to digest. (1) The inclusion of the joint and pre-joint audio does not fit as the figure mostly illustrates prompt comparisons and while it states in 3.3 that it will be explained exactly what these columns mean in 3.4, I never quite could understand what this represented? (2) Could be helpful to have the prompts on the same page as this figure or even somewhere in the figure, as looking at "prompt 1...etc" means nothing to the reader without flipping back and forth
- I think more qualitative analysis and discussion could enhance your analysis even further. The discussion is mostly centered on metrics for instrument recognition (which also makes sense), but it would be great to have a bit more discussion of what you really saw (qualitatively) in the recognition when using the different embeddings, similar to the discussion you have surrounding the importance of prompt engineering in the text encoder. 
- The metric for semantic meaningfulness based on instrument ontology is quite interesting in theory, but Figure 5 shows that across systems the results were quite similar. Is there a way the metric could be modified to emphasize wider spreads in some way?</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Apart from problems residing in arguments used or claims, most of the critique was into not providing more experimentation and testing some of the hypothesis/proposals left for future work. We need to highlight that the main topic of this paper is to evaluate the SOTA two-tower systems and establish a baseline (or absence). Our results alongside the performance reported from most of the authors for the two-tower systems proposed provide evidence that we are far from properly utilizing extra modalities. As a novel step, we tried to pinpoint the cause of this issue and found that the text branch or alignment is the main issue, as the audio encoders perform greatly. The 6 pages format was a hindering aspect to further provide more experiments and test some of our hypothesis for potential improvements over the problems stated and this is mainly the reason why we abstained from including results as they would decrese the value of this paper as an evaluation one.</p>
<p>Figure 2 was criticized but we argue that it clearly show the following two takeaways: Changing prompts lead to almost erratic performance changes and using Audio only information almost doubles the performance.</p>
<p>We generally included pretty bold claims, and in some cases unjustified (as claiming that we have an issue with using two-tower systems for multi-label zero-shot classification). We changed the phrasing to imply that this is just a stepping stone and we cannot have a strong conclusion given those experiments. Despite this fact, we still think its a great way of establishing a more thorough way of evaluating Deep Learning systems and sheding more light in the reasons why these systems might fail apart from presenting a table with metrics.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;275&#39;, &#39;session&#39;: &#39;6&#39;, &#39;position&#39;: &#39;12&#39;, &#39;forum&#39;: &#39;275&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1Ku9p2tOaHugEDAx1tH0ervgBcpre2E4M/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Vasilakis, Yannis*&#39;, &#39; Bittner, Rachel&#39;, &#39; Pauwels, Johan&#39;], &#39;authors_and_affil&#39;: [&#39;Yannis Vasilakis (Queen Mary University of London)*&#39;, &#39; Rachel Bittner (Spotify)&#39;, &#39; Johan Pauwels (Queen Mary University of London)&#39;], &#39;keywords&#39;: [&#39;Evaluation, datasets, and reproducibility -&gt; evaluation methodology; Evaluation, datasets, and reproducibility -&gt; novel datasets and use cases; MIR fundamentals and methodology -&gt; metadata, tags, linked data, and semantic web; MIR tasks -&gt; automatic classification&#39;, &#39;MIR fundamentals and methodology -&gt; multimodality&#39;], &#39;abstract&#39;: &#34;Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems&#39; instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.&#34;, &#39;TLDR&#39;: &#34;Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition, and a detailed analysis of the properties of the pre-projected and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an existing instrument ontology is proposed. This method reveals deficiencies in the systems&#39; instrumental knowledge and provides evidence of the need for fine-tuning text encoders on musical data.&#34;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1ShVguNWg8DDYlHO5NZP6wZb4i2O-9U6G/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;6&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1vHktHxDFIuRjWpEIZHFEaWsNpVNc1Ox_/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/17p5K8VOFeqe5La36MwY4hC87sLFS2089/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UM61SQ85&#39;, &#39;slack_channel&#39;: &#39;p6-11-i-can-listen&#39;, &#39;slides_pdf&#39;: &#39;https://drive.google.com/file/d/174XlTrh62UtEsTEAAqaU58lyZ38Bx4Tw/view?usp=drive_link&#39;, &#39;day&#39;: &#39;3&#39;, &#39;review_1&#39;: &#39;Summary:\n\nThe paper discusses the applicability of multimodal audio-text for musical instrument classification. Such models enable a user to measure the \&#39;distance\&#39; between an audio clip and a text prompt. The authors determine that such models are suitable for tasks such as instrument classification due to the scarcity of large datasets containing many labeled instruments as well as the fluidity of the output label space, i.e., new instruments might be needed to be added to a trained classifier.\nThe authors aim to assess the zero-shot capability of such models for the task, specifically probing the importance of the text prompt used, and the power of so-called pre-joint and joint embedding spaces of three models. Through various experiments, they find that models do not utilize text context instead focusing on the label word within the input prompt. They also find that the text branch of these models perform significantly poorly compared to the audio-only branch. The conclusion of this study is that alignment between text and audio is lacking in the chosen music-text joint models and should be the focus of future research.\n\nStrengths:\n- The paper is discussing research questions that are well aligned with latest trends of machine learning research. Multimodal models are being heavily studied and there is a surge of audio/music/text models. The findinds in this paper seem to be helpful in guiding future direction of research in the space.\n- The authors have done a good job organizing the paper and covering various research questions that arise when evaluating the zero-shot capabilities of the models.\n\nWeaknesses:\n- I am not sure why the authors refer to the two &#34;musically informed&#34; prompts as such. I do not really see any semantic difference between the MusCALL prompt and the #1 prompt.\n- I believe the study would have been complete if the authors chose to include one of the experiments that has been left as future work: using some kind of prompt upsampling/augmentation to inject stochasticity to the training of such models. It is well-known that such methods improve the generalizability of these models and thus the findings of this paper might potentially have changed significantly using more robustly trained models.&#39;, &#39;review_2&#39;: &#39;The paper compares three different algorithms for two-tower multimodal systems which produce a joint audio and text embedding based on text and audio encoders. Although the data set used is rather simple and not very large, the authors justify very well its selection.\n\nThe paper is well written, all necessary literature is discussed. After some backgrounds into methodology several experiments are described in detail and supported with figures. Also the problematic issues of the used system are reported.\n\nHowever, the statistical evaluation of experiments can be done in a more thorough way: e.g., statistics over prompts for n different instruments could be reported with standard deviations (cf. Figure 2) and also statistical tests could be applied for some quantitative measurements / comparison of prompts and methods. For instance, the statement that &#34;MusCALL prompt .. leads to the highest top-1 accuracy&#34; is not clear: for Music/Speech CLAP it can be observed, but for Music CLAP the values seem to be very close, and for MusCALL it is even below the most of other prompts. &#39;, &#39;review_3&#39;: &#39;The authors provide a thorough, well-organized analysis of the pre-joint and joint audio-text embedding spaces in the context of musical instrument recognition. Thanks for your contributions!\n\nStrengths:\n- Related work section is very strong and covers all of the bases, which is especially great in an analysis paper!\n- Very clear motivation and setup of experimentation.\n- Figures 3 and 4 were really interesting and illustrated some of your key discussion points well!\n\nComments:\n- Figure 2 was hard to digest. (1) The inclusion of the joint and pre-joint audio does not fit as the figure mostly illustrates prompt comparisons and while it states in 3.3 that it will be explained exactly what these columns mean in 3.4, I never quite could understand what this represented? (2) Could be helpful to have the prompts on the same page as this figure or even somewhere in the figure, as looking at &#34;prompt 1...etc&#34; means nothing to the reader without flipping back and forth\n- I think more qualitative analysis and discussion could enhance your analysis even further. The discussion is mostly centered on metrics for instrument recognition (which also makes sense), but it would be great to have a bit more discussion of what you really saw (qualitatively) in the recognition when using the different embeddings, similar to the discussion you have surrounding the importance of prompt engineering in the text encoder. \n- The metric for semantic meaningfulness based on instrument ontology is quite interesting in theory, but Figure 5 shows that across systems the results were quite similar. Is there a way the metric could be modified to emphasize wider spreads in some way?\n&#39;, &#39;meta_review&#39;: &#34;Most reviewers agree that the paper provides a good overview of the problem space with one or two comments regarding some additional work that may be included there. Overall the introduction and definition of the problem is well done. Most reviewer criticisms come down to the interpretation of results, and more incorporation of future work into the current work. Specifically, many of the conclusions drawn in the text are not clearly evident in the results shown, and reviewer&#39;s raise the need for a clearer discussion of results in the text that specifically is proven in the results presented.\n\nPlease take into careful consideration the reviewer&#39;s comments to improve this work for the camera ready version.&#34;, &#39;author_changes&#39;: &#39;Apart from problems residing in arguments used or claims, most of the critique was into not providing more experimentation and testing some of the hypothesis/proposals left for future work. We need to highlight that the main topic of this paper is to evaluate the SOTA two-tower systems and establish a baseline (or absence). Our results alongside the performance reported from most of the authors for the two-tower systems proposed provide evidence that we are far from properly utilizing extra modalities. As a novel step, we tried to pinpoint the cause of this issue and found that the text branch or alignment is the main issue, as the audio encoders perform greatly. The 6 pages format was a hindering aspect to further provide more experiments and test some of our hypothesis for potential improvements over the problems stated and this is mainly the reason why we abstained from including results as they would decrese the value of this paper as an evaluation one.\n\nFigure 2 was criticized but we argue that it clearly show the following two takeaways: Changing prompts lead to almost erratic performance changes and using Audio only information almost doubles the performance.\n\nWe generally included pretty bold claims, and in some cases unjustified (as claiming that we have an issue with using two-tower systems for multi-label zero-shot classification). We changed the phrasing to imply that this is just a stepping stone and we cannot have a strong conclusion given those experiments. Despite this fact, we still think its a great way of establishing a more thorough way of evaluating Deep Learning systems and sheding more light in the reasons why these systems might fail apart from presenting a table with metrics.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>