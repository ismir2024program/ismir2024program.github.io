

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>MiniConf 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Buisson, Morgan*"
               class="text-muted"
            >Buisson, Morgan*</a
            >,
            
            <a href="papers.html?filter=authors&search= McFee, Brian"
               class="text-muted"
            > McFee, Brian</a
            >,
            
            <a href="papers.html?filter=authors&search= Essid, Slim"
               class="text-muted"
            > Essid, Slim</a
            >
            
        </h3>
        
        <div class="btn-group ml-3 mb-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MY243B" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p1-20-using-pairwise-link</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search="
                    class="text-secondary text-decoration-none"
            ></a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Musical features and properties -&gt; structure, segmentation, and form"
                    class="text-secondary text-decoration-none"
            >Musical features and properties -&gt; structure, segmentation, and form</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1J7VODLY0uirvo78S5oTCZ2Sj1uZWwc8H/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/14wfben86kWD1tsG3bRS-8e_RcM5kjQ6V/preview" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1ItBVYLES5QsKMp0kVJ6DAfRvT3JgHrqa/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The reviewers ranged widely in their view of this paper. All had constructive criticisms, on different aspects of the paper. The concerns included:</p>
<ul>
<li>lack of context about the kind of music assumed to be the target of the method;</li>
<li>lack of certainty that the evaluation is fair;</li>
<li>lack of motivation for using graph neural networks, instead of a simpler Transformer approach;</li>
<li>lack of depth in the ablation study.</li>
</ul>
<p>In the discussion between reviewers, we all agreed that these were valid concerns, but we disagreed about their severity. We hope that the authors can take all of the suggestions on board as they revise this paper.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>The paper proposes a music structure analysis technique based on pairwise link prediction using graph neural networks. A general criticism I have of this work is a problem that is noticeable in various studies in this field: before starting a proposal for music structure analysis, there is never a preliminary musical discussion. Specifically:</p>
<p>1- Repertoire as a corpus: What is the repertoire to be used as corpus? This only appears in the discussion of the experiment.
2 - Notion of musical structure: Given a specific corpus, what is the notion of musical structure and what should be examined in the corpus to highlight this structure?
3 - Detection difficulties and tools: Given a specific corpus, what are the difficulties in detecting structure and what tools have already been used to address this challenge?</p>
<p>For example, the audio representation used in this case is a MEL spectrogram representation with slices centred around each detected beat position. This means that there is an extremely limited universe of corpora that this algorithm can analyse, i.e. music where the beat is a crucial element.</p>
<p>Therefore, I strongly suggest a better contextualisation of the tasks and the corpus for which this tool is intended, trying not to ignore the problems of musical nature involved in tasks of musical structure analysis. </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>The paper contains a novel approach, is mostly well-written and has high-quality Figures (and very estetically pleasant). However, as I wrote above, there are two points which I find very problematic. I believe that correcting them could make for a very strong paper. In the following there is a list of other minor problems and suggestions.</p>
<p>11: “as whether connecting elements from the same segment or section”. Something sounds wrong in this sentence. I would suggest changing it to “as belonging to the same section (or segment) or not”.
15: “boundary locations between musical segments and section labels”. This is confusing for me. Is the music consisting of a an alternance of musical segments and section labels?
The terms “segment” and “section” and “structural entity” are used in the abstract without specifying if they are synonyms, or if they are not, in what they differ. This is a bit confusing for a reader that is approaching the field. I suggest using only one term if possible, or explaining the differences between the terms.
29: typo. “These” refer to a plural but “corpus” is singular. The authors can use either “corpuses” or “corpora”.
36: what is an “event”? A sound from an instrument? And is the term “musical observation” used in the next sentence synonym of “event”? The part up to line 40 is not very clear to me.
38: what is the “multi-level dependency”. With the term “dependency” I imagine a link between two things. How can it be multi-level? The paper [1] speaks about “multiple timescales, but not about dependencies. This term is used further in the Section, but I can’t understand what it means.
Section 1.1 or 1.2: the gap sentence is missing. What is the problem in the current approaches that is solved in this paper? This is the core point of scientific papers, and without it, this paper loses a lot of its interest. The sole goal of “doing something different” is not so appealing to a reader. I invite the authors to find something that their model can do better than others and to write it very clearly.
42: how is the paper [7] relevant to to the point of the paragraph? This is a graph neural network paper that doesn’t deal with music. Moreover, is [16] also part of this group of papers that uses self-attention? If yes, it should be added.
85: a new term “audio observation” without a very intuitive meaning is introduced. Do the authors mean “audio frame”? In this case, I would use this more common term.</p>
<p>I’m extremely confused about the use of the GNN. A GNN is useful when there is a predefined graph structure to leverage. When connecting everything to everything (like </p>
<p>180-185: How can an MLP limit the oversmoothing problem? In my understanding, if the representations are too similar, there is nothing an MLP (which receives such representations as input) can do. A citation could help here if this is true. </p>
<p>The claim that the system is lightweight and has low parameters should be supported by numbers. But the number of parameters, or the training time for other papers is missing.</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>— RESUME —
This paper presents a novel structure analysis method that outputs each section's boundaries and labels. The main idea is to exploit the pairwise links between audio segments, i.e., predict whether a pair of audio segments below has the same structure. The authors suggest aggregating all these connections into a graph, which is then used to address the tasks in supervised learning.SSM</p>
<p>— SCIENTIFIC CONTRIBUTION —
The main contribution lies in applying graph neural networks for modelling music structures, a novel approach that opens up new avenues for music analysis and machine learning. </p>
<p>— METHODOLOGY —</p>
<p>The authors propose a three-step method:
1- Feature extraction: this step creates the best feature for capturing the pairwise links and builds a self-similarity matrix (SSM) as an adjacency matrix. This process consists of:</p>
<pre><code>a) Audio frame selection: it reduces the length of the audio sequence by selecting frames around the beats. Beats are obtained via a pertaining beat detection method. It's not explicitly said, but I guess this is due to memory limitations. Otherwise, why is there interest in doing that? And what's the reason that backs up the hypnosis that beat frames are more informative than off-beat frames regarding structure detections?  
b) Frame encoder transforms the audio signal into a feature vector that captures structure information. This block is based on previous works and applies self-supervised independently of the following blocks. It outputs a feature vector X. 
c) Feature refinement: This step involves making each frame exchange information between all the other frames in the track. The final features are called X'. This step is quite exciting and introduces the first graph network block. It is based on the hypothesis that the features obtained from the frame encoder are independent and, thus, won't capture proper pairwise links nor share any information between them. While this sounds intuitive, there are no experiments to back up this hypothesis since no ablation study has been conducted to test the importance of this feature refinement block.
</code></pre>
<p>On top of these feature vectors, the author gets an adjacency matrix that resumes all the pairwise connections between audio frames. They called this matrix A'.</p>
<p>2- A method to find and classify each link between features. The authors define three types of pairwise links: same segment, same section, and different section. I don't see an interest in differentiating between segments that belong to the same segment from those that belong to the same section since the former is a subcategory of the latter. Why is this distinction needed?<br />
    a) The authors propose a CNN block to process the SSM to find regular homogeny areas repeated over time. The idea is that these areas correspond to the final structure of the song. This step aims to categorise (and impose) pairwise links between the frames w.r.t. the overall song structure. The output has the same dimension as A', called E'.
    b) They add positional embedding to distinguish between the same segment/section category. I need clarification about why this is required and its utility is never shown in the ablation studies. </p>
<p>Here, we have the first loss, which classifies each E' component into one of the three categories they define. It is not detailed how the cross-entropy loss is applied in this context. I'm assuming the loss is computed per row (?), where the index row serves as a reference point, and the loss evaluates the rest of the components within the row. </p>
<p>3- The final block combines the acoustics information X' with the matrix E', a refined version highlighting mutual information. This block consists of a graph attention mechanism. The output is a final feature vector X'' used for the final boundary and label classifications. They add a final regularisation term to encourage orthogonality between label classes. The author fixed the number of possible classes to K for the whole dataset. </p>
<p>One missing explanation is that one song is equal to a batch to be sure that all the selected points after the beat track step below the same song to avoid refining them with features from other songs. How does this affect the training? i.e. since all the points in a batch share the same musical traits, isn't that problematic when propagating the gradient? </p>
<ul>
<li>RESULTS AND DISCUSSION - </li>
</ul>
<p>The authors validate their model on two datasets, RWC-Pop and Harmonix. However, they don't use the Jazz Structure Dataset (JSD) or Salami, which are commonly used to benchmark the music structure task.
It is good to see a k-fold cross-validation study and a cross-dataset evaluation. This helps assess the model's actual performance while providing proper metric ranges.</p>
<p>I appreciate the ablation study, but it misses some important experiments. I miss some further studies of each step independently:</p>
<p>1- On the feature extraction step to address the quality of the SSM matrices obtained. This could have been done by comparing them directly with the ground-truth annotation. Moreover, the benefit of the Feature refinement block has yet to be tested. How much does it contribute to the overall performance? Is it better than computing the adjacency matrix on top of the feature produced by the feature encoder? The fact that the results drop so much when removing the Link Feature block may indicate that the obtained SSM A' is not informative enough for detecting structures. </p>
<p>2— Link Feature block: Since this block can process any SSM, how powerful is it? I would have been curious to see the performance when training this block on top of another SSM. For instance, a good baseline could have been to compare the performance obtained with SSM obtained from well-known musical feature descriptors such as chroma, MFCC, or tempo grams vs. their proposed feature extraction method. </p>
<p>3- Combining link features with acoustics features. Since this block consists of an attention mechanism that combines X' and E', how does it work for the ablation experiment where the link feature (which produces E') is removed? Similarly, how informative is the E' matrix w.r.t the final tasks? Why do we need a combination of both?</p>
<p>A good point is the small size of the model—only 330K. To better understand its significance, it would have been more informative to include the number of parameters of the compared model.  </p>
<p>The significant insight here is that their model can effectively generate a feature vector X'' that captures structural section information. This is an interesting observation because their feature vector is correlated with meaningful musical sections. I'm curious about how widely applicable this insight is. The authors have set the number of sections to K = 7 (they don't mention their taxonomy), which may not be representative of all possible music sections but is likely sufficient to cover most popular Western music.</p>
<p>— FINAL COMMENT —
The utilization of graph neural networks is quite fascinating. The pairwise relationship hypothesis is intriguing, and the outcomes emphasize the grouping capabilities of their final feature vector. However, they introduce a three-step model that is primarily tested as a whole, making it difficult to evaluate the contribution of each component as well as their reusability out of their full formalisation. Furthermore, many of the design choices made need to be justified or properly evaluated. </p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>We thank the reviewers and meta-reviewer for their valuable insight and feedback on our work. We address here the main concerns that emerged through the reviewing process:</p>
<ul>
<li>
<p>Lack of context about the kind of music assumed to be the target of the method: We added a mention in our contribution that this work addresses the analysis of structure mostly for western popular music. This decision was based upon the availability of annotated data and the label taxonomy employed in previous work, so as to ease comparison with our method. In fact, the extension of the approach to different types of musical structures is a research direction we aim to undertake, and mention it in the conclusion of the paper. </p>
</li>
<li>
<p>Lack of certainty that the evaluation is fair: Our evaluation process has tried to closely follow that of previous work, where baseline systems are evaluated both in cross-validation and cross-dataset settings. However, comparisons should still be cautiously apprehended as some baselines used additional training data or augmentation strategies. Concerning the reported baseline results, we added a row in Table 1 to include the different configurations of one of them (SpecTNT). </p>
</li>
<li>
<p>Lack of motivation for using graph neural networks, instead of a simpler Transformer approach: The graph neural network framework is actually a generalization of the transformer approach when applied on a fully-connected graph. We added however a few words as to why graph neural networks are employed in our method (mainly to include link features into the attention coefficients calculation and the frame feature update).</p>
</li>
<li>
<p>Lack of depth in the ablation study: Our initial ablation study aimed at focusing on the most peculiar aspects of our method. However, we ran additional ablation experiments by also discarding the remaining steps of our method and updated Figure 4 accordingly. </p>
</li>
</ul></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;405&#39;, &#39;session&#39;: &#39;1&#39;, &#39;position&#39;: &#39;21&#39;, &#39;forum&#39;: &#39;405&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1DZNsWdPBl_rb51o_SxePiDnzqOt7bPY4/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Buisson, Morgan*&#39;, &#39; McFee, Brian&#39;, &#39; Essid, Slim&#39;], &#39;authors_and_affil&#39;: [&#39;Morgan Buisson (Telecom-Paris)*&#39;, &#39; Brian McFee (New York University)&#39;, &#39; Slim Essid (Telecom Paris - Institut Polytechnique de Paris)&#39;], &#39;keywords&#39;: [&#39;&#39;, &#39;Musical features and properties -&gt; structure, segmentation, and form&#39;], &#39;abstract&#39;: &#39;The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.&#39;, &#39;TLDR&#39;: &#39;The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/14wfben86kWD1tsG3bRS-8e_RcM5kjQ6V/view?usp=sharing&#39;, &#39;session&#39;: [&#39;1&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1J7VODLY0uirvo78S5oTCZ2Sj1uZWwc8H/view?usp=sharing&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1ItBVYLES5QsKMp0kVJ6DAfRvT3JgHrqa/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MY243B&#39;, &#39;slack_channel&#39;: &#39;p1-20-using-pairwise-link&#39;, &#39;day&#39;: &#39;1&#39;, &#39;review_1&#39;: &#39;The paper proposes a music structure analysis technique based on pairwise link prediction using graph neural networks. A general criticism I have of this work is a problem that is noticeable in various studies in this field: before starting a proposal for music structure analysis, there is never a preliminary musical discussion. Specifically:\n\n1- Repertoire as a corpus: What is the repertoire to be used as corpus? This only appears in the discussion of the experiment.\n2 - Notion of musical structure: Given a specific corpus, what is the notion of musical structure and what should be examined in the corpus to highlight this structure?\n3 - Detection difficulties and tools: Given a specific corpus, what are the difficulties in detecting structure and what tools have already been used to address this challenge?\n\nFor example, the audio representation used in this case is a MEL spectrogram representation with slices centred around each detected beat position. This means that there is an extremely limited universe of corpora that this algorithm can analyse, i.e. music where the beat is a crucial element.\n\nTherefore, I strongly suggest a better contextualisation of the tasks and the corpus for which this tool is intended, trying not to ignore the problems of musical nature involved in tasks of musical structure analysis. &#39;, &#39;review_2&#39;: &#39;The paper contains a novel approach, is mostly well-written and has high-quality Figures (and very estetically pleasant). However, as I wrote above, there are two points which I find very problematic. I believe that correcting them could make for a very strong paper. In the following there is a list of other minor problems and suggestions.\n\n11: “as whether connecting elements from the same segment or section”. Something sounds wrong in this sentence. I would suggest changing it to “as belonging to the same section (or segment) or not”.\n15: “boundary locations between musical segments and section labels”. This is confusing for me. Is the music consisting of a an alternance of musical segments and section labels?\nThe terms “segment” and “section” and “structural entity” are used in the abstract without specifying if they are synonyms, or if they are not, in what they differ. This is a bit confusing for a reader that is approaching the field. I suggest using only one term if possible, or explaining the differences between the terms.\n29: typo. “These” refer to a plural but “corpus” is singular. The authors can use either “corpuses” or “corpora”.\n36: what is an “event”? A sound from an instrument? And is the term “musical observation” used in the next sentence synonym of “event”? The part up to line 40 is not very clear to me.\n38: what is the “multi-level dependency”. With the term “dependency” I imagine a link between two things. How can it be multi-level? The paper [1] speaks about “multiple timescales, but not about dependencies. This term is used further in the Section, but I can’t understand what it means.\nSection 1.1 or 1.2: the gap sentence is missing. What is the problem in the current approaches that is solved in this paper? This is the core point of scientific papers, and without it, this paper loses a lot of its interest. The sole goal of “doing something different” is not so appealing to a reader. I invite the authors to find something that their model can do better than others and to write it very clearly.\n42: how is the paper [7] relevant to to the point of the paragraph? This is a graph neural network paper that doesn’t deal with music. Moreover, is [16] also part of this group of papers that uses self-attention? If yes, it should be added.\n85: a new term “audio observation” without a very intuitive meaning is introduced. Do the authors mean “audio frame”? In this case, I would use this more common term.\n\nI’m extremely confused about the use of the GNN. A GNN is useful when there is a predefined graph structure to leverage. When connecting everything to everything (like \n\n180-185: How can an MLP limit the oversmoothing problem? In my understanding, if the representations are too similar, there is nothing an MLP (which receives such representations as input) can do. A citation could help here if this is true. \n\nThe claim that the system is lightweight and has low parameters should be supported by numbers. But the number of parameters, or the training time for other papers is missing.&#39;, &#39;review_3&#39;: &#34;— RESUME —\nThis paper presents a novel structure analysis method that outputs each section&#39;s boundaries and labels. The main idea is to exploit the pairwise links between audio segments, i.e., predict whether a pair of audio segments below has the same structure. The authors suggest aggregating all these connections into a graph, which is then used to address the tasks in supervised learning.SSM\n\n— SCIENTIFIC CONTRIBUTION —\nThe main contribution lies in applying graph neural networks for modelling music structures, a novel approach that opens up new avenues for music analysis and machine learning. \n\n— METHODOLOGY —\n\nThe authors propose a three-step method:\n1- Feature extraction: this step creates the best feature for capturing the pairwise links and builds a self-similarity matrix (SSM) as an adjacency matrix. This process consists of:\n\n\ta) Audio frame selection: it reduces the length of the audio sequence by selecting frames around the beats. Beats are obtained via a pertaining beat detection method. It&#39;s not explicitly said, but I guess this is due to memory limitations. Otherwise, why is there interest in doing that? And what&#39;s the reason that backs up the hypnosis that beat frames are more informative than off-beat frames regarding structure detections?  \n\tb) Frame encoder transforms the audio signal into a feature vector that captures structure information. This block is based on previous works and applies self-supervised independently of the following blocks. It outputs a feature vector X. \n\tc) Feature refinement: This step involves making each frame exchange information between all the other frames in the track. The final features are called X&#39;. This step is quite exciting and introduces the first graph network block. It is based on the hypothesis that the features obtained from the frame encoder are independent and, thus, won&#39;t capture proper pairwise links nor share any information between them. While this sounds intuitive, there are no experiments to back up this hypothesis since no ablation study has been conducted to test the importance of this feature refinement block.\n\nOn top of these feature vectors, the author gets an adjacency matrix that resumes all the pairwise connections between audio frames. They called this matrix A&#39;.\n\n2- A method to find and classify each link between features. The authors define three types of pairwise links: same segment, same section, and different section. I don&#39;t see an interest in differentiating between segments that belong to the same segment from those that belong to the same section since the former is a subcategory of the latter. Why is this distinction needed?  \n\ta) The authors propose a CNN block to process the SSM to find regular homogeny areas repeated over time. The idea is that these areas correspond to the final structure of the song. This step aims to categorise (and impose) pairwise links between the frames w.r.t. the overall song structure. The output has the same dimension as A&#39;, called E&#39;.\n\tb) They add positional embedding to distinguish between the same segment/section category. I need clarification about why this is required and its utility is never shown in the ablation studies. \n\nHere, we have the first loss, which classifies each E&#39; component into one of the three categories they define. It is not detailed how the cross-entropy loss is applied in this context. I&#39;m assuming the loss is computed per row (?), where the index row serves as a reference point, and the loss evaluates the rest of the components within the row. \n\n3- The final block combines the acoustics information X&#39; with the matrix E&#39;, a refined version highlighting mutual information. This block consists of a graph attention mechanism. The output is a final feature vector X&#39;&#39; used for the final boundary and label classifications. They add a final regularisation term to encourage orthogonality between label classes. The author fixed the number of possible classes to K for the whole dataset. \n\nOne missing explanation is that one song is equal to a batch to be sure that all the selected points after the beat track step below the same song to avoid refining them with features from other songs. How does this affect the training? i.e. since all the points in a batch share the same musical traits, isn&#39;t that problematic when propagating the gradient? \n\n- RESULTS AND DISCUSSION - \n\nThe authors validate their model on two datasets, RWC-Pop and Harmonix. However, they don&#39;t use the Jazz Structure Dataset (JSD) or Salami, which are commonly used to benchmark the music structure task.\nIt is good to see a k-fold cross-validation study and a cross-dataset evaluation. This helps assess the model&#39;s actual performance while providing proper metric ranges.\n\nI appreciate the ablation study, but it misses some important experiments. I miss some further studies of each step independently:\n\n1- On the feature extraction step to address the quality of the SSM matrices obtained. This could have been done by comparing them directly with the ground-truth annotation. Moreover, the benefit of the Feature refinement block has yet to be tested. How much does it contribute to the overall performance? Is it better than computing the adjacency matrix on top of the feature produced by the feature encoder? The fact that the results drop so much when removing the Link Feature block may indicate that the obtained SSM A&#39; is not informative enough for detecting structures. \n\n2— Link Feature block: Since this block can process any SSM, how powerful is it? I would have been curious to see the performance when training this block on top of another SSM. For instance, a good baseline could have been to compare the performance obtained with SSM obtained from well-known musical feature descriptors such as chroma, MFCC, or tempo grams vs. their proposed feature extraction method. \n\n3- Combining link features with acoustics features. Since this block consists of an attention mechanism that combines X&#39; and E&#39;, how does it work for the ablation experiment where the link feature (which produces E&#39;) is removed? Similarly, how informative is the E&#39; matrix w.r.t the final tasks? Why do we need a combination of both?\n\nA good point is the small size of the model—only 330K. To better understand its significance, it would have been more informative to include the number of parameters of the compared model.  \n\nThe significant insight here is that their model can effectively generate a feature vector X&#39;&#39; that captures structural section information. This is an interesting observation because their feature vector is correlated with meaningful musical sections. I&#39;m curious about how widely applicable this insight is. The authors have set the number of sections to K = 7 (they don&#39;t mention their taxonomy), which may not be representative of all possible music sections but is likely sufficient to cover most popular Western music.\n\n— FINAL COMMENT —\nThe utilization of graph neural networks is quite fascinating. The pairwise relationship hypothesis is intriguing, and the outcomes emphasize the grouping capabilities of their final feature vector. However, they introduce a three-step model that is primarily tested as a whole, making it difficult to evaluate the contribution of each component as well as their reusability out of their full formalisation. Furthermore, many of the design choices made need to be justified or properly evaluated. &#34;, &#39;meta_review&#39;: &#39;The reviewers ranged widely in their view of this paper. All had constructive criticisms, on different aspects of the paper. The concerns included:\n\n- lack of context about the kind of music assumed to be the target of the method;\n- lack of certainty that the evaluation is fair;\n- lack of motivation for using graph neural networks, instead of a simpler Transformer approach;\n- lack of depth in the ablation study.\n\nIn the discussion between reviewers, we all agreed that these were valid concerns, but we disagreed about their severity. We hope that the authors can take all of the suggestions on board as they revise this paper.&#39;, &#39;author_changes&#39;: &#39;We thank the reviewers and meta-reviewer for their valuable insight and feedback on our work. We address here the main concerns that emerged through the reviewing process:\n\n- Lack of context about the kind of music assumed to be the target of the method: We added a mention in our contribution that this work addresses the analysis of structure mostly for western popular music. This decision was based upon the availability of annotated data and the label taxonomy employed in previous work, so as to ease comparison with our method. In fact, the extension of the approach to different types of musical structures is a research direction we aim to undertake, and mention it in the conclusion of the paper. \n\n- Lack of certainty that the evaluation is fair: Our evaluation process has tried to closely follow that of previous work, where baseline systems are evaluated both in cross-validation and cross-dataset settings. However, comparisons should still be cautiously apprehended as some baselines used additional training data or augmentation strategies. Concerning the reported baseline results, we added a row in Table 1 to include the different configurations of one of them (SpecTNT). \n\n- Lack of motivation for using graph neural networks, instead of a simpler Transformer approach: The graph neural network framework is actually a generalization of the transformer approach when applied on a fully-connected graph. We added however a few words as to why graph neural networks are employed in our method (mainly to include link features into the attention coefficients calculation and the frame feature update).\n\n- Lack of depth in the ablation study: Our initial ablation study aimed at focusing on the most peculiar aspects of our method. However, we ran additional ablation experiments by also discarding the remaining steps of our method and updated Figure 4 accordingly. &#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>