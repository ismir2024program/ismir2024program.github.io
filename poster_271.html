

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
  <!-- {"TLDR": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.", "abstract": "Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.", "author_changes": "Here is the summary of the changes we made to address the reviewers\u2019 critiques:\n- Global changes: we mostly replaced the words \"synthetic\" and \"spoofing\" by \"cloned\" throughout this paper. The former two words caused confusion amongst reviewers. As such, the title of the paper is now \"From Real to Cloned Singer Identification\". We also use percentages in all figures to match our other results\u2019 reporting. Finally, the words \"music streaming service\" are replaced by Deezer now that anonymity is not needed.\n- Section 3.1: we remove the term \"valid\" to describe a song, clarify that a \"unique singer\" here means a track with \"more than one singer\", highlight that \"singer\" annotations are collected to create the closed dataset, and edit out the reference to the current section.\n- Section 3.2: We explicitly list the positive stem pairs that can occur during the Hybrid model\u2019s pre-training. We also emphasise that only mixtures are used during the downstream singer identification task when the Hybrid backbone is used.\n- We clarify the train and validation splits used in Sections 3.2 and 3.3.\n- Section 3.3: we emphasise that the classifiers have the same architecture as the projector head.\n- We emphasise that the CLMR embeddings in Section 4.1 are used for both training and testing our classifiers for the real singer identification task on open datasets. We also better described the \"majority vote\" scheme used to generate classification results.\n- Section 4.2: We enlarge Figure 1 for better clarity. We also introduce a table to compare our results with others in the literature. As such, the paragraph that used to do so is drastically shortened. Finally, we add a description of the genres\u2019 distribution in Figure 2\u2019s caption.\n- Section 4.3: We clarify Figure 4\u2019s caption by directly referring to each plot\u2019s colour and legend.\nNote that we also slightly reformulated some sections of the paper and resized some figures and tables to make everything fit in six pages!", "authors": ["Desblancs, Dorian*", " Meseguer Brocal, Gabriel", " Hennequin, Romain", " Moussallam, Manuel"], "authors_and_affil": ["Dorian Desblancs (Deezer Research)*", " Gabriel Meseguer Brocal (Deezer)", " Romain Hennequin (Deezer Research)", " Manuel Moussallam (Deezer)"], "channel_url": "https://ismir2024.slack.com/archives/C07UQ1MJEBC", "day": "1", "keywords": ["MIR tasks -\u003e automatic classification", "Generative Tasks -\u003e music and audio synthesis; MIR fundamentals and methodology -\u003e music signal processing; MIR tasks -\u003e sound source separation; Musical features and properties -\u003e representations of music; Musical features and properties -\u003e timbre, instrumentation, and singing voice"], "long_presentation": "FALSE", "meta_review": "The reviews of this paper found the task to be important and the experimental validation to be thorough and rigorous. While there were comments about the paper\u0027s clarity and title that we hope the authors will address, we feel that this paper will make a strong candidate for publication at this year\u0027s ISMIR.", "paper_presentation": "San Francisco", "pdf_path": "https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link", "poster_pdf": "https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/view?usp=drive_link", "review_1": "This paper explores the use of contrastive learning for learning embeddings for the task of singer identification in three configurations: mixture, vocal and hybrid. The experiments show promising results, especially in the case of using the vocal model to identify real singers (as opposed to synthetic). When the instrumental stems are present in the pre-training (mixture or hybrid model) the performance is reduced. The models perform particularly worse in the case of identifying synthetic versions of the singers. These observations are explained and explored in the paper in detail. I just have a few small suggestions:\n\n- In line 129, annotations are mentioned. However, it is unclear to me what these annotations contain and how they were used in the experiments (I\u2019m assuming the name of the artists were used, but was there anything else?).\n- In Figure 1, the Closed dataset plot is a bit too jammed together. I\u2019d recommend plotting the accuracies of the models next to each other, rather than on top of each other.\n- The observation of the non-uniform performance over musical genres (lines 322--346) would benefit from mentioning the distribution of genres in the training datasets.\n- Figure 4 (as well as its caption) is a bit confusing. I\u2019d recommend to, instead of using numbers in the caption, mention the bars themselves (e.g. \u201cthe purple bars (test/other) show \u2026 \u201c). I\u2019d also suggest to put the Mixture, Hybrid, Vocal as subplot titles rather than on the right.\n\nAside from these minimal suggestions, I believe the paper is well written and complete, making it a very good contribution to ISMIR. ", "review_2": "The strengths of this paper is that (a) it provides a massive dataset (with detailed description), and (b) it confirms a system trained on the real inputs alone is not capable to perform well in the synethic inputs; This implies the system should also be trained with the synthetic inputs.\nThus, the major weakness of this paper is that it triggers the reader to ask why this paper does not also examine a singing identification system trained with synthetic inputs and both (real and synthetic). \nThus, in order to match the claim of this paper, below is the suggestion. Focus on 3 systems first (trained with (a) real, (b) synthetic, and (c) real + synthetic, and then their performance on 3 types of dataset (a) real alone, (b) synthetic alone, and (c) both real + synthetic), resulting 9 types of performance result. Given such performance results, and/or with the optional variety like genres, this paper would be more easier to read and its strengths would be strengthened. ", "review_3": "This paper trained the embedding of singing features by singer-level contrastive learning.\nUsing a singer identification model with the projector head removed, the paper reported the differences in classification  rates and characteristics between human singing voices and synthesized singing voices. NT-Xent was used for losses in embedded model training. Three types of models were trained in embedding learning: targeting separated singing voice, mixture, and their hybrid.\n\nIn an evaluation using the open dataset, the Mixture-based embedding model outperformed CLMR [36] as a conventional method. Compared to training the CLMR model, the proposed model used less than 1/20 of the number of tracks, indicating the effectiveness of singer-level sampling.\n\nIn the evaluation with the closed dataset, the paper experimented with a subset of 100 to 1000 classes. Although the performance was better with the singing voices separated, the difference in performance was small. Although it is difficult to compare performance with previous studies using inhouse datasets, this paper believe that the proposed model is at least as good. In addition, this paper showed that there can be differences in classification performance due to musical genres and difficulties in classification for singers with many songs.\n\nEvaluations using synthetic singing datasets showed that performance was lower than when targeting real singing voices. In particular, the performance of the Mixture or Hybrid model was degraded. To investigate the cause of the performance degradation, results on the cosine similarity of the embedding vectors were also reported, such as higher similarity for the instrumental embeddings in the Mixture or Hybrid models.\n\nThe paper is well written and has adequate references. The fact that the performance of this method is equal to or better than conventional methods when targeting human singing voices and the fact that it investigates the factors that cause performance degradation are good.\nFuture directions following this paper include efforts to improve performance on synthetic singing voices.\n\nThe following are the issues that need to be corrected.\n- Line 209: The details of the architecture for singer identification are not clear and need to be stated.\n- Line 400: Figure 4 is discussed in the text without explanation and is difficult to understand. The explanation is needed. The following comments are also relevant.\n- Line 405: \"the instrumental embeddings\" is the first occurrence of here, at least in the text, and it is unclear how this was obtained.\n- Figure 4: It is necessary to specify which legend each of the explanations 1) to 5) corresponds to.\n\nIn addition, the following are minor comments, but these should also be corrected.\n- Line 175: Regarding \"Finally, for our Hybrid model, these segments are randomly sampled from either the songs\u2019 mixtures or their vocal stems\", it is unclear whether it means that mixture-mixture and vocal-vocal pairs are mixed in B=128, or whether mixture-vocal is also possible.\n- Line 219: Regarding \"At least three tracks per singer are then used for training.\", different singers have different numbers of training data?\n- Figure 4: I find the notation of each legend a little hard to understand.\n", "session": ["2"], "slack_channel": "p2-16-from-real-to", "slides_pdf": "https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link", "title": "From Real to Cloned Singer Identification", "video": "https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/view?usp=drive_link"} -->
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            From Real to Cloned Singer Identification
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Dorian Desblancs (Deezer Research)*"
               class="text-muted"
            >Dorian Desblancs (Deezer Research)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Gabriel Meseguer Brocal (Deezer)"
               class="text-muted"
            > Gabriel Meseguer Brocal (Deezer)</a
            >,
            
            <a href="papers.html?filter=authors&search= Romain Hennequin (Deezer Research)"
               class="text-muted"
            > Romain Hennequin (Deezer Research)</a
            >,
            
            <a href="papers.html?filter=authors&search= Manuel Moussallam (Deezer)"
               class="text-muted"
            > Manuel Moussallam (Deezer)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07UQ1MJEBC" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p2-16-from-real-to</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; automatic classification"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; automatic classification</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; music signal processing; MIR tasks -&gt; sound source separation; Musical features and properties -&gt; representations of music; Musical features and properties -&gt; timbre, instrumentation, and singing voice"
                    class="text-secondary text-decoration-none"
            >Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; music signal processing; MIR tasks -&gt; sound source separation; Musical features and properties -&gt; representations of music; Musical features and properties -&gt; timbre, instrumentation, and singing voice</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation"> 
        <a class="nav-link" id="slides-tab" data-toggle="tab" href="#slides" role="tab" aria-controls="slides" aria-selected="false">Slides</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
 
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive" style="height: 80vh;">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="slides" role="tabpanel" aria-labelledby="slides-tab"> 
      
        <div class="text-center"> 
          <div class="embed-responsive" style="height:80vh;"> 
            <iframe class="embed-responsive-item" 
                    src="https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/preview?usp=drive_link" 
                    frameborder="0" 
                    allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                  </iframe>
          </div>
        </div>
      
    </div>


    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>The reviews of this paper found the task to be important and the experimental validation to be thorough and rigorous. While there were comments about the paper's clarity and title that we hope the authors will address, we feel that this paper will make a strong candidate for publication at this year's ISMIR.</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>This paper explores the use of contrastive learning for learning embeddings for the task of singer identification in three configurations: mixture, vocal and hybrid. The experiments show promising results, especially in the case of using the vocal model to identify real singers (as opposed to synthetic). When the instrumental stems are present in the pre-training (mixture or hybrid model) the performance is reduced. The models perform particularly worse in the case of identifying synthetic versions of the singers. These observations are explained and explored in the paper in detail. I just have a few small suggestions:</p>
<ul>
<li>In line 129, annotations are mentioned. However, it is unclear to me what these annotations contain and how they were used in the experiments (I’m assuming the name of the artists were used, but was there anything else?).</li>
<li>In Figure 1, the Closed dataset plot is a bit too jammed together. I’d recommend plotting the accuracies of the models next to each other, rather than on top of each other.</li>
<li>The observation of the non-uniform performance over musical genres (lines 322--346) would benefit from mentioning the distribution of genres in the training datasets.</li>
<li>Figure 4 (as well as its caption) is a bit confusing. I’d recommend to, instead of using numbers in the caption, mention the bars themselves (e.g. “the purple bars (test/other) show … “). I’d also suggest to put the Mixture, Hybrid, Vocal as subplot titles rather than on the right.</li>
</ul>
<p>Aside from these minimal suggestions, I believe the paper is well written and complete, making it a very good contribution to ISMIR. </p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>The strengths of this paper is that (a) it provides a massive dataset (with detailed description), and (b) it confirms a system trained on the real inputs alone is not capable to perform well in the synethic inputs; This implies the system should also be trained with the synthetic inputs.
Thus, the major weakness of this paper is that it triggers the reader to ask why this paper does not also examine a singing identification system trained with synthetic inputs and both (real and synthetic). 
Thus, in order to match the claim of this paper, below is the suggestion. Focus on 3 systems first (trained with (a) real, (b) synthetic, and (c) real + synthetic, and then their performance on 3 types of dataset (a) real alone, (b) synthetic alone, and (c) both real + synthetic), resulting 9 types of performance result. Given such performance results, and/or with the optional variety like genres, this paper would be more easier to read and its strengths would be strengthened. </p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper trained the embedding of singing features by singer-level contrastive learning.
Using a singer identification model with the projector head removed, the paper reported the differences in classification  rates and characteristics between human singing voices and synthesized singing voices. NT-Xent was used for losses in embedded model training. Three types of models were trained in embedding learning: targeting separated singing voice, mixture, and their hybrid.</p>
<p>In an evaluation using the open dataset, the Mixture-based embedding model outperformed CLMR [36] as a conventional method. Compared to training the CLMR model, the proposed model used less than 1/20 of the number of tracks, indicating the effectiveness of singer-level sampling.</p>
<p>In the evaluation with the closed dataset, the paper experimented with a subset of 100 to 1000 classes. Although the performance was better with the singing voices separated, the difference in performance was small. Although it is difficult to compare performance with previous studies using inhouse datasets, this paper believe that the proposed model is at least as good. In addition, this paper showed that there can be differences in classification performance due to musical genres and difficulties in classification for singers with many songs.</p>
<p>Evaluations using synthetic singing datasets showed that performance was lower than when targeting real singing voices. In particular, the performance of the Mixture or Hybrid model was degraded. To investigate the cause of the performance degradation, results on the cosine similarity of the embedding vectors were also reported, such as higher similarity for the instrumental embeddings in the Mixture or Hybrid models.</p>
<p>The paper is well written and has adequate references. The fact that the performance of this method is equal to or better than conventional methods when targeting human singing voices and the fact that it investigates the factors that cause performance degradation are good.
Future directions following this paper include efforts to improve performance on synthetic singing voices.</p>
<p>The following are the issues that need to be corrected.
- Line 209: The details of the architecture for singer identification are not clear and need to be stated.
- Line 400: Figure 4 is discussed in the text without explanation and is difficult to understand. The explanation is needed. The following comments are also relevant.
- Line 405: "the instrumental embeddings" is the first occurrence of here, at least in the text, and it is unclear how this was obtained.
- Figure 4: It is necessary to specify which legend each of the explanations 1) to 5) corresponds to.</p>
<p>In addition, the following are minor comments, but these should also be corrected.
- Line 175: Regarding "Finally, for our Hybrid model, these segments are randomly sampled from either the songs’ mixtures or their vocal stems", it is unclear whether it means that mixture-mixture and vocal-vocal pairs are mixed in B=128, or whether mixture-vocal is also possible.
- Line 219: Regarding "At least three tracks per singer are then used for training.", different singers have different numbers of training data?
- Figure 4: I find the notation of each legend a little hard to understand.</p></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>Here is the summary of the changes we made to address the reviewers’ critiques:
- Global changes: we mostly replaced the words "synthetic" and "spoofing" by "cloned" throughout this paper. The former two words caused confusion amongst reviewers. As such, the title of the paper is now "From Real to Cloned Singer Identification". We also use percentages in all figures to match our other results’ reporting. Finally, the words "music streaming service" are replaced by Deezer now that anonymity is not needed.
- Section 3.1: we remove the term "valid" to describe a song, clarify that a "unique singer" here means a track with "more than one singer", highlight that "singer" annotations are collected to create the closed dataset, and edit out the reference to the current section.
- Section 3.2: We explicitly list the positive stem pairs that can occur during the Hybrid model’s pre-training. We also emphasise that only mixtures are used during the downstream singer identification task when the Hybrid backbone is used.
- We clarify the train and validation splits used in Sections 3.2 and 3.3.
- Section 3.3: we emphasise that the classifiers have the same architecture as the projector head.
- We emphasise that the CLMR embeddings in Section 4.1 are used for both training and testing our classifiers for the real singer identification task on open datasets. We also better described the "majority vote" scheme used to generate classification results.
- Section 4.2: We enlarge Figure 1 for better clarity. We also introduce a table to compare our results with others in the literature. As such, the paragraph that used to do so is drastically shortened. Finally, we add a description of the genres’ distribution in Figure 2’s caption.
- Section 4.3: We clarify Figure 4’s caption by directly referring to each plot’s colour and legend.
Note that we also slightly reformulated some sections of the paper and resized some figures and tables to make everything fit in six pages!</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;271&#39;, &#39;session&#39;: &#39;2&#39;, &#39;position&#39;: &#39;17&#39;, &#39;forum&#39;: &#39;271&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1QDJhWyLpD5Oheyeq0z7cFInOtFksVAxr/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;From Real to Cloned Singer Identification&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Desblancs, Dorian*&#39;, &#39; Meseguer Brocal, Gabriel&#39;, &#39; Hennequin, Romain&#39;, &#39; Moussallam, Manuel&#39;], &#39;authors_and_affil&#39;: [&#39;Dorian Desblancs (Deezer Research)*&#39;, &#39; Gabriel Meseguer Brocal (Deezer)&#39;, &#39; Romain Hennequin (Deezer Research)&#39;, &#39; Manuel Moussallam (Deezer)&#39;], &#39;keywords&#39;: [&#39;MIR tasks -&gt; automatic classification&#39;, &#39;Generative Tasks -&gt; music and audio synthesis; MIR fundamentals and methodology -&gt; music signal processing; MIR tasks -&gt; sound source separation; Musical features and properties -&gt; representations of music; Musical features and properties -&gt; timbre, instrumentation, and singing voice&#39;], &#39;abstract&#39;: &#39;Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.&#39;, &#39;TLDR&#39;: &#39;Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1NScXZ7Ut0OslxqE4kqxtDEqZMU6qYr9T/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;2&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1JbDn3mE6q9S8gzy0-xgnIrTPbZZ7jPuz/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07UQ1MJEBC&#39;, &#39;slack_channel&#39;: &#39;p2-16-from-real-to&#39;, &#39;slides_pdf&#39;: &#39;https://drive.google.com/file/d/1okGYXLSh31trEZ8_jDqM_lODG0_TzDNK/view?usp=drive_link&#39;, &#39;day&#39;: &#39;1&#39;, &#39;review_1&#39;: &#39;This paper explores the use of contrastive learning for learning embeddings for the task of singer identification in three configurations: mixture, vocal and hybrid. The experiments show promising results, especially in the case of using the vocal model to identify real singers (as opposed to synthetic). When the instrumental stems are present in the pre-training (mixture or hybrid model) the performance is reduced. The models perform particularly worse in the case of identifying synthetic versions of the singers. These observations are explained and explored in the paper in detail. I just have a few small suggestions:\n\n- In line 129, annotations are mentioned. However, it is unclear to me what these annotations contain and how they were used in the experiments (I’m assuming the name of the artists were used, but was there anything else?).\n- In Figure 1, the Closed dataset plot is a bit too jammed together. I’d recommend plotting the accuracies of the models next to each other, rather than on top of each other.\n- The observation of the non-uniform performance over musical genres (lines 322--346) would benefit from mentioning the distribution of genres in the training datasets.\n- Figure 4 (as well as its caption) is a bit confusing. I’d recommend to, instead of using numbers in the caption, mention the bars themselves (e.g. “the purple bars (test/other) show … “). I’d also suggest to put the Mixture, Hybrid, Vocal as subplot titles rather than on the right.\n\nAside from these minimal suggestions, I believe the paper is well written and complete, making it a very good contribution to ISMIR. &#39;, &#39;review_2&#39;: &#39;The strengths of this paper is that (a) it provides a massive dataset (with detailed description), and (b) it confirms a system trained on the real inputs alone is not capable to perform well in the synethic inputs; This implies the system should also be trained with the synthetic inputs.\nThus, the major weakness of this paper is that it triggers the reader to ask why this paper does not also examine a singing identification system trained with synthetic inputs and both (real and synthetic). \nThus, in order to match the claim of this paper, below is the suggestion. Focus on 3 systems first (trained with (a) real, (b) synthetic, and (c) real + synthetic, and then their performance on 3 types of dataset (a) real alone, (b) synthetic alone, and (c) both real + synthetic), resulting 9 types of performance result. Given such performance results, and/or with the optional variety like genres, this paper would be more easier to read and its strengths would be strengthened. &#39;, &#39;review_3&#39;: &#39;This paper trained the embedding of singing features by singer-level contrastive learning.\nUsing a singer identification model with the projector head removed, the paper reported the differences in classification  rates and characteristics between human singing voices and synthesized singing voices. NT-Xent was used for losses in embedded model training. Three types of models were trained in embedding learning: targeting separated singing voice, mixture, and their hybrid.\n\nIn an evaluation using the open dataset, the Mixture-based embedding model outperformed CLMR [36] as a conventional method. Compared to training the CLMR model, the proposed model used less than 1/20 of the number of tracks, indicating the effectiveness of singer-level sampling.\n\nIn the evaluation with the closed dataset, the paper experimented with a subset of 100 to 1000 classes. Although the performance was better with the singing voices separated, the difference in performance was small. Although it is difficult to compare performance with previous studies using inhouse datasets, this paper believe that the proposed model is at least as good. In addition, this paper showed that there can be differences in classification performance due to musical genres and difficulties in classification for singers with many songs.\n\nEvaluations using synthetic singing datasets showed that performance was lower than when targeting real singing voices. In particular, the performance of the Mixture or Hybrid model was degraded. To investigate the cause of the performance degradation, results on the cosine similarity of the embedding vectors were also reported, such as higher similarity for the instrumental embeddings in the Mixture or Hybrid models.\n\nThe paper is well written and has adequate references. The fact that the performance of this method is equal to or better than conventional methods when targeting human singing voices and the fact that it investigates the factors that cause performance degradation are good.\nFuture directions following this paper include efforts to improve performance on synthetic singing voices.\n\nThe following are the issues that need to be corrected.\n- Line 209: The details of the architecture for singer identification are not clear and need to be stated.\n- Line 400: Figure 4 is discussed in the text without explanation and is difficult to understand. The explanation is needed. The following comments are also relevant.\n- Line 405: &#34;the instrumental embeddings&#34; is the first occurrence of here, at least in the text, and it is unclear how this was obtained.\n- Figure 4: It is necessary to specify which legend each of the explanations 1) to 5) corresponds to.\n\nIn addition, the following are minor comments, but these should also be corrected.\n- Line 175: Regarding &#34;Finally, for our Hybrid model, these segments are randomly sampled from either the songs’ mixtures or their vocal stems&#34;, it is unclear whether it means that mixture-mixture and vocal-vocal pairs are mixed in B=128, or whether mixture-vocal is also possible.\n- Line 219: Regarding &#34;At least three tracks per singer are then used for training.&#34;, different singers have different numbers of training data?\n- Figure 4: I find the notation of each legend a little hard to understand.\n&#39;, &#39;meta_review&#39;: &#34;The reviews of this paper found the task to be important and the experimental validation to be thorough and rigorous. While there were comments about the paper&#39;s clarity and title that we hope the authors will address, we feel that this paper will make a strong candidate for publication at this year&#39;s ISMIR.&#34;, &#39;author_changes&#39;: &#39;Here is the summary of the changes we made to address the reviewers’ critiques:\n- Global changes: we mostly replaced the words &#34;synthetic&#34; and &#34;spoofing&#34; by &#34;cloned&#34; throughout this paper. The former two words caused confusion amongst reviewers. As such, the title of the paper is now &#34;From Real to Cloned Singer Identification&#34;. We also use percentages in all figures to match our other results’ reporting. Finally, the words &#34;music streaming service&#34; are replaced by Deezer now that anonymity is not needed.\n- Section 3.1: we remove the term &#34;valid&#34; to describe a song, clarify that a &#34;unique singer&#34; here means a track with &#34;more than one singer&#34;, highlight that &#34;singer&#34; annotations are collected to create the closed dataset, and edit out the reference to the current section.\n- Section 3.2: We explicitly list the positive stem pairs that can occur during the Hybrid model’s pre-training. We also emphasise that only mixtures are used during the downstream singer identification task when the Hybrid backbone is used.\n- We clarify the train and validation splits used in Sections 3.2 and 3.3.\n- Section 3.3: we emphasise that the classifiers have the same architecture as the projector head.\n- We emphasise that the CLMR embeddings in Section 4.1 are used for both training and testing our classifiers for the real singer identification task on open datasets. We also better described the &#34;majority vote&#34; scheme used to generate classification results.\n- Section 4.2: We enlarge Figure 1 for better clarity. We also introduce a table to compare our results with others in the literature. As such, the paragraph that used to do so is drastically shortened. Finally, we add a description of the genres’ distribution in Figure 2’s caption.\n- Section 4.3: We clarify Figure 4’s caption by directly referring to each plot’s colour and legend.\nNote that we also slightly reformulated some sections of the paper and resized some figures and tables to make everything fit in six pages!&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>