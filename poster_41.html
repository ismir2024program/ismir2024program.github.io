

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>
            <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Gantari:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Library libs_ext --> 
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer" />


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>ISMIR 2024</title>
    
<meta name="citation_title" content=""/>

<meta name="citation_publication_date" content="May 2024"/>
<meta name="citation_conference_title"
      content="Ismir 2024"/>
<meta name="citation_inbook_title" content="Proceedings of the 25th International Society for Music Information Retrieval"/>
<meta name="citation_abstract" content=""/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/ismir24_logo.svg"
                    height="auto"
            width="95px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Schedule</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Papers</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="tutorials.html">Tutorials</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="lbds.html">LBDs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="industry.html">Industry</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="jobs.html">Jobs</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        
<div class="pp-card m-3" style="background-color: #fff">
    <div class="card-header" style="background-color: rgba(0,0,0,0);">
        <h2 class="card-title main-title text-left" style="color: #4383EC">
            Audio Conditioning for Music Generation via Discrete Bottleneck Features
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-left">
            
            <a href="papers.html?filter=authors&search=Simon Rouard (Meta AI Research)*"
               class="text-muted"
            >Simon Rouard (Meta AI Research)*</a
            >,
            
            <a href="papers.html?filter=authors&search= Alexandre Defossez (Kyutai)"
               class="text-muted"
            > Alexandre Defossez (Kyutai)</a
            >,
            
            <a href="papers.html?filter=authors&search= Yossi Adi (Facebook AI Research )"
               class="text-muted"
            > Yossi Adi (Facebook AI Research )</a
            >,
            
            <a href="papers.html?filter=authors&search= Jade Copet (Meta AI Research)"
               class="text-muted"
            > Jade Copet (Meta AI Research)</a
            >,
            
            <a href="papers.html?filter=authors&search= Axel Roebel (IRCAM)"
               class="text-muted"
            > Axel Roebel (IRCAM)</a
            >
            
        </h3>
        
        <div class="btn-group mb-3 mt-3">
          <a href="https://ismir2024.slack.com/archives/C07V2MM27CZ" class="btn btn-primary" style="background-color: #2294e0;"><svg style="display: inline-block; width: 23px;" version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="75 75 150 150" style="enable-background:new 0 0 270 270;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#FFFFFF;}
</style>
<g>
	<g>
		<path class="st0" d="M99.4,151.2c0,7.1-5.8,12.9-12.9,12.9s-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h12.9V151.2z"/>
		<path class="st0" d="M105.9,151.2c0-7.1,5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v32.3c0,7.1-5.8,12.9-12.9,12.9
			s-12.9-5.8-12.9-12.9C105.9,183.5,105.9,151.2,105.9,151.2z"/>
	</g>
	<g>
		<path class="st0" d="M118.8,99.4c-7.1,0-12.9-5.8-12.9-12.9s5.8-12.9,12.9-12.9s12.9,5.8,12.9,12.9v12.9H118.8z"/>
		<path class="st0" d="M118.8,105.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9H86.5c-7.1,0-12.9-5.8-12.9-12.9
			s5.8-12.9,12.9-12.9C86.5,105.9,118.8,105.9,118.8,105.9z"/>
	</g>
	<g>
		<path class="st0" d="M170.6,118.8c0-7.1,5.8-12.9,12.9-12.9c7.1,0,12.9,5.8,12.9,12.9s-5.8,12.9-12.9,12.9h-12.9V118.8z"/>
		<path class="st0" d="M164.1,118.8c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9V86.5c0-7.1,5.8-12.9,12.9-12.9
			c7.1,0,12.9,5.8,12.9,12.9V118.8z"/>
	</g>
	<g>
		<path class="st0" d="M151.2,170.6c7.1,0,12.9,5.8,12.9,12.9c0,7.1-5.8,12.9-12.9,12.9c-7.1,0-12.9-5.8-12.9-12.9v-12.9H151.2z"/>
		<path class="st0" d="M151.2,164.1c-7.1,0-12.9-5.8-12.9-12.9c0-7.1,5.8-12.9,12.9-12.9h32.3c7.1,0,12.9,5.8,12.9,12.9
			c0,7.1-5.8,12.9-12.9,12.9H151.2z"/>
	</g>
</g>
</svg> p1-13-audio-conditioning-for</a>
        </div>
        
        <p class="card-text text-left">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=MIR tasks -&gt; music generation"
                    class="text-secondary text-decoration-none"
            >MIR tasks -&gt; music generation</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=Creativity -&gt; human-ai co-creativity; Generative Tasks -&gt; music and audio synthesis; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR fundamentals and methodology -&gt; music signal processing"
                    class="text-secondary text-decoration-none"
            >Creativity -&gt; human-ai co-creativity; Generative Tasks -&gt; music and audio synthesis; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR fundamentals and methodology -&gt; music signal processing</a
            >
            
        </p>
    </div>

</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                <p>While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding "pseudowords" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.</p>
            </div>
        </div>
        <p></p>
    </div>
</div>


  
  <div class="text-center">
    <ul class="nav nav-tabs" id="posterTabs" role="tablist">
      <li class="nav-item" role="presentation">
        <a class="nav-link active" id="paper-tab" data-toggle="tab" href="#paper" role="tab" aria-controls="paper" aria-selected="true">Paper</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="poster-tab" data-toggle="tab" href="#poster" role="tab" aria-controls="poster" aria-selected="false">Poster</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="video-tab" data-toggle="tab" href="#video" role="tab" aria-controls="video" aria-selected="false">Video</a>
      </li>
      <li class="nav-item" role="presentation">
        <a class="nav-link" id="Reviews-tab" data-toggle="tab" href="#Reviews" role="tab" aria-controls="Reviews" aria-selected="false">Reviews</a>
      </li>
    </ul>
  </div>
  

  <div class="tab-content" id="posterTabsContent">
    <div class="tab-pane fade show active" id="paper" role="tabpanel" aria-labelledby="paper-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1AXwd4qZXBWY7jJ12ASDW13nKpzPeW910/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="poster" role="tabpanel" aria-labelledby="poster-tab">
      
      <div style="height: 80vh;">
        <iframe src="https://drive.google.com/file/d/1I1TzCEWABu-BD52VU9EDr0hjPFr3Cg1k/preview?usp=drive_link" 
                width="100%" 
                height="100%" 
                frameborder="0"
                allow="autoplay"></iframe>
      </div>
      
    </div>
  
    <div class="tab-pane fade" id="video" role="tabpanel" aria-labelledby="video-tab">
      
      <div class="text-center">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"           
                  src="https://drive.google.com/file/d/1C_XUyhaIMcDv04fFALbIhQemHpVAyYw9/preview?usp=drive_link" 
                  frameborder="0" 
                  allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen></iframe>
        </div>
      </div>
      
    </div>
    <div class="tab-pane fade" id="Reviews" role="tabpanel" aria-labelledby="Reviews-tab">
      <div class="pp-card m-3">
        <div class="card-body">
          <h5 class="card-title">
            Reviews
          </h5>
          <div class="card-text">
            
            <div> 
                <b> Meta Review </b> 
                <p> <p>This paper was generally well received by the reviewers. Congrats, authors!</p>
<p>Please, address the reviewers comments as closely as possible. Especially the following:
- Cite the appropriate previous work (i.e., this is not the only audio-conditioned music generative model)
- Have a proficient English speaker to proofread the manuscript</p></p>
              </div>
              <hr/>
        
            <div>
                <b> Review 1 </b>
                <p class="card-text"><p>The paper is well-written, with an extensive related work section.  The accompanying website features many interesting examples and code will be released on acceptance. The authors propose to investigate the use of textual inversion with a pretrained MusicGen model to generate variations of an existing song.</p>
<p>Two similarity metrics are introduced to spot copies in the generated material.</p>
<p>My main concern is about the original motivation and statement of the problem. If textual inversion proves to be able to capture audio features from the audio conditioning and regenerate audio with similar characteristics, we can regret that this is very close from standard continuation and that the possibilities offered by textual inversion are not fully exploited. Examples like "Chill lofi remix ex. 1" from the demo website is very similar to the prompt for instance, </p>
<p>In this particular case, the learnt textual embeddings are only learnt from one song: I would have liked to see more general and diverse applications. 
For instance, is it possible to learn new styles / instruments / chord progressions based on a collection of songs? (in this paper style is often used as a replacement for song, which may be misleading). If such application is closer to what's mentioned in the introduction "Given a few images (3-5) of a concept or object", it is never discussed in the paper. It would also remove the necessity to introduce bottlenecks.</p>
<p>Some questions:
"For some song, we never achieve to obtain hearable
392 music as the result suffers from glitches, and tempo instabilities.": is it possible to automatically know when this method fails?</p>
<p>Some things to clarify in §4.4:
-nearest neighbours written i_1^C but chunks indexed with i,j: this is confusing, is it done on the chunk or song level?
-"However, if a model copies the conditioning (i.e. xG ≈ xC) the metric will tend to 1, we thus need a second metric to avoid xG and xC being too similar." it sounds as if the metric had an influence on the generation process
- G is the Nearest Neighbor of C: should be clearer for a subtitle, what are G and C?</p>
<p>Interesting paper but this may seem like a straightforward application of the textual inversion method. Choosing the audio condition from the same song when performing the textual inversion may not be the best use case as this forces the introduction of bottlenecks in the feature extractor. The comparision with different feature extractors is interesting, especially the fact that  "Self-supervised encoder like MERT and MusicFM outperforms low level acoustic models like EnCodec.". Demos are of good quality and well presented.</p></p>
              </div>
            </div>
            <hr/>

            <div> 
                <b> Review 2 </b>
                <p>Very well written and complete paper with a lot of impressive details and convincing demo results. I have troubles finding anything majorly wrong with this work and just have to say kudos for the impressive piece of work!</p></p>
              </div>
              <hr/>


            <div> 
                <b> Review 3 </b>
                <p>This paper introduces a method for generating music conditioned on other music — using a high-level semantic embedding from another piece of music as conditioning. </p>
<p>The authors evaluate their method against a textual inversion baseline (adapted from the image domain by the authors) and find that their proposed method is faster and generates higher-quality audio than the given baselines. </p>
<p>The authors perform ablations of several aspects of their system and discuss challenges, like how text conditioning is ignored when audio conditioning is present, and how including too much information from a style embedding can lead to the model perfectly reconstructing the audio. </p>
<p>Overall, I like the authors' approach and find their evaluation sufficient. My recommendation is Strong Accept.</p>
<p><strong>strengths</strong></p>
<ul>
<li>The claims of the paper are held together well by objective experiments and a human listening study.</li>
<li>The listening examples demonstrate that the author’s proposed method generates high-quality music that “riffs” on the musical idea given in an audio “style” prompt.</li>
</ul>
<p><strong>weaknesses</strong></p>
<ul>
<li>While the KNN-based objective metrics make intuitive sense, I wonder if there’s a more interpretable way to observe the effects of which aspects of the “style” get preserved or not.  For example, I could imagine it is possible to check if the generated audio has too similar of a tempo/accents/rhythms/harmony/melodic contour as the style reference audio by leveraging pretrained MIR models/feature extractors to extract these music descriptors from both the reference and generated audio.</li>
<li>The paper claims to be the only existing audio-conditioned music generative model, but VampNet (ISMIR 2023) year also claims to be an audio-conditioned model, albeit with a different approach. It would have been great to include it as a baseline in evaluations. VampNet is mentioned in line 136, but is missing a citation.</li>
</ul></p>
              </div>
              <hr/>

            
            <div> 
            <b> Author description of changes: </b>
            <p>I have modified the paper according to the reviewers comments. The paper has been corrected by a native english speaker. I have modified details and citations according to their comments as well.</p></p>
            </div>
                   
          </div>
                    
    
        </div>
      </div>
    
  </div>
  





  <script>
  document.addEventListener('DOMContentLoaded', function() {
    console.log(JSON.stringify("{&#39;id&#39;: &#39;41&#39;, &#39;session&#39;: &#39;1&#39;, &#39;position&#39;: &#39;14&#39;, &#39;forum&#39;: &#39;41&#39;, &#39;pic_id&#39;: &#39;https://drive.google.com/file/d/1KIzQlQbxxz8C4TrXwWVAgSnKHl5QL_vU/view?usp=drive_link&#39;, &#39;content&#39;: {&#39;title&#39;: &#39;Audio Conditioning for Music Generation via Discrete Bottleneck Features&#39;, &#39;paper_presentation&#39;: &#39;San Francisco&#39;, &#39;long_presentation&#39;: &#39;FALSE&#39;, &#39;authors&#39;: [&#39;Rouard, Simon*&#39;, &#39; Defossez, Alexandre&#39;, &#39; Adi, Yossi&#39;, &#39; Copet, Jade&#39;, &#39; Roebel, Axel&#39;], &#39;authors_and_affil&#39;: [&#39;Simon Rouard (Meta AI Research)*&#39;, &#39; Alexandre Defossez (Kyutai)&#39;, &#39; Yossi Adi (Facebook AI Research )&#39;, &#39; Jade Copet (Meta AI Research)&#39;, &#39; Axel Roebel (IRCAM)&#39;], &#39;keywords&#39;: [&#39;MIR tasks -&gt; music generation&#39;, &#39;Creativity -&gt; human-ai co-creativity; Generative Tasks -&gt; music and audio synthesis; Knowledge-driven approaches to MIR -&gt; machine learning/artificial intelligence for music; MIR fundamentals and methodology -&gt; music signal processing&#39;], &#39;abstract&#39;: &#39;While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding &#34;pseudowords&#34; in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.&#39;, &#39;TLDR&#39;: &#39;While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding &#34;pseudowords&#34; in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies and provide music samples in order to show the quality of our model.&#39;, &#39;poster_pdf&#39;: &#39;https://drive.google.com/file/d/1I1TzCEWABu-BD52VU9EDr0hjPFr3Cg1k/view?usp=drive_link&#39;, &#39;session&#39;: [&#39;1&#39;], &#39;pdf_path&#39;: &#39;https://drive.google.com/file/d/1AXwd4qZXBWY7jJ12ASDW13nKpzPeW910/view?usp=drive_link&#39;, &#39;video&#39;: &#39;https://drive.google.com/file/d/1C_XUyhaIMcDv04fFALbIhQemHpVAyYw9/view?usp=drive_link&#39;, &#39;channel_url&#39;: &#39;https://ismir2024.slack.com/archives/C07V2MM27CZ&#39;, &#39;slack_channel&#39;: &#39;p1-13-audio-conditioning-for&#39;, &#39;day&#39;: &#39;1&#39;, &#39;review_1&#39;: &#39;The paper is well-written, with an extensive related work section.  The accompanying website features many interesting examples and code will be released on acceptance. The authors propose to investigate the use of textual inversion with a pretrained MusicGen model to generate variations of an existing song.\n\nTwo similarity metrics are introduced to spot copies in the generated material.\n\nMy main concern is about the original motivation and statement of the problem. If textual inversion proves to be able to capture audio features from the audio conditioning and regenerate audio with similar characteristics, we can regret that this is very close from standard continuation and that the possibilities offered by textual inversion are not fully exploited. Examples like &#34;Chill lofi remix ex. 1&#34; from the demo website is very similar to the prompt for instance, \n\nIn this particular case, the learnt textual embeddings are only learnt from one song: I would have liked to see more general and diverse applications. \nFor instance, is it possible to learn new styles / instruments / chord progressions based on a collection of songs? (in this paper style is often used as a replacement for song, which may be misleading). If such application is closer to what\&#39;s mentioned in the introduction &#34;Given a few images (3-5) of a concept or object&#34;, it is never discussed in the paper. It would also remove the necessity to introduce bottlenecks.\n\n\nSome questions:\n&#34;For some song, we never achieve to obtain hearable\n392 music as the result suffers from glitches, and tempo instabilities.&#34;: is it possible to automatically know when this method fails?\n\nSome things to clarify in §4.4:\n-nearest neighbours written i_1^C but chunks indexed with i,j: this is confusing, is it done on the chunk or song level?\n-&#34;However, if a model copies the conditioning (i.e. xG ≈ xC) the metric will tend to 1, we thus need a second metric to avoid xG and xC being too similar.&#34; it sounds as if the metric had an influence on the generation process\n- G is the Nearest Neighbor of C: should be clearer for a subtitle, what are G and C?\n\n\n\n\nInteresting paper but this may seem like a straightforward application of the textual inversion method. Choosing the audio condition from the same song when performing the textual inversion may not be the best use case as this forces the introduction of bottlenecks in the feature extractor. The comparision with different feature extractors is interesting, especially the fact that  &#34;Self-supervised encoder like MERT and MusicFM outperforms low level acoustic models like EnCodec.&#34;. Demos are of good quality and well presented.&#39;, &#39;review_2&#39;: &#39;Very well written and complete paper with a lot of impressive details and convincing demo results. I have troubles finding anything majorly wrong with this work and just have to say kudos for the impressive piece of work!&#39;, &#39;review_3&#39;: &#34;This paper introduces a method for generating music conditioned on other music — using a high-level semantic embedding from another piece of music as conditioning. \n\nThe authors evaluate their method against a textual inversion baseline (adapted from the image domain by the authors) and find that their proposed method is faster and generates higher-quality audio than the given baselines. \n\nThe authors perform ablations of several aspects of their system and discuss challenges, like how text conditioning is ignored when audio conditioning is present, and how including too much information from a style embedding can lead to the model perfectly reconstructing the audio. \n\nOverall, I like the authors&#39; approach and find their evaluation sufficient. My recommendation is Strong Accept.\n\n**strengths**\n\n- The claims of the paper are held together well by objective experiments and a human listening study.\n- The listening examples demonstrate that the author’s proposed method generates high-quality music that “riffs” on the musical idea given in an audio “style” prompt.\n\n**weaknesses**\n\n- While the KNN-based objective metrics make intuitive sense, I wonder if there’s a more interpretable way to observe the effects of which aspects of the “style” get preserved or not.  For example, I could imagine it is possible to check if the generated audio has too similar of a tempo/accents/rhythms/harmony/melodic contour as the style reference audio by leveraging pretrained MIR models/feature extractors to extract these music descriptors from both the reference and generated audio.\n- The paper claims to be the only existing audio-conditioned music generative model, but VampNet (ISMIR 2023) year also claims to be an audio-conditioned model, albeit with a different approach. It would have been great to include it as a baseline in evaluations. VampNet is mentioned in line 136, but is missing a citation.&#34;, &#39;meta_review&#39;: &#39;This paper was generally well received by the reviewers. Congrats, authors!\n\nPlease, address the reviewers comments as closely as possible. Especially the following:\n- Cite the appropriate previous work (i.e., this is not the only audio-conditioned music generative model)\n- Have a proficient English speaker to proofread the manuscript&#39;, &#39;author_changes&#39;: &#39;I have modified the paper according to the reviewers comments. The paper has been corrected by a native english speaker. I have modified details and citations according to their comments as well.&#39;}, &#39;poster_pdf&#39;: &#39;GLTR_poster.pdf&#39;}"));
    // Ensure jQuery and Bootstrap are loaded
    if (typeof $ === 'undefined') {
      console.error('jQuery is not loaded');
      return;
    }
    
    // Initialize all tabs
    $('#posterTabs a').on('click', function (e) {
      e.preventDefault();
      $(this).tab('show');
    });
  
    // Show paper tab by default
    $('#posterTabs a[href="#paper"]').tab('show');
    MathJax.typesetPromise().then(() => {
    // modify the DOM here
        MathJax.typesetPromise();
    }).catch((err) => console.log(err.message));
  });
  </script>
  
  
    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 International Society for Music Information Retrieval</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
   
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>